<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"
  xmlns:atom="http://www.w3.org/2005/Atom"
  xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Yunsaijc&#39;s Blog</title>
    <link>http://yunsaijc.top/</link>
    
    <atom:link href="http://yunsaijc.top/rss2.xml" rel="self" type="application/rss+xml"/>
    
    <description>Enjoy</description>
    <pubDate>Sun, 22 Oct 2023 03:00:06 GMT</pubDate>
    <generator>http://hexo.io/</generator>
    
    <item>
      <title>PLM+KG论文复现</title>
      <link>http://yunsaijc.top/2023/10/22/16-PLM+KG%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0/</link>
      <guid>http://yunsaijc.top/2023/10/22/16-PLM+KG%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0/</guid>
      <pubDate>Sun, 22 Oct 2023 02:53:21 GMT</pubDate>
      
        
        
      <description>&lt;h1 id=&quot;Grapher&quot;&gt;&lt;a href=&quot;#Grapher&quot; class=&quot;headerlink&quot; title=&quot;Grapher&quot;&gt;&lt;/a&gt;Grapher&lt;/h1&gt;&lt;h2 id=&quot;Setup&quot;&gt;&lt;a href=&quot;#Setup&quot; class=&quot;headerlink&quot; ti</description>
        
      
      
      
      <content:encoded><![CDATA[<h1 id="Grapher"><a href="#Grapher" class="headerlink" title="Grapher"></a>Grapher</h1><h2 id="Setup"><a href="#Setup" class="headerlink" title="Setup"></a>Setup</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="built_in">clone</span> project</span>   </span><br><span class="line">git clone git@github.com:IBM/Grapher.git</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">navigate to the directory</span></span><br><span class="line">cd Grapher</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="built_in">clone</span> an external repository <span class="keyword">for</span> reading the data</span></span><br><span class="line">git clone https://gitlab.com/webnlg/corpus-reader.git corpusreader</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="built_in">clone</span> another external repositories <span class="keyword">for</span> scoring the results</span></span><br><span class="line">git clone https://github.com/WebNLG/WebNLG-Text-to-triples.git WebNLG_Text_to_triples</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">download the dataset</span>   </span><br><span class="line">git clone https://gitlab.com/shimorina/webnlg-dataset.git</span><br></pre></td></tr></table></figure><p>打开代理（否则运行训练的时候无法下载模型，或者将模型下载到本地之后修改一下代码也行），随后开始进行训练。期间可能会报错，说缺少某些包，pip安装一下即可</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd scripts/</span><br><span class="line">bash train_gen.sh</span><br><span class="line"># or</span><br><span class="line">bash train_class.sh</span><br></pre></td></tr></table></figure><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><blockquote><p><a href="https://github.com/IBM/Grapher">https://github.com/IBM/Grapher</a></p><p><a href="https://zhuanlan.zhihu.com/p/662552814">https://zhuanlan.zhihu.com/p/662552814</a></p></blockquote>]]></content:encoded>
      
      
      <category domain="http://yunsaijc.top/categories/%E9%A1%B9%E7%9B%AE%E8%AE%B0%E5%BD%95/">项目记录</category>
      
      
      <category domain="http://yunsaijc.top/tags/AI/">AI</category>
      
      <category domain="http://yunsaijc.top/tags/LLM/">LLM</category>
      
      
      <comments>http://yunsaijc.top/2023/10/22/16-PLM+KG%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Transformer原文&lt;Attention Is All You Need&gt;阅读笔记</title>
      <link>http://yunsaijc.top/2023/10/17/15-Transformer%E5%8E%9F%E6%96%87%3CAttention%20Is%20All%20You%20Need%3E%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</link>
      <guid>http://yunsaijc.top/2023/10/17/15-Transformer%E5%8E%9F%E6%96%87%3CAttention%20Is%20All%20You%20Need%3E%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</guid>
      <pubDate>Tue, 17 Oct 2023 00:54:12 GMT</pubDate>
      
        
        
      <description>&lt;blockquote&gt;
&lt;p&gt;原文链接：&lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;&gt;https://arxiv.org/abs/1706.03762&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;Intro&quot;&gt;&lt;a hre</description>
        
      
      
      
      <content:encoded><![CDATA[<blockquote><p>原文链接：<a href="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a></p></blockquote><h1 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h1><p>RNN（特别是LSTM和GRN）在序列的建模与转换问题上（如语言建模、机器翻译）成为了SOTA方法，因此学者们对循环语言模型 (recurrent language models) 和编码-解码器架构 (encoder-decoder) 进行了大量研究。</p><p>循环网络模型考虑了输入和输出序列中，字符的位置来进行计算。循环模型用上一个隐藏状态$h_{t-1}$和$t$时刻的输入来产生下一个隐藏状态$h_t$，也就是将时序信息融入了计算。这种时序模型难以并行化处理，计算性能差。虽然有许多并行化改进的研究，但这个问题仍然非常存在且关键</p><blockquote><p>注意力机制此前已经应用于编码-解码器架构中，用于将编码器的信息有效地传递给解码器，然而这种机制大都与RNN结合使用</p></blockquote><p>本文提出了Transformer架构，不依赖循环网络，而是完全基于注意力机制来建立输入与输出之间的全局关系。Transformer更加支持并行化，并且在更短的训练时间内达到新的SOTA性能</p><h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>为了减少时序计算的开销，Extended Neural GPU，ByteNet和 ConvS2S这些基于CNN的方法，都并行地计算所有输入和输出位置的隐藏状态</p><p>这些模型中，关联任意两个输入或输出位置的数据，所需要的运算次数随着两者之间的距离而增长。在ConvS2S中线性增长，在ByteNet中以对数增长。这就使得关联两个较远位置的数据变得更加困难</p><blockquote><p>引用：“因为卷积计算时，卷积核/感受野比较小，如果序列很长，需要使用多层卷积才可以将两个比较远的位置关联起来）。但是使用Transformer的注意力机制的话，每次（一层）就能看到序列中所有的位置”</p></blockquote><p>在Transformer中，运算次数是以常数随着距离增长的。虽然，因为我们对序列中的各个位置进行了注意力加权的平均，减少了有效解，但我们会用多头注意力机制 (Multi-Head Attention) 来抵消这种影响</p><blockquote><p>引用：卷积的好处是输出可以有多个通道，每个通道可以认为是识别不同的模式。所以作者提出了多头注意力机制，来模拟卷积的多通道输出效果</p></blockquote><p>自注意力 (self-attetion/ intra-attention) 是一种注意力机制，将一个序列中不同位置的数据关联起来，来计算这个序列的一种（数学）表示 (representation)。该机制在阅读理解、抽象总结等方面都得到了成功的应用。</p><p>Transformer是第一个完全依靠self-attention，而不使用卷积或循环的encoder-decoder转换模型</p><h1 id="模型架构"><a href="#模型架构" class="headerlink" title="模型架构"></a>模型架构</h1><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>encoder-decoder结构：encoder把输入序列$(x_1,…,x_n)$映射到一个连续的（数学）表示$\textbf{z}=(z_1,…,z_n)$。给定$\textbf{z}$，decoder就会每次一个元素地产生输出序列$(y_1,…,y_m)$</p><blockquote><p>个人理解：我在“表示”的前面加上“（数学）”，是因为$x$是非数学的字符（如文字），将其转换为数学的字符（如向量中的元素，即数字）后，即为$z$</p></blockquote><p>每一步中，模型都是自回归的 (auto-regressive) ，在生成下一结果时，会将之前生成的结果加到输入序列中。（自回归模型，即过去时刻的输出可以作为当前时刻的输入）</p><p>Transformer遵循这种架构，对encoder和decoder使用堆叠的自注意力和逐点全连接层，分别如图1的左半和右半部分所示</p><img src="/2023/10/17/15-Transformer%E5%8E%9F%E6%96%87%3CAttention%20Is%20All%20You%20Need%3E%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/1.png" class=""><h2 id="Encoder和Decoder"><a href="#Encoder和Decoder" class="headerlink" title="Encoder和Decoder"></a>Encoder和Decoder</h2><h3 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h3><p>Encoder由$N=6$个相同的层组成，每个层有两个子层：</p><ol><li>multi-head self-attention</li><li>position-wise fully connected feed-forward network </li></ol><p>每层后面都使用残差连接 (residual connection) 和层标准化 (layer normalization)。即每个子层的输出为：<script type="math/tex">LayerNorm(x+Sublayer(x))</script><br>所有子层、嵌入层 (embedding layer) 的输出维度都是$d_{model}=512$</p><blockquote><p>引用-Layer Norm：<br>首先看一下Batch Norm，它针对同一个特征来进行Norm（如把均值变为0，方差变为1）；而Layer Norm是针对同一个样本进行Norm。</p><p>Batch Norm需要计算全局样本的同一特征，序列长度变化大时，计算出来的均值和方差抖动很大。预测时使用训练时记录下来的全局均值和方差。如果预测时新的样本长度超过训练时的长度，那么超过部分是没有记录的均值和方差的，预测会出现问题</p><p>而Layer Norm只需要对该单个样本进行计算，序列变长时，计算的数值更稳定。不需要存一个全局的均值和方差，预测样本长度不影响最终结果</p></blockquote><h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><p>Decoder也由$N=6$个相同的层组成，每个层有三个子层（多了一个在encoder输出上运用多头注意力机制的层）。同样在每层后面使用残差连接和层标准化</p><p>不同之处在于对自注意力模块进行了修改，防止当前位置的输出收到后面位置数据的影响。这样的掩码，加上对输出的嵌入偏移一个位置，确保了位置$i$的预测只依赖于位置$i$之前的已知输出</p><h2 id="注意力机制"><a href="#注意力机制" class="headerlink" title="注意力机制"></a>注意力机制</h2><p>注意力函数将一个查询 (query) 和一组键值对 (key-value pairs) 映射到一个输出，输出是这些值的一个加权和，其中每个值的权重由query与相应key的相似度 (compatibility function) 计算</p><blockquote><p>引用-对QKV的理解：K，V的值是不变的。根据不同的Q，去计算这个Q与每个K的相似度，QK相近的时候，相似度会大一些，因此这个K对应的V权重也就大一些。这也就是注意力机制</p></blockquote><h3 id="缩放的点积注意力-Scaled-Dot-Product-Attention"><a href="#缩放的点积注意力-Scaled-Dot-Product-Attention" class="headerlink" title="缩放的点积注意力 (Scaled Dot-Product Attention)"></a>缩放的点积注意力 (Scaled Dot-Product Attention)</h3><img src="/2023/10/17/15-Transformer%E5%8E%9F%E6%96%87%3CAttention%20Is%20All%20You%20Need%3E%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/2.png" class=""><p>如图。计算过程：</p><ol><li>输入维度为$d_k$的 Q (Queries) 和 K (Keys)，以及维度为$d_v$的 V (Values)</li><li>计算 Q 和所有 K 的点积（左下的MatMul），得到两个向量的相似度（结果越大相似度越高）</li><li>对每个点积结果除以$\sqrt{d_k}$（Scale）</li><li>点积结果输入softmax函数获得V的权重​（SoftMax）</li><li>对V进行加权求和（上方的MatMul）</li></ol><blockquote><p>引用-QK点积：比如两个向量作内积，正交时内积为0，相似度也为0；反之内积值越大，相似度越高。所以此处的QK点积同理</p></blockquote><p>实践中对所有Q同时计算，组成一个矩阵$Q$，K和V也组成矩阵$K,\ V$，那么注意力矩阵的计算为：<script type="math/tex">Attention(Q,\ K,\ V)=softmax(\frac{QK^T}{\sqrt{d_k}})V</script></p><p>有两种最常用的注意力函数：加法注意力和点积注意力。除了没有系数$\frac{1}{\sqrt{d_k}}$之外，后者与上述注意力函数并无不同。加法注意力使用单个隐藏层的前馈网络 (feed-forward network) 来进行计算。两者在理论复杂度上相似，而实践中点积注意力要快于加法注意力，因为它可以使用高度优化的矩阵乘法</p><p>当$d_k$较小时，点积和加法注意力性能相当；而$d_k$较大时，加法注意力的性能优于不带缩放的点积注意力。我们认为当$d_k$较大时，点积的值非常大，导致映射到softmax时梯度非常小，所以使用$\frac{1}{\sqrt{d_k}}$来进行缩放</p><blockquote><p>引用-使用Scaled的原因：不缩放的话内积值较大，那么通过softmax之后，大的值就很大，小的值很小（也就是权重的值向softmax的两边靠拢），权重之间的差距变大。而平常使用softmax的目标就是，置信的地方趋近于1，不置信的地方趋近于0，那么这样子就会趋近于收敛，使训练过程难以往下进行</p><p>再次阅读之后，个人重新理解一下图2。左侧QK做的运算就是为了计算注意力的权重，计算出来的权重直接与V进行矩阵乘法，就是注意力机制</p></blockquote><h3 id="多头注意力"><a href="#多头注意力" class="headerlink" title="多头注意力"></a>多头注意力</h3><img src="/2023/10/17/15-Transformer%E5%8E%9F%E6%96%87%3CAttention%20Is%20All%20You%20Need%3E%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/3.png" class=""><p>我们将Q, K, V 进行$h$次投影，线性投影到$d_k$、$d_k$和$d_v$维度。随后在这些投影上运用注意力机制，产生$d_v$维度的输出值，把这些值连接起来再作一次投影，就得到了最终结果值。</p><blockquote><p>引用-多头：把QKV投影h次，到更低的维度，做h次的注意力函数。把每个注意力的输出合并，最后再进行投影。</p><p>这个投影的w是可以进行学习的，也就是说，给h次机会，希望能学到不同的投影方法，来匹配不同的模式。所以说类似于CNN，有多个输出通道的感觉</p></blockquote><img src="/2023/10/17/15-Transformer%E5%8E%9F%E6%96%87%3CAttention%20Is%20All%20You%20Need%3E%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/4.png" class=""><p>本文参数设置：$h=8$，对于每个head有$d<em>k=d_v=d</em>{model}/h=64$，这样下来，总的计算开销与全维度的单头注意力相似</p><h3 id="本模型中注意力的应用"><a href="#本模型中注意力的应用" class="headerlink" title="本模型中注意力的应用"></a>本模型中注意力的应用</h3><ul><li>encoder-decoder attention ：即decoder的第二个子层。Q来自前一decoder子层，K和V来自encoder的输出。这使decoder中的每个位置都能关注到输入序列中的所有位置</li><li>multi-head self-attention：即encoder的第一个子层。这使得encoder中的每个位置都可以关注到encoder上一层的所有位置</li><li>masked-self-attention：用在decoder中，序列中的每个位置只能看到当前位置之前的位置，这是为了保持decoder的自回归特性，防止看到未来位置的信息 </li></ul><blockquote><p>引用-自注意力机制：同一个输入复制一下，同时作为KVQ，也就是同一个东西。</p><p>在encoder-decoder attention中，Q来自decoder上一子层，而KV来自encoder。那么这个 attention的输出，就是根据encoder输出和decoder上一子层输出的相似性，来对encoder的输出计算加权和（也就是说把encoder的输出，根据Q想要的东西，把它拎出来）</p><p>引用-关于Mask：对于$Q<em>t$，这个查询只能看到该时刻之前的信息，所以就只能看到$K_1,…,K</em>{t-1}$。在算好权重并且scale完之后，进入softmax之前，把需要在矩阵中mask的位置都替换成非常大的负数，那么softmax过后，对应的权重就会变成0</p></blockquote><h2 id="基于位置的前馈神经网络（Position-wise-Feed-Forward-Networks）"><a href="#基于位置的前馈神经网络（Position-wise-Feed-Forward-Networks）" class="headerlink" title="基于位置的前馈神经网络（Position-wise Feed-Forward Networks）"></a>基于位置的前馈神经网络（Position-wise Feed-Forward Networks）</h2><p>encoder和decoder的每一层还包含一个全连接的FFN，对每个位置（上的token）都要做一次以下的变换（包含两个线性变换）：</p><img src="/2023/10/17/15-Transformer%E5%8E%9F%E6%96%87%3CAttention%20Is%20All%20You%20Need%3E%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/5.png" class=""><p>换一种描述方法，其实就是两个卷积核大小为1的卷积层。其中输入和输出的维度为$d<em>{model}=512$，内层维度为$d</em>{ff}=2048$</p><blockquote><p>引用：其实就是一个单隐藏层的MLP，映射到想要的语义空间。其中的max..就是一个ReLU激活层；$W_1$将$x$映射到2048维，$W_2$映射回512维</p><p>Point-wise就是指对每个词都做一个同样的MLP。由于经过注意力机制后，输出的向量已经从序列中提取到了想要的信息，所以只需要对每个位置独立做MLP即可。</p><p>引用-与RNN的区别：两者都使用线性层/ MLP去做向语义空间的转换。但提取序列信息的方式不同。RNN把上一时刻的输出作为输入，传递给下一时刻；而Transformer通过注意力机制来提取全局的序列信息</p></blockquote><h2 id="嵌入与softmax"><a href="#嵌入与softmax" class="headerlink" title="嵌入与softmax"></a>嵌入与softmax</h2><p>我们使用可学习的embedding，将输入和输出转换为$d_{model}$​维的向量，并使用线性变换和softmax函数将decoder的输出转换为下一个token的概率</p><blockquote><p>“可学习的”也就是说，这个embedding的方式是一个可学习的参数</p><p>此处的softmax前的“线性变换”其实也就是一个embedding</p></blockquote><p>本模型中，（输入、输出）两个embedding层和pre-softmax线性变换所使用的权重矩阵相同。此外，我们将embedding层中的权重乘了$\sqrt{d_{model}}$</p><blockquote><p>引用-乘权重的原因：在学习embedding时，会把向量的L2 Norm学成比较小的值（假设为1）。那么当向量的维度一大，为了让L2 Norm学成1，学习时会让权重变小。但之后还要加上位置编码，而位置编码的scale会随着向量长度变大而增大，所以此处乘一下是为了和位置编码保持相似的scale</p><p>再换句话：L2 Norm会将向量的所有值归一化，维度越大的向量归一化后，单个元素的值就越小，而时序信息（也就是后面的位置编码）是递增的整数</p></blockquote><h2 id="位置编码"><a href="#位置编码" class="headerlink" title="位置编码"></a>位置编码</h2><blockquote><p>引用：attention不会学习时序信息。也就是说把输入顺序打乱之后，语义已经变化了，但结果是一样的</p></blockquote><p>上述的描述中还缺少对位置信息的嵌入，所以在输入的embedding中加入了位置编码（因为处理语言必须要考虑语序）。其维度与embedding相同，都是$d_{model}$，便于两者相加。本文使用以下方式：</p><img src="/2023/10/17/15-Transformer%E5%8E%9F%E6%96%87%3CAttention%20Is%20All%20You%20Need%3E%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/6.png" class=""><p>其中$pos$是位置，$i$是维度。使用这种编码的原因是，作者认为这能让模型易于注意相对位置，因为对于任何偏移量k，$PE<em>{pos+k}$可以表示为$PE</em>{pos}$的线性函数。此外，在遇到比训练数据更长的数据时，这种编码方便模型估计数据的长度=</p><h1 id="为什么用自注意力"><a href="#为什么用自注意力" class="headerlink" title="为什么用自注意力"></a>为什么用自注意力</h1><p>本节将自注意力与卷积、循环进行了对比。考虑了三个问题：</p><ol><li>每层的计算复杂度</li><li>可并行的计算量，也就是所需的最少顺序计算量</li><li>网络中长距离依赖之间的路径长度。影响学习长距离的依赖的能力的一个关键因素，是前向和后向信号在网络中传播的路径长度。输入和输出序列中任意位置之间的这些路径越短，学习长距离的依赖就越容易。因此，我们比较了由不同层类型组成的网络中，任意两个输入和输出位置之间的最大路径长度</li></ol><blockquote><p>引用：第3点换种说法就是，得到两个位置之间的关系所需要的计算量</p></blockquote><img src="/2023/10/17/15-Transformer%E5%8E%9F%E6%96%87%3CAttention%20Is%20All%20You%20Need%3E%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/7.png" class=""><p>其中$n$是序列长度，$d$是token维度，$k$是卷积核大小。可以发现自注意的复杂度更小</p><blockquote><p>引用-restricted的自注意：也就是限制一下范围，只关注最近的r个邻居。但这种方法用得不多</p><p>引用：实际上attetion对模型的假设更少，所以需要更大的训练数据量才能达到跟CNN/ RNN相同的效果</p></blockquote><h1 id="训练、实验、结论"><a href="#训练、实验、结论" class="headerlink" title="训练、实验、结论"></a>训练、实验、结论</h1><p>略</p><h1 id="个人问题与理解"><a href="#个人问题与理解" class="headerlink" title="个人问题与理解"></a>个人问题与理解</h1><ol><li><p>e-d attention为什么是decoder的第二层，不是第一层？</p><p> 首先要正确理解注意力。自注意就是对某个序列自身进行查询，得到该序列自己对自己的各个部分的关注度。那么encoder和decoder输入后，都先做一个自注意，得到自注意分数，就便于在encoder的输出中进行查询</p></li><li><p>mask为什么不是先mask再算权重？</p><p> 如果先mask的话，就是对$Q, K$做mask。但是QK长宽不一样，而先算权重再mask的话就是对$QK^T$做mask，只需要用下三角方阵做mask即可（也就是说对方阵做mask更好做）<br> （以上为个人理解，不一定对）</p></li><li><p>为什么decoder第一个子层后面不加MLP？</p><p>未解决</p></li></ol><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><blockquote><p><a href="https://blog.csdn.net/qq_56591814/article/details/127313216?spm=1001.2014.3001.5502">https://blog.csdn.net/qq_56591814/article/details/127313216?spm=1001.2014.3001.5502</a></p><p><a href="https://www.bilibili.com/video/BV1pu411o7BE/?spm_id_from=333.999.0.0&amp;vd_source=31c543d6ef0ac6a08fbd17482736f920">https://www.bilibili.com/video/BV1pu411o7BE/?spm_id_from=333.999.0.0&amp;vd_source=31c543d6ef0ac6a08fbd17482736f920</a></p></blockquote>]]></content:encoded>
      
      
      <category domain="http://yunsaijc.top/categories/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/">文献阅读</category>
      
      
      <category domain="http://yunsaijc.top/tags/AI/">AI</category>
      
      <category domain="http://yunsaijc.top/tags/LLM/">LLM</category>
      
      
      <comments>http://yunsaijc.top/2023/10/17/15-Transformer%E5%8E%9F%E6%96%87%3CAttention%20Is%20All%20You%20Need%3E%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>ML的4个指标——准确率、精确率、召回率与F1 score</title>
      <link>http://yunsaijc.top/2023/10/11/13-ML%E7%9A%844%E4%B8%AA%E6%8C%87%E6%A0%87%E2%80%94%E2%80%94%E5%87%86%E7%A1%AE%E7%8E%87%E3%80%81%E7%B2%BE%E7%A1%AE%E7%8E%87%E3%80%81%E5%8F%AC%E5%9B%9E%E7%8E%87%E4%B8%8EF1%20score/</link>
      <guid>http://yunsaijc.top/2023/10/11/13-ML%E7%9A%844%E4%B8%AA%E6%8C%87%E6%A0%87%E2%80%94%E2%80%94%E5%87%86%E7%A1%AE%E7%8E%87%E3%80%81%E7%B2%BE%E7%A1%AE%E7%8E%87%E3%80%81%E5%8F%AC%E5%9B%9E%E7%8E%87%E4%B8%8EF1%20score/</guid>
      <pubDate>Wed, 11 Oct 2023 12:55:23 GMT</pubDate>
      
        
        
      <description>&lt;p&gt;机器学习的分类任务中，准确率(accuracy)、精确率(precision)、召回率(recall)与F1 score是常见的4个评估指标。之前对于这些指标有粗略的了解，如今加深一下理解&lt;/p&gt;
&lt;h1 id=&quot;混淆矩阵-Confusion-Matrix&quot;&gt;&lt;a href</description>
        
      
      
      
      <content:encoded><![CDATA[<p>机器学习的分类任务中，准确率(accuracy)、精确率(precision)、召回率(recall)与F1 score是常见的4个评估指标。之前对于这些指标有粗略的了解，如今加深一下理解</p><h1 id="混淆矩阵-Confusion-Matrix"><a href="#混淆矩阵-Confusion-Matrix" class="headerlink" title="混淆矩阵(Confusion Matrix)"></a>混淆矩阵(Confusion Matrix)</h1><p>混淆矩阵如图所示：</p><img src="/2023/10/11/13-ML%E7%9A%844%E4%B8%AA%E6%8C%87%E6%A0%87%E2%80%94%E2%80%94%E5%87%86%E7%A1%AE%E7%8E%87%E3%80%81%E7%B2%BE%E7%A1%AE%E7%8E%87%E3%80%81%E5%8F%AC%E5%9B%9E%E7%8E%87%E4%B8%8EF1%20score/1.png" class=""><p>四个值的具体解释为：</p><ul><li>TP(True Positive)：被正确预测的正例。即真实值为正，预测值为正</li><li>TN：被正确预测的反例</li><li>FP：被错误预测的正例。即真实值为负，预测值为正</li><li>FN(False Negative)：被错误预测的反例</li></ul><blockquote><p>即，第一个字母指的是预测正误，第二个字母指的是预测结果</p></blockquote><h1 id="准确率-Accuracy"><a href="#准确率-Accuracy" class="headerlink" title="准确率(Accuracy)"></a>准确率(Accuracy)</h1><p>表示分类正确的样本占总样本个数的比例，即：<script type="math/tex">Accuracy=\frac{TP+TN}{TP+TN+FP+FN}</script></p><p>这是最直接的指标，但缺陷在于：当不同类别的样本占比不平衡时，占比大的类别会是影响准确率的最主要因素</p><p>举例：当数据集中99%为正例，那么只要分类器一直预测为正，即可达到很高的准确率</p><p>因此只有当数据集中各个类别样本比例均衡时，准确率才有较大的参考意义</p><h1 id="精确率-Precision"><a href="#精确率-Precision" class="headerlink" title="精确率(Precision)"></a>精确率(Precision)</h1><p>表示被预测为正的样本中，实际为正样本的比例。即：<script type="math/tex">Precision=\frac{TP}{TP+FP}</script></p><p>个人理解一下：</p><ul><li>首先，能够影响精确率的只有被预测为正的样本</li><li>被预测为正的样本中，如果其他不变<ul><li>TP增大（被正确地判断为正的样本增多），精确率变大</li><li>FP增大（被错误地判断为正的样本增多），精确率变小</li></ul></li></ul><p>所以，精确率越高，将负样本误判为正的概率越小</p><p>换句话说，提升精确率，是为了不将负样本误判为正（不错判）</p><h1 id="召回率-Recall"><a href="#召回率-Recall" class="headerlink" title="召回率(Recall)"></a>召回率(Recall)</h1><p>表示实际为正的样本中，被预测为正的样本所占比例。即：<script type="math/tex">Recall=\frac{TP}{TP+FN}</script></p><blockquote><p>可以发现，上述文字定义就是将精确率的文字定义倒转过来</p></blockquote><p>个人理解一下：</p><ul><li>首先，能够影响召回率的只有实际为正的样本</li><li>实际为正的样本中，如果其他不变：<ul><li>TP增大（被正确地判断为正的样本增多），召回率变大</li><li>FN增大（被错误地判断为负的样本增多），召回率变小</li></ul></li></ul><p>所以，召回率越高，将正样本误判为负的概率越小</p><p>换句话说，提高精确率，是为了不将正样本误判为负（不漏判）</p><h1 id="F1-Score"><a href="#F1-Score" class="headerlink" title="F1 Score"></a>F1 Score</h1><p>通过上述分析可以发现，精确率和召回率是两难全的。综合一下<br>两者，F1 score是精确率和召回率的一个加权平均：<script type="math/tex">F1=2\times\frac{Precision\times Recall}{Precision+Recall}</script></p><p>因为精确率体现分类器不错判的能力，召回率体现不漏判的能力，所以F1 Score越高，模型越稳健</p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><blockquote><p><a href="https://zhuanlan.zhihu.com/p/405658103">https://zhuanlan.zhihu.com/p/405658103</a></p><p><a href="https://zhuanlan.zhihu.com/p/93107394">https://zhuanlan.zhihu.com/p/93107394</a></p><p><a href="https://zhuanlan.zhihu.com/p/93586831">https://zhuanlan.zhihu.com/p/93586831</a></p></blockquote>]]></content:encoded>
      
      
      <category domain="http://yunsaijc.top/categories/AI%E5%9F%BA%E7%A1%80/">AI基础</category>
      
      
      <category domain="http://yunsaijc.top/tags/AI/">AI</category>
      
      
      <comments>http://yunsaijc.top/2023/10/11/13-ML%E7%9A%844%E4%B8%AA%E6%8C%87%E6%A0%87%E2%80%94%E2%80%94%E5%87%86%E7%A1%AE%E7%8E%87%E3%80%81%E7%B2%BE%E7%A1%AE%E7%8E%87%E3%80%81%E5%8F%AC%E5%9B%9E%E7%8E%87%E4%B8%8EF1%20score/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>与PLM有关的知识图谱构建相关文献-阅读笔记</title>
      <link>http://yunsaijc.top/2023/10/10/14-%E4%B8%8EPLM%E6%9C%89%E5%85%B3%E7%9A%84%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E6%9E%84%E5%BB%BA%E7%9B%B8%E5%85%B3%E6%96%87%E7%8C%AE-%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</link>
      <guid>http://yunsaijc.top/2023/10/10/14-%E4%B8%8EPLM%E6%9C%89%E5%85%B3%E7%9A%84%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E6%9E%84%E5%BB%BA%E7%9B%B8%E5%85%B3%E6%96%87%E7%8C%AE-%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</guid>
      <pubDate>Tue, 10 Oct 2023 09:28:12 GMT</pubDate>
      
        
        
      <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;h1 id=&quot;2023-PiVe&quot;&gt;&lt;a href=&quot;#2023-PiVe&quot; class=&quot;headerlink&quot; title=&quot;(2023)PiVe&quot;&gt;&lt;/a&gt;(2023)PiVe&lt;/h1&gt;&lt;p&gt;现有的LLM大多基于非结构化的数据进行预训练，所以对于</description>
        
      
      
      
      <content:encoded><![CDATA[<p>[TOC]</p><h1 id="2023-PiVe"><a href="#2023-PiVe" class="headerlink" title="(2023)PiVe"></a>(2023)PiVe</h1><p>现有的LLM大多基于非结构化的数据进行预训练，所以对于“将文本组织成机器可读的结构化的格式”的任务，它们的表现并不理想</p><p>语义图是一种图结构的数据，存储着机器可以访问的信息。从文本生成语义图，成为text-to-graph(T2G)。过去的T2G主要依靠微调的小语言模型来完成。然而对于LLM来说，即使在少样本的条件下，LLM（如GPT-3.5）仍然存在错误</p><p>本文中，我们重点讨论了如何提高LLM生成图的能力。为此，我们提出了Prompting through Iterative Verification (PiVe)的框架，如图1所示。PiVe利用外部验证模块（即一个小得多的LM），并将验证模块的反馈合并到prompt中。PiVe反复使用验证器模块，并在将prompt发送回LLM之前，通过纠正指令来完善prompt，从而大大提高生成的语义图的质量</p><p>训练验证模块的方法：我们从一个种子数据集开始（包含文本和图的(T, G)对），通过在种子集上随意选取一个图G，并对其中的实体、关系或三元组进行任意的扰动，从而构造成一个图扰动数据集。文本、扰动后的图、以及校正描述（用于逆转施加的扰动）组成了一个验证数据集中的三元组，供验证模块进行自监督学习。随后，我们设计了微调(fine-tuning)和指令调优(instruction-tuning)来分别训练特定领域和通用的验证模块</p><p>在通过LLM生成T2G的过程中，验证器从LLM获取文本T和输出的图G，并向LLM发送校正信号（例如，“将文本转换为语义图，并将给定的三元组添加到生成的语义图”）。此过程将持续到验证模块验证输出是否正确为止，称之为Iterative Promoting。此外，验证器模块还有另一种（效率更高的）模式，它一开始调用LLM来以获得一个初始的图，然后剩下的校正步骤都会在验证模块中循环地以离线方式进行，我们称之为迭代离线校正(Iterative Offline Correction)</p><p>我们在3个图数据集上的实验结果表明，通过迭代地校正，PiVe框架的有效性在3个数据集上平均提升了26%。我们还表明，验证模块可以用作数据增强工具，以帮助创建高质量的并行文本和图数据集。我们还表明了如何利用PiVe作为一种数据增强技术，来提高自动构建的文本-图数据集的质量</p><h1 id="2023-AutoKG"><a href="#2023-AutoKG" class="headerlink" title="(2023)AutoKG"></a>(2023)AutoKG</h1><h2 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h2><p>知识图（KG）是一个由实体、概念和关系组成的语义网络，它可以催化各种场景的应用，如推荐系统、搜索引擎和问答系统。通常，KG构建由几个任务组成，包括：</p><ul><li>命名实体识别(Named Entity Recognition, NER)</li><li>关系提取(Relation Extraction, RE)</li><li>事件提取（Event Extraction, EE）</li><li>实体链接（Entity Link, EL）</li></ul><p>另一方面，KG推理，通常被称为链接预测(LP)，在理解这些KG中发挥着重要作用。此外，通过对与问题相关的关系子图进行推理，可以在问答(QA)任务中使用KG</p><p>早期的KG构建与推理通常以监督学习的方式进行。本文以ChatGPT和GPT-4为例，调研了LLM在KG构建与推理方面的潜力。本文贡献可以归纳为以下几点：</p><ul><li>使用8个数据集，通过评估LLM（包括GPT-3.5、ChatGPT、GPT-4）在KG构建和推理方面的zero-shot和one-shot表现，以对他们的能力有一个初步的了解</li><li>设计了一个全新的虚拟知识抽取(Virtual Knowledge Extraction)任务，构建了一个VINE数据集。通过评估LLM在这个数据集上的表现，本文证明了LLM具有强大的泛化能力</li><li>引入了新的概念AutoKG，使用通信代理(communicative agents)来自动地进行KG构建与推理。利用LLM的知识库，通过迭代的对话，让LLM的多个代理来协助KG的构建和推理过程</li></ul><h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><h3 id="大语言模型LLM"><a href="#大语言模型LLM" class="headerlink" title="大语言模型LLM"></a>大语言模型LLM</h3><p>LLM是在大量的文本数据上预先训练的，已经成为当代NLP研究的重要组成部分。NLP的进步导致了高性能LLM的开发，如GPT-3、ChatGPT和GPT-4，它们在各种NLP任务中表现出非凡的性能，包括机器翻译、文本求和和问答。同时，先前的几项研究表明，LLMs可以在相关的下游任务中取得显著的结果，而在提示中只需很少甚至没有演示。这为LLMs的复杂性和通用性提供了进一步的证据</p><h3 id="ChatGPT和GPT-4"><a href="#ChatGPT和GPT-4" class="headerlink" title="ChatGPT和GPT-4"></a>ChatGPT和GPT-4</h3><p>ChatGPT是OpenAI开发的一种高级LLM，主要用于进行类似人类的对话。在微调过程中，ChatGPT利用RLHF，从而增强其与人类偏好和价值观的一致性。</p><p>GPT-4是在GPT-3和ChatGPT等前辈的成功基础上发展起来的。它在规模空前的计算和数据上进行了训练，在各个领域都表现出了非凡的泛化、引用和解决问题的能力。此外，作为一个大规模的多模态模型，GPT-4能够处理图像和文本输入</p><p>越来越多的研究人员正在探索LLM所具有的特定的涌现能力和优势。Bang等人（2023）对ChatGPT在多任务、多语言和多时间方面进行了深入分析。研究结果表明，Chat-GPT在各种任务中的零样本(zero-shot)学习方面表现出色，甚至在某些情况下优于微调模型。然而，当将其推广到低资源语言时，它面临着挑战。此外，在多模态方面，与其他先进的视觉语言模型相比，ChatGPT的能力仍然较为有限</p><p>ChatGPT在其他各个领域也受到了相当大的关注，包括信息提取、推理、文本摘要、问答和机器翻译。这体现了了它在更广阔的NLP领域的通用性和适用性</p><h2 id="近期LLM在KG构建和推理方面的能力"><a href="#近期LLM在KG构建和推理方面的能力" class="headerlink" title="近期LLM在KG构建和推理方面的能力"></a>近期LLM在KG构建和推理方面的能力</h2><p>本文选择了代表性的ChatGPT和GPT-4，在八个不同数据集中，对他们在KG构建和推理中的表现进行了评估</p><h3 id="评估原则"><a href="#评估原则" class="headerlink" title="评估原则"></a>评估原则</h3><p>首先，分析这些模型在零样本(zero-shot)和单样本(one-shot )NLP任务中的能力。我们的主要目的是检查他们在有限数据下的泛化能力，以及他们在没有演示(demonstration)的情况下有效使用预训练的知识的能力</p><p>其次，根据评估结果，我们全面分析了导致模型在不同任务中表现不同的因素。我们目的在于探究他们在某些任务中表现优异的原因和潜在的缺点</p><h3 id="KG构建与推理"><a href="#KG构建与推理" class="headerlink" title="KG构建与推理"></a>KG构建与推理</h3><h4 id="实验设置（数据集）"><a href="#实验设置（数据集）" class="headerlink" title="实验设置（数据集）"></a>实验设置（数据集）</h4><ul><li>实体、关系、事件提取(Entity, Relation and Event Extraction)<ul><li>DuIE2.0：业界最大的基于模式的中文关系提取数据集，包括超过21万个中文句子和48个预定义的关系类别</li><li>SciERC：一个由七个关系注释的科学摘要集</li><li>Re-TACRED：是用于关系提取的TACRED数据集的一个显著增强版本，包含分布在40个关系中的91000多个句子</li><li>MAVEN：是一个通用的领域事件提取基准，包含4480个文档和168个事件类型</li></ul></li><li>链接预测(Link Prediction)<ul><li>FB15K-237：被广泛用作评估KG嵌入模型在链接预测方面性能的基准，包括237个关系和14541个实体</li><li>ATOMIC 2020：一个全面的综合常识库，包含133万个关于实体和事件的推理知识元组</li></ul></li><li>问答(Question Answering)<ul><li>FreebaseQA:一个基于Freebase知识图构建的开放域QA数据集，专门为知识图QA任务设计。包括来自各种来源的问答对，例如TriviaQA数据集等</li><li>MetaQA：是从WikiMovies数据集扩展而来的，提供了大量的单跳和多跳(hop)问答集，总数超过400000</li></ul></li></ul><h4 id="总体结果"><a href="#总体结果" class="headerlink" title="总体结果"></a>总体结果</h4><h5 id="实体和关系提取"><a href="#实体和关系提取" class="headerlink" title="实体和关系提取"></a>实体和关系提取</h5><p>本文在SciERC、Re-TACRED和DuIE2.0数据集上进行了实验，每个实验都涉及test/valid集中的20个样本，并使用标准微F1 Score进行评估。本文使用PaddleNLP，PL- Marker，EXOBRAIN作为基准模型来进行评估（也就是State-of-the-art, SOTA）</p><p>结果如表1所示，在零样本和单样本任务中，GPT-4在这些数据集上都展现了相对不错的性能，也比ChatGPT有了一些进步，但仍然比不过全监督的小模型</p><h6 id="零样本"><a href="#零样本" class="headerlink" title="零样本"></a>零样本</h6><p>GPT-4在零样本时的性能，展现出在3个数据集中都有进步，其中最显著的进步体现在DuIE2.0中。其中，GPT-4的Score为31.03，而ChatGPT的Score为10.3.。该结果进一步证明了GPT-4在提取复杂和新知识方面具有强大的泛化能力</p><p>如图2中的（1）和（2）所示，与ChatGPT相比，GPT-4在头尾(Head-to-tail)实体提取方面表现出更明显的进步。</p><p>在Re-TACRED的例句中，目标三元组是(Helen Keller Inter-national, org: alternate_names，HKI)。ChatGPT未能提取出这样的关系，可能是因为头部和尾部的位置非常接近，并且此时谓词较为模糊。而GPT-4成功提取了头部和尾部实体之间的“org:alternate_names”关系，完成了三重提取。这也在一定程度上证明了GPT-4增强了语言理解（阅读）能力</p><h6 id="单样本"><a href="#单样本" class="headerlink" title="单样本"></a>单样本</h6><p>同时，文本指令的优化有助于提高GPT-4的性能。结果表明，在单样本的方式中，引入一个训练样本可以进一步提高GPT-4和ChatGPT的三重提取能力</p><p>以DuIE2.0数据集为例。考虑以下句子：“George Wilcombe was selected for the Honduras national team in 2008, and he par- ticipated in the 2009 North and Central America and Caribbean Gold Cup with the team（乔治·威尔科姆于2008年入选洪都拉斯国家队，他曾与该队一起参加2009年北美、中美洲和加勒比金杯赛）”</p><p>相应的三元组应该是(George Wilcombe, Nationality, Honduras)。尽管文本中没有明确说明，但GPT-4成功地提取了这些信息。这不仅是因为我们提供了有价值的单个训练示例，也源于GPT-4的综合知识库——GPT-4根据乔治·威尔科姆加入了国家队这一事实，从而推断出他的国籍</p><p>尽管如此，但GPT-4的表现并不完美，而且在面对复杂的句子时表现不佳。需要注意的是，数据集噪声、类型歧义、复杂的句子上下文，都会导致这种结果。</p><p>此外，为了实现数据集上不同模型的公平、横向比较，实验没有在提示中指定提取实体的类型（这也可能会影响实验结果）。这一结果有助于解释为什么GPT-4在SciERC和Re-TACRED上的性能不如在DuIE2.0上的性能</p><h5 id="事件提取"><a href="#事件提取" class="headerlink" title="事件提取"></a>事件提取</h5><p>我们对MAVEN数据集中的20个随机样本进行了事件检测(Event Detection)实验。此外，王等人（2022a）被用作微调过的SOTA</p><p>值得注意的是，GPT-4在没有演示的情况下取得了值得称赞的成果。此处使用F-Score作为评估指标</p><h6 id="零样本-1"><a href="#零样本-1" class="headerlink" title="零样本"></a>零样本</h6><p>表1中的结果显示，GPT-4在零样本实验中表现得比ChatGPT要好。对于例句”Now an established member of the line-up, he agreed to sing it more often”，ChatGPT的结果是”Becoming_a_member”，而GPT-4确定了三种事件类型：”Becoming_a_member”, “Agree_or_refuse_to_act”和”Performing”</p><p>可以发现，ChatGPT的答案经常只有一种事件类型，而GPT-4更善于获取上下文信息，产生更多样的答案，提取更全面的事件类型。因此，GPT-4在MAVEN数据集上取得了优异的结果（该数据集包含具有一个或多个关系的集合）</p><h6 id="单样本-1"><a href="#单样本-1" class="headerlink" title="单样本"></a>单样本</h6><p>此时我们观察到，ChatGPT的性能显著提高，而GPT-4的性能略有下降。单个的演示能够纠正ChatGPT在零样本下做出的错误判断，从而提高其性能</p><p>我们便着重分析GPT-4的表现。如图3所示：</p><p>数据集中的事件类型为Process_end 和 Com_together。但是GPT-4会生成三个结果：Comparison、Earnings_and_losss和Ranking。在这个过程中，GPT-4确实注意到了句子中隐藏的排序和比较信息，但忽略了与Process_end和相对应的触发词final、以及Come_together相对应的触发器host。同时，在单样本下，当GPT-4无法正确识别类型时，它往往会产生更多的错误。这在一定程度上导致了GPT-4的性能下降</p><p>我们认为这可能是由于数据集中提供的类型不明确。此外，单句话中存在多个事件类型，这进一步增加了任务的复杂性，导致了不太好的结果</p><h5 id="链接预测"><a href="#链接预测" class="headerlink" title="链接预测"></a>链接预测</h5><p>链接预测在两个数据集FB15k-237和ATOMIC2020上进行实验。前者是一个包含25个实例的随机样本集，而后者包含代表所有可能关系的23个实例。SOTA中，对于FB15k-237数据集表现最好的是基于BERT的C-LMKE，对于ATOMIC2020数据集表现最好的是COMET</p><h6 id="零样本-2"><a href="#零样本-2" class="headerlink" title="零样本"></a>零样本</h6><p>在表2中，GPT-4在零样本时优于text-davinci-003和ChatGPT模型。值得注意的是，GPT-4在数据集FB15k-237上的Hits@1 Score表明，它达到了最先进的水平，超过了微调模型的性能。关于ATOMIC2020，尽管GPT-4超过了其他两个模型，但GPT-4的性能和微调过的SOTA在blue1 Score上仍存在相当大的差距</p><p>在零样本的响应中，在预测的链接中存在歧义的情况下，ChatGPT拒绝直接回答，并主动要求提供额外的上下文来解决这种歧义。这在GPT-4中并不常见，它倾向于直接提供答案。这一结果表明了它们推理和决策过程中的差异</p><h6 id="单样本-2"><a href="#单样本-2" class="headerlink" title="单样本"></a>单样本</h6><p>结果表明，单样本下的GPT-4在两个数据集上的表现都有所提升，有助于更准确地预测三元组的尾部实体</p><p>在图4的例子中，[MASK]对应的答案是”Primetime Emmy Award”。零样本时，GPT-4没能正确理解；但单样本时，他就能成功给出尾部的实体了</p><h5 id="问答"><a href="#问答" class="headerlink" title="问答"></a>问答</h5><p>我们基于两个问答数据集进行了评估：FreebaseQA和MetaQA。从每个数据集中随机抽取20个实例。对于MetaQA，它由具有不同跳数的问题组成，我们根据它们在数据集中的比例进行采样。我们对两个数据集使用的评估矩阵是AnswerExactMatch</p><h6 id="零样本-3"><a href="#零样本-3" class="headerlink" title="零样本"></a>零样本</h6><p>如表2所示，text-davinci-003、ChatGPT和GPT-4在FreebaseQA数据集上具有相同的性能，并且都比以前的全监督SOTA提高了16%。FreebaseQA数据集上的监督SOTA（Yuet等人，2022）提出了一种新的框架DECAF，它可以共同生成逻辑的形式和直接的答案。然而，GPT-4与text-davinci-003或ChatGPT相比没有任何优势。</p><p>至于MetaQA，在LLM和监督SOTA之间仍然存在很大的差距。监督SOTA模型（Madani和Joseph，2023）为T5-small配备了经典的逻辑编程语言，并以Prolog查询的形式表示问题，然后可以使用Prolog查询以编程方式回答查询。SOTA模型能够正确回答MetaQA测试数据集中的所有问题。</p><p>造成这种差距的一个可能原因可能是MetaQA中存在多个答案的问题，而FreebaseQA数据集的问题通常只有一个答案。此外，由于输入Token长度的限制，MetaQA数据集提供的知识图无法提供到LLM中，LLM只能依靠其内部知识进行多跳推理，因此LLM往往无法覆盖所有正确答案</p><p>不过，GPT-4在LLM中仍是最优的，分别比text-davinci-003和ChatGPT高了29.9分和11.1分。例如MetaQA在图4中的示例问题。回答这个问题需要一个多跳推理过程，包括将电影与其他电影的编剧联系起来，最后与上映年份联系起来。GPT-4能够正确回答这个问题，而ChatGPT和text-davinci-003未能提供正确的答案，体现了GPT-4在多跳问答任务中的卓越性能</p><h6 id="单样本-3"><a href="#单样本-3" class="headerlink" title="单样本"></a>单样本</h6><p>我们还从训练集中随机抽取一个示例，在单样本下进行实验。表2中的结果表明，只有text-davinci-003在提示下得到改良，而ChatGPT和GPT-4的性能都有所下降。这可以归因于臭名昭著的调整税(alignment tax)，模型牺牲了一些上下文学习能力，来与人类的反馈保持一致</p><h4 id="KG构建vs推理"><a href="#KG构建vs推理" class="headerlink" title="KG构建vs推理"></a>KG构建vs推理</h4><p>在KG构造和KG推理的实验中，LLM的推理能力通常优于构造能力。对于KG的构造，LLM在零样本和单样本上都没有超过SOTA的性能。这与之前在信息提取任务上进行的实验（Ma et al.，2023）一致，表明LLM在少样本条件下的信息提取并不理想</p><p>相反地，在KG推理任务中，所有LLM在单样本的条件下，以及GPT-4在零样本的条件下，都达到了SOTA的水准</p><p>我们对这种现象提出了几个可能的解释：</p><ul><li>首先，KG构建包含了对实体、关系、事件等的识别和提取，比推理任务更复杂；相反，以链接预测为代表的KG推理主要依赖于现有的实体和关系进行推理，任务相对简单。</li><li>其次，LLM在推理中的优异表现，可能是因为他们在预训练阶段就处理过了相关的知识</li></ul><h4 id="通用领域vs专业领域"><a href="#通用领域vs专业领域" class="headerlink" title="通用领域vs专业领域"></a>通用领域vs专业领域</h4><p>为了探索大模型在通用、专业领域下的性能差异，我们设计了不同知识领域的任务。我们把目光放在SciERC和Re-TACRED数据集上：</p><ul><li>SciERC专注于科学领域，包括大量与科学研究相关的文章摘要和实体/关系</li><li>相反，Re-TACRED数据集的目标是通用领域，包括各种实体和关系类型</li></ul><p>结果表明，与Re-TACRED数据集相比，ChatGPT和GPT-4在SciERC数据集上表现出相对较差的性能。尽管Re-TACRED中的关系类型范围更广，但类型的增加并不会导致性能的下降。这表明，LLM在辨识专业数据时仍然存在局限性，与全监督的SOTA相比，性能差距变得更加明显。</p><p>以GPT-4为例，引入单样本可将其在Re-TACRED上的性能从15.5显著提高到22.5。然而，单样本对GPT-4在SciERC上性能的提升较为一般。这说明，引入单样本对于模型在专业数据集上的表现的影响是有限的。随着样本数量的增加，可能会有更大的性能提升</p><p>我们认为，大模型在专业数据集上的性能不良，可能的原因是：</p><ul><li>它们主要在大规模的通用语料库上进行训练，而这些语料库可能没有包含足够的专业领域知识</li><li>此外，在专业数据集中，某些实体、关系或概念可能呈现长尾分布</li></ul><h3 id="讨论：为什么LLM在某些任务上的表现不尽人意？"><a href="#讨论：为什么LLM在某些任务上的表现不尽人意？" class="headerlink" title="讨论：为什么LLM在某些任务上的表现不尽人意？"></a>讨论：为什么LLM在某些任务上的表现不尽人意？</h3><p>上述结果表明，LLM能够提取各种类型和领域的知识，但尚未超过微调模型的性能，这与之前的研究结果一致（Wei et al.，2023b；Gao et al.，2021）。</p><p>影响评估结果的可能因素如下：</p><ul><li>数据集质量：以KG构建为例，数据中的噪声会导致某些类型的模糊（例如在关系提取中，数据集可能没有明确地提供头部和尾部的实体类型）。此外，数据集可能包含高度复杂的文本和不准确的标签</li><li>指令质量：指令的语义丰富度显著地影响了模型的性能。使用不同指令进行实验，从而确定最有效的指令，可能会提高性能（prompt工程研究的就是如何通过指令来充分利用模型的能力）。此外，在上下文学习（Dong et al.，2023）中整合相关的样本可以进一步提高性能。值得注意的是，Code4Construct（Wang et al.，2022b）证明，使用基于代码的提示可以提高模型在提取结构化信息方面的性能</li><li>评估方法：现有的评估方法可能不完全适合评估LLM的能力。例如，数据集的标签可能不包含所有的正确答案，但LLM可能会给出给定答案范围外的响应。此外，在涉及相同指称（如同义词）的情况下，答案也可能无法正确识别</li></ul><h3 id="讨论：LLM是记住了知识，还是真正具有泛化能力？"><a href="#讨论：LLM是记住了知识，还是真正具有泛化能力？" class="headerlink" title="讨论：LLM是记住了知识，还是真正具有泛化能力？"></a>讨论：LLM是记住了知识，还是真正具有泛化能力？</h3><p>从先前的实验中可以看出，大型模型善于从微小信息中快速提取结构化知识。这让我们想到一个关于LLM性能优势的理论来源的问题：这是由于在预训练阶段使用了大量的文本数据，使模型能够获得相关的知识，还是因为它们强大的推理和泛化能力？</p><p>为了研究这个问题，我们设计了一个虚拟知识提取任务，用于评估LLM泛化和提取陌生知识的能力。而当前的数据集无法满足我们的需求，所以我们引入了一种新的虚拟知识提取数据集——VINE</p><p>更加具体地说，我们构建了现实世界中不存在的实体和关系，并将它们组织成知识三元组。随后，我们通过指令来让模型提取这些虚拟的知识，提取的效果用于衡量LLM处理陌生知识的能力。我们基于Re-TACRED数据集的测试集构建了VINE。构建过程的主要思想是用看不见的实体和关系替换原始数据集中的实体和关系，从而创建独特的虚拟知识场景</p><h4 id="数据收集"><a href="#数据收集" class="headerlink" title="数据收集"></a>数据收集</h4><p>考虑到像GPT-4这样LLM的庞大训练集，我们很难找到它们不熟悉的知识。以截至2021年9月的GPT-4数据为基础，我们从《纽约时报》组织的两次竞赛中选择了参与者的部分回答作为数据来源，包括2022年1月和2023年2月举行的两项创造新单词的比赛</p><p>然而，由于上述竞赛的回答数量有限，为了增强数据的多样性，我们还通过随机生成字母序列来创建新词。我们生成了长度在7到9个字符之间的随机序列，并随机添加常用的名词后缀</p><h4 id="数据分析"><a href="#数据分析" class="headerlink" title="数据分析"></a>数据分析</h4><p>我们构建的数据集包括1400个句子、39个全新的关系和786个独一无二的实体，并确保每个关系类型至少有10个样本。此外，我们发现在Re-TACRED测试集中，某些关系类型的样本少于10个，于是我们从训练集中选择了相应类型的句子来进行弥补</p><h4 id="初步结果"><a href="#初步结果" class="headerlink" title="初步结果"></a>初步结果</h4><p>实验中，我们随机挑选了不同关系类型的10个句子来进行评估。在学习了相同关系类型的两个示例后，我们评估了Chat GPT和GPT-4在这十个测试样本上的性能</p><p>结果表明，在学习了一定数量的虚拟知识后，ChatGPT在虚拟知识提取中的表现显著低于GPT-4，而GPT-4能够根据指令准确地提取从未见过的实体和关系的知识。GPT-4成功提取了80%的虚拟三元组，而ChatGPT的准确率仅为27%</p><p>在图5所示的例子中，我们提供了由虚拟关系类型和虚拟头尾实体组成的三元组以及相应的示例。结果表明，GPT-4有效地完成了虚拟三元组的提取</p><p>因此我们初步得出结论，GPT-4表现出相对较强的泛化能力，可以通过指令快速获取新知识，而不仅仅依赖相关知识的记忆。相关的工作（Wei et al.，2023a）也证实了大型模型对指令具有异常强大的泛化能力</p><h2 id="未来机遇：自动的KG构建与推理"><a href="#未来机遇：自动的KG构建与推理" class="headerlink" title="未来机遇：自动的KG构建与推理"></a>未来机遇：自动的KG构建与推理</h2><p>近来LLM的成功仍然主要依赖于大量的人类输入来指导会话文本的生成。随着用户不断完善任务描述和要求，并通过ChatGPT建立对话上下文，该模型可以提供越来越精确和高质量的响应。然而，从模型开发的角度来看，这个过程仍然是劳动密集型和耗时的。因此，研究人员已经开始研究预测<strong>大型模型自主生成引导文本</strong>的潜力</p><p>例如，AutoGPT可以独立生成提示并执行事件分析、营销计划创建、编程和数学运算等任务。同时，李等人（2023）深入研究了通信代理之间自主合作的潜力，并提出了一种新的合作代理框架，称为角色扮演(Role-playing)。在此基础上，我们进一步探究：利用通信代理来完成KG构建和推理任务是否可行？</p><p>本实验中，我们使用了CAMEL中的角色扮演方法（Li et al.，2023）。AI助手被指定为顾问，AI用户被指定为KG领域专家。在收到提示和指定的角色分配后，”任务指定代理“提供详细的描述来将概念具体化。此后，AI助手和AI用户在多方设置中合作完成指定的任务，直到AI用户确认完成为止</p><h3 id="评论"><a href="#评论" class="headerlink" title="评论"></a>评论</h3><p>通过将人工智能和人类专业知识相结合，AutoKG可以快速构建特定领域的KG。该系统允许领域专家与LLM进行交互，从而通过专家知识和经验的交流，为构建特定领域的知识图创造一个协作环境</p><p>同时，由于这种人机协作，它可以提高大型语言模型在处理特定领域任务时的准确性。AutoKG不仅可以加快特定领域知识图的定制，还可以提高大型模型的透明度和代理间的互动</p><p>更准确地说，AutoKG有助于更深入地理解LLM的内部知识结构和操作机制，从而增强模型的透明度。此外，AutoKG可以部署为一个可操作的人机交互平台，实现人与模型之间的有效沟通和互动，从而提高模型在处理复杂任务时的效率和准确性</p><h3 id="局限性"><a href="#局限性" class="headerlink" title="局限性"></a>局限性</h3><p>尽管我们的方法带来了显著的进步，但它并非没有局限性，然而，它本身就是进一步探索和改进的机会：</p><ul><li>API的使用受到最大token限制：此限制会影响KGs的构建，因为如果超过此限制，任务可能无法正确执行</li><li>AutoKG目前在促进高效人机交互方面存在不足：在任务完全由机器自主执行的情况下，人类无法及时记录通信中发生的错误；相反，让人类参与机器通信的每一步都会大大增加时间和劳动力成本。因此，确定人为干预的最佳时刻至关重要</li><li>LLM的训练数据对时间敏感：未来的工作可能需要结合互联网上的检索，以弥补当前LLM在获取最新或特定领域知识方面的不足</li></ul><h2 id="结论与后续工作"><a href="#结论与后续工作" class="headerlink" title="结论与后续工作"></a>结论与后续工作</h2><p>本文中，我们试图初步研究LLM在KG构建、推理等任务中的性能，以GPT系列为例。虽然这些模型擅长于此类任务，但我们提出了一个问题：LLM在提取任务方面的优势是来自其庞大的知识库还是潜在的上下文学习能力？为了探索这一点，我们提出了一个虚拟知识提取任务，并创建了相应的数据集进行实验。结果表明，LLM的确拥有强大的上下文学习能力</p><p>此外，我们提出了一种创新方法，通过部署多个代理来完成KG的构建和推理。这不仅减少了劳动，还补偿了LLM在各个领域专业知识的不足，从而提高了LLM的性能。尽管这种方法仍然有一些局限性，但它为LLM的未来应用提供了一个新的视角</p><p>虽然我们的研究已经取得了一些结果，但它也有一定的局限性</p><ul><li>如前所述，无法访问GPT-4 API使我们不得不依赖交互式界面进行实验，这无疑增加了工作量和时间成本</li><li>此外，由于GPT-4的多模态功能目前还不可供公众使用，我们暂时无法深入研究其性能以及对多模式处理的贡献</li></ul><h1 id="2022-PL-Marker"><a href="#2022-PL-Marker" class="headerlink" title="(2022)PL-Marker"></a>(2022)PL-Marker</h1><h1 id="2022-Grapher"><a href="#2022-Grapher" class="headerlink" title="(2022)Grapher"></a>(2022)Grapher</h1><h2 id="Intro-1"><a href="#Intro-1" class="headerlink" title="Intro"></a>Intro</h2><p>关于先前KG的构建：</p><ul><li>一些方法考虑简单的图补全问题，给定一个不完全的三元组（如缺少其中一个实体），目标是通过生成缺失的实体、或对给定的候选集进行排序，来补全三元组。该方法的缺点是，它们仅限于通过局部的修改来扩展现有的KG，不适合构建整个KG</li><li>其他方法提出查询PLM，以提取所学的事实和常识。他们促使PLM像做完形填空一样来预测三元组中缺失的部分。同上，该方法只适用于局部KG修补</li></ul><p>对于构建整个KG：</p><ul><li>有使用GraphGNN来构建KG的。但其序列性和贪婪性可能无法带来最优的图架构</li><li>CycleGT是一个text-to-graph的无监督方法。首先依据现成的实体提取器来提取实体，然后通过分类器来预测关系。但它对外部模块的依赖破坏了系统的连续性，可能导致不好的结果</li><li>DualTKBe采用无监督循环损失来实现图文双向翻译。但他们的方法只适用于单句的KG生成，限制了对大型图的适用性</li><li>BT5提出利用PLM T5模型来线性化地生成KG，其中对象-谓词-主体三元组连接在一起，整个T2G问题被视为序列到序列的建模问题。这种方法的问题是，图线性化不是唯一的，并且由于图组件多次重复而效率低下，导致长序列和增加复杂性</li><li>MaMa中，实体和关系首先使用LM向前传递的注意力权重矩阵进行匹配。然后将它们映射到现有的KG模式以生成最终的图</li></ul><p>本文提出了一个新的KG构建架构。首先生成节点，然后生成节点之间的边：</p><h2 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h2><h3 id="节点生成"><a href="#节点生成" class="headerlink" title="节点生成"></a>节点生成</h3><p>以下是Grapher节点生成框架的选择过程</p><h4 id="Text-Nodes方法"><a href="#Text-Nodes方法" class="headerlink" title="Text Nodes方法"></a>Text Nodes方法</h4><p>给定文本输入，该模块的目标是生成一组唯一的节点。我们使用预训练的编码器-解码器语言模型(PLM)，例如说T5。通过PLM，我们可以将节点的生成描述为一个从<strong>序列到序列</strong>的问题，将文本输入转换为由特殊标记分离的节点序列，即，\<PAD\>NODE1\<NODE_SEP\>NODE2\<NODE_SEP\>NODE3\&lt;\s>，其中NODE_i表示一个或多个单词</p><p>如图3所示，该模块还提供了节点的特征，来为KG生成边做准备。由于每个节点可以有多个相关的单词，因此我们对生成的字符串进行贪婪解码，用分隔令牌&lt; NODE_SEP &gt;来分割节点，并对解码器最后一层的隐藏状态进行平均池化。注意，在实践中，我们提前设定了生成节点的最大数量，并用一个特殊的&lt; NO_NODE &gt;符号填充缺失的节点</p><h4 id="Query-Nodes方法"><a href="#Query-Nodes方法" class="headerlink" title="Query Nodes方法"></a>Query Nodes方法</h4><p>在上述方法中，生成的节点序列是有顺序的，而在一个图中的节点不必是有顺序的，可以同等地看待这些节点。因此，我们提出了第二种架构（灵感来自DETR图像对象检测方法）。如图4所示，我们首先学习节点的查询以获得节点特征，然后估计（节点的）排列，以与目标节点顺序保持一致</p><ul><li>可学习节点查询：解码器接收一组可学习节点查询作为输入，以嵌入矩阵的形式输入。我们还禁用因果masking，以确保Transformer能够同时处理所有查询。这与传统的编码器-解码器架构形成对比，传统的编码器架构通常在训练期间将目标序列与因果masking作为输入嵌入，或者在推理期间嵌入自生成序列。</li></ul><p>解码器的输出现在可以直接读取为N维的节点特征$F_n\in\mathbb{R}^{d\times N}$，并传递给预测器(LSTM或GRU)解码为节点逻辑(logits) $L_n\in \mathbb{R}^{S\times V\times N}$，其中S为生成的节点序列长度，V为词汇表大小。</p><ul><li>为了避免系统记住节点的顺序并使序列的顺序不变，我们将逻辑和特征排列为$L’_n(s)=L_n(s)P,\ F’_n(s)=F_n(s)P$，其中$s=1,…,S$，$P$是用二部匹配算法在目标节点和贪婪解码节点之间得到的置换矩阵。我们使用交叉熵损失作为匹配的损失函数。排列的节点特征$F’_n$现在与目标是齐次的，可以在边的生成阶段使用</li></ul><h3 id="生成边"><a href="#生成边" class="headerlink" title="生成边"></a>生成边</h3><p>随后使用上一步的节点特征进行边的生成，如图5。给定一对节点特征，预测头决定它们各自节点之间存在(或不存在)一条边。</p><ul><li>一种选择是使用类似于Query Nodes中的预测器（或生成器，LSTM或GRU）来生成边</li><li>另一种选择是使用分类器来预测边</li></ul><p>这两种选择各有利弊，具体选择取决于应用的领域</p><ul><li>生成器的优点：能够构造任何的边序列，包括在训练过程中未见过的边序列，但风险是，结果可能不会与目标边序列完全匹配。</li><li>另一方面，如果可能关系的集合是固定且已知的，那么分类器会更有效、准确；但是如果训练中对所有可能的边的覆盖有限（没有考虑到所有可能的边），则系统在推理过程中可能会误分类</li></ul><h3 id="不平衡的边分布"><a href="#不平衡的边分布" class="headerlink" title="不平衡的边分布"></a>不平衡的边分布</h3><p>我们观察到，由于我们需要检查所有节点对之间是否存在边，我们必须生成或预测多达$N^2$条边，其中N是节点的数量。当生成的节点之一是\<NO_NODE\>符号时，可以通过忽略他们之间的边来节省时间。当两点之间没有边时，用\<NO_EDGE\>符号来表示。</p><p>此外，由于通常实际边的数量很小而\<NO_EDGE\>很大，生成和分类任务对\<NO_EDGE\>符号是不平衡的。为了解决这个问题，我们提出了两种解决方案：</p><ul><li>修改交叉熵损失（针对分类器提出）：将交叉熵(CE)替换为焦点损失(Focal Loss, 主要用于解决类别不平衡问题)。它的主要思想是降低分类效果好的样本的CE的权重（在我们的实验中即\<NO_EDGE\>），增加容易误分类的样本的CE的权重，也就是写成如下的形式：<script type="math/tex">\begin{aligned}  CE(p,\ t)&=-log(p_t)\\  FL(p,\ t,\ \gamma)&=-(1-p_t)^\gamma log(p_t)  \end{aligned}</script><br>  其中，$p$表示一条边对应的概率分布，$t$表示目标类，$\gamma$是一个权重参数，当为0时所有的损失都相等。将这种损失应用于分类器非常直观，而对于生成器，我们先根据边序列的长度来累加（每条边的）预测概率，得到与$p_t$地位相等的量，然后将其应用到损失上。在实践中，我们观察到FL提高了分类器的准确性，而对于生成器的性能没有显着变化</li><li>改变训练范式（针对生成器提出）：通过使邻接矩阵变得稀疏来修改训练设置，以删除大部分的\<NO_EDGE\>边，手动地重新进行平衡，如图6所示。    这里，我们保留所有实际的边，只留下一些随机选择的\<NO_EDGE\>边（注意，这种修改只是为了提高训练效率，在推理过程中，系统仍然需要输出所有的边），因为它们的真实位置是未知的，如图5所示。在实践中，除了将准确率提高了10-20%之外，还观察到使用稀疏边比使用完全邻接矩阵的训练时间快了10%</li></ul><h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><ol><li><p>WebNLG+ corpus v3.0：是2020年WebNLG挑战赛的数据集，提供了两个任务：从一组RDF三元组(主语-谓语-宾语)生成文本，以及语义解析的反向任务，将文本描述转换为RDF三元组。本文使用文本到RDF任务上的算法，其统计信息如表1所示。</p><p> 每个三元组集合都与一个或多个文本相关联，因此当将一个三元组集合分配给所有（相关的）文本时，训练、开发和测试集的总大小如表1的第二行所示。数据由16个DBpedia类别组成：11个只在训练和开发部分出现，5个不可见的类别仅在测试部分出现</p><p> 我们对数据预处理，删除下划线和引号以减少数据中的噪声。此外，由于T5的tokenizer和WebNLG数据集的词汇覆盖范围不匹配，WebNLG中的一些字符在T5的词汇表中不存在，在tokenization的过程中被忽略。我们将数据规范化，把缺失的字符映射到最接近的可用字符，例如，将’ ø ‘转换为’ o ‘，或将’ ã ‘转换为’ a ‘</p> <p> WebNLG是一个相当小的数据集，所以我们使用TEKGEN数据集的数据来对其进行扩展。由于WebNLG不使用DBpedia模式，而TEKGEN基于Wikidata KG，因此我们将Wikidata的谓词近似地映射到DBpedia的谓词。我们只映射训练集中的谓词。例如，TEKGEN三元组(SUBJECT， ‘ location of formation ‘ OBJECT)被转换为WebNLG三元组(SUBJECT， ‘ foundationPlace ‘， OBJECT)。总的来说，我们向WebNLG数据集添加了17855对，使总训练集的大小扩展到了53,281。</p></li><li><p>TEKGEN：是将Wikidata KG与Wikipedia文本对齐而构建的大规模并行文本-图数据集，其统计数据如表2第一行所示。使用TEKGEN的主要困难是三元组的数据形式为 (SUBJECT PREDICATE1 OBJECT1, PREDICATE2 OBJECT2，…)用逗号分隔，而谓词有可能由多个词组成，那么多个单词的时候就可能无法判断该词是属于谓词还是subject/ object</p> <p> 然而，上述困难可以通过使用Wikidata的谓词列表和人工制作的启发式方法来解决，另外过滤掉包含超过7个谓词的三元组，三元组组件长度超过100个字符，</p><p> 这样做是为了和WebNLG数据的设置更接近，并降低评分函数的计算复杂性。数据集的最终统计信息显示在表2的第二行中。注意，为了进一步管理有限的计算资源，我们只在Dev和Test集的一半上评估结果</p></li></ol><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="设置"><a href="#设置" class="headerlink" title="设置"></a>设置</h3><ul><li>PLM：T5 large，参数量770M</li><li>learnable embedding matrix：$M\in \mathbb{R}^{H\times N}$，$H=1024$是T5的隐藏大小，$N=8$是一个图中可能节点的最大数量</li><li>node generation head：单层GRU解码器，$H_{GRU}=1024$，紧跟一个投射到大小为32,128的词汇表的线性变换</li><li>edge generation head：同上</li><li>一个图中可能边的最大数量：7</li><li>edge classification head：用ReLU定义全连接层，将输出投射到大小为407的边的类型空间</li><li>全参数微调，使用AdamW优化器，学习率为$10^{-4}$，$\beta$默认值为$[0.9,\ 0.999]$，权重衰减默认值为$10^{-2}$</li><li>batch_size：10（WebNLG-一个A100）； 100（TEKGEN-10个A100）</li></ul><ol><li>完成WebNLG的一个训练epoch大约需要5300步，每1000步进行一次验证，我们得到的模型大约在6-7小时内达到其最佳性能</li><li>对于TEKGEN，每个epoch大约需要56,000步，每1,000步进行一次评估，我们训练和验证模型进行25,000次迭代，大约需要6天的计算时间。</li></ol><h3 id="Baseline"><a href="#Baseline" class="headerlink" title="Baseline"></a>Baseline</h3><p>我们使用WebNLG 2020挑战赛排行榜上报告的最佳表现团队作为基准：</p><ul><li>Amazon AI (Shanghai) 是Text-to-RDF任务的冠军。他们使用一种简单的启发式方法，首先进行实体链接以匹配输入文本中出现的实体与DBpedia的实体，然后查询DBpedia数据库以提取它们之间的关系</li><li>BT5是第二位，使用T5模型以线性形式生成KG，其中 （object-谓词-subject）三元组被连接在一起，整个T2G问题被视为传统的序列到序列建模</li><li>第三名CycleGT，使用了一种无监督的方法进行文本到图和图到文本的生成，其中KB构建部分依赖于现成的实体提取模块来识别输入文本中的实体，以及多标签分类器来预测实体对之间的关系。</li><li>Stanford CoreNLP Open IE：是一种无监督方法，在测试集的输入文本部分上运行，以提取主题、关系和对象，以产生输出三元组，以提供WebNLG 2020挑战赛的基准性能</li><li>ReGen：利用T5和强化学习(RL)进行双向文本到图和图到文本的生成，与BT5类似，也遵循线性化的图表示方法。</li></ul><h3 id="指标"><a href="#指标" class="headerlink" title="指标"></a>指标</h3><p>为了对生成的图进行评分，我们使用了WebNLG 2020挑战赛的评估脚本，它计算了输出三元组相对于真实值的精度、召回率和F1 Score。使用基于命名实体评价的指标以不同的方式衡量准确率、召回率和F1值：</p><ul><li>Exact：结果应该精确匹配参考结果，而类型(主语、谓语、宾语)不重要</li><li>Partial：结果应该至少部分匹配参考结果，而类型(主语、谓语、宾语)是不相关的</li><li>Strict：结果应该完全匹配参考结果，元素类型(主语、谓语、宾语)也应该完全匹配</li></ul><h3 id="WebNLG实验结果"><a href="#WebNLG实验结果" class="headerlink" title="WebNLG实验结果"></a>WebNLG实验结果</h3><p>如上图所示，黑色粗体为top，蓝色为second。可以发现使用T Nodes比Q Nodes要好；C Edges比G Edges要好。（T Nodes更好的可能原因：当前模型和数据集较小）</p><p>在使用T Nodes、C Edges时，用不同的随机初始化设置来实验，以检验模型的变化。从表4中的结果可知，结果非常稳定</p><p>针对边分布不平衡的问题，由于C Edges使用方法是用FL代替CE，所以本文使用不同的$\gamma$来进行实验。结果表明当$\gamma=3$时的效果最好</p><p>图8暂时没读懂…</p><h3 id="TEKGEN实验结果"><a href="#TEKGEN实验结果" class="headerlink" title="TEKGEN实验结果"></a>TEKGEN实验结果</h3><p>如图，T Nodes仍然比Q Nodes要好，但G Edges与C Edges几乎完全一样（在较小的数据集WebNLG中，C Edges表现更好）。</p><p>如图，可以发现数据量的增加对于边的生成器帮助很大；同时到了20000步之后，两种方式的F1值都开始趋于收敛，说明数据量增加带来的帮助有限</p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>本文通过Grapher解决了从文本构建KG的问题，将KG的生成分割成了节点生成和边生成两个环节，并对于每个环节提出了不同的方法来实现。此外，本文还解决了边的分布不平衡的问题。结果表明Grapher在两个数据集上的表现都非常不错</p><h1 id="2021-PURE"><a href="#2021-PURE" class="headerlink" title="(2021)PURE"></a>(2021)PURE</h1><h2 id="Intro-2"><a href="#Intro-2" class="headerlink" title="Intro"></a>Intro</h2><p>之前对于NER和RE的工作采用流水线的方法，用一个模型做NER，另一个做RE。而最近较多人采用了联合两者的方法，它们认为这样能够更好地捕获实体与关系之间的联系。</p><p>本文也属于联合的方法，使用两个基于PLM的encoder（分别称为实体模型和关系模型），其中两者独立训练，关系模型仅依赖实体模型来提供输入。实体模型基于span- level的表示，关系模型基于给定spans pair的上下文表示</p><p>创新点：</p><ul><li>提出一个简单的端到端RE方法，独立训练NER和RE两个模型</li><li>对本方法高效的原因进行了分析，并发现，学习实体和关系的不同上下文表示，比共同学习它们更有效</li><li>对算法效率进行优化</li></ul><h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><p>我们把现有的联合提取方法分为两类：结构预测和多任务学习</p><p>其余相关工作见原文与参考：<a href="https://blog.csdn.net/lairongxuan/article/details/124831157">https://blog.csdn.net/lairongxuan/article/details/124831157</a></p><h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><p>如图。首先，实体模型预测出不同类型的实体，并插入标记来辨别主/宾和实体类型；随后关系模型对每个实体对的关系进行预测</p><blockquote><p>个人对于span的理解（不一定对）：即一个span可以包括任意多个token，但这些token都用来表示同一个意思，换句话说就是一个语义单元</p></blockquote><ol><li><p>实体模型</p><p>   本实体模型是基于span的。首先用PLM来获得每一个token的上下文表示，然后将其输入到FFN来预测实体的概率分布</p></li><li><p>关系模型</p><p>   通过输入的一对spans（即主语和宾语）来预测关系类型。之前的方法重复地使用span的（上下文）表示，来对关系进行预测，但这样就只捕获到了span周围的上下文信息，而忽略了这对spans之间的联系</p><p>   （例如：在对“MORPA”和“PARSER”之间的关系进行分类时，单词“is a”是很重要的，但是对于“MORPA”和“TEXT-TO-SPEECH”之间关系分类并不重要）</p><p>   所以此处作出的改进是：在句子中的subject span和object span的前后插入typed marker（就是一对标记），以此来强化主语、谓语及实体类型特征</p><p>   并且，在预测不同span pairs之间的关系时，采用不同的上下文表示能够获得更好的效果</p></li><li><p>跨句子的上下文</p><p>   本文将句子的长度扩展到一个固定的值，以此来获取更多的上下文信息</p></li></ol><h1 id="2021-Guo"><a href="#2021-Guo" class="headerlink" title="(2021)Guo"></a>(2021)Guo</h1><h2 id="Intro-3"><a href="#Intro-3" class="headerlink" title="Intro"></a>Intro</h2><p>现有的KG都是基于现代汉语的，对文言文的研究很少。中国古典文学蕴含着丰富的历史知识，而这些信息并没有得到充分利用。此外，由于文言文语法和语义的困难，高质量的标记数据集很少，这阻碍了文言文KGC (Knowledge Graph Construction)的进程</p><p>本文使用由NER和RE组成的PLM和流水线模型从大规模未标记的经典文献中提取知识，并构建了一个基于中国古典文献的高质量KG。主要贡献如下：</p><ul><li>提出了一种基于BERT的端到端流水线知识提取模型，可应用于中国古典文本中的知识提取，并在中国二十四史中取得了竞争性的结果</li><li>为了保证关系预测的高精度，我们结合NER阶段预测的实体类型信息，提出了一种改进的RE模型。实验表明，改进模型的性能优于基线模型</li><li>我们构建了一个高质量的文言文NER和RE标记数据集（CCNR），并对其进行了一系列实验。扩展实验表明，改进的RoBERTa和Guwen-BERT1（使用我们的RE模型）的性能优于原始模型</li><li>基于中国历史文献，我们构建了一个高质量的KG，并设计了一个可视化系统来显示CHL-KG并提供查询功能</li></ul><h2 id="相关工作-1"><a href="#相关工作-1" class="headerlink" title="相关工作"></a>相关工作</h2><h3 id="命名实体识别"><a href="#命名实体识别" class="headerlink" title="命名实体识别"></a>命名实体识别</h3><p>NER是NLP的基础，是信息提取的子任务，将非结构化文本中的命名实体进行定位，并分类为预定义的类别，例如人名和组织。Yu等集成了经典中文NER的BiLSTM-CRF模型，证明BERT具有较强的迁移学习能力</p><ul><li>基于规则的方法：可以自动发现和生成规则，Collins等人提出的DL Cotram是最具代表性的方法。然而，这些方法的可扩展性较差，无法识别未注册的单词</li><li>基于统计的机器学习方法：基于概率，HiddenMarkov模型（HMM）和条件随机场（CRF）改进了基于规则的方法，并将NER视为序列标记过程</li><li>基于深度学习的方法：传统的深度学习方法主要集中在CNN和RNN上。Collobert等人提出了两种网络结构来完成NER任务：窗口法和句子法。李等人将基于RNN的网络应用于中国生物医学NER任务。此外，Huang等人提出了一系列用于序列注释的神经网络模型，包括LSTM、双向LSTM（Bi-LSTM）、具有CRF层的LSTM和具有CRF层的Bi-LSTM，它们在各种NLP任务中表现良好</li></ul><p>近年来，基于transformer的端到端模型得到了广泛应用。</p><ul><li>Vaswani等人提出了transformer模型，对不同语言之间的中间语言表示进行编码和解码，以解决序列到序列的问题</li><li>谷歌提出的BERT已经在11个不同的NLP任务中实现了SOTA性能，成为NLP发展史上的一块基石</li><li>与BERT相比，RoBERTa动态调整了掩码机制，改进了预训练任务，并使用了更大的预训练语料库</li><li>ALBert还修改了BERT的预训练任务，并减少了训练参数以加快训练过程</li><li>百度提出的ERNIE充分结合了现有知识库中包含的结构化知识，并利用了语法和语义信息的优势</li></ul><h3 id="关系提取"><a href="#关系提取" class="headerlink" title="关系提取"></a>关系提取</h3><ul><li>联合RE模型：通过同时提取实体及其关系，联合RE模型避免了pipeline中误差的积累<ul><li>Katiyar等人首先使用深度Bi-LSTM序列注释方法进行联合实体提取，并通过添加约束和关系优化进一步提高了模型的准确性</li><li>郑等人提出了一种新的标注策略和关系标注方法。通过学习实体的位置信息，利用神经网络直接提取实体关系的类型信息、实体的角色信息和三元组</li></ul></li><li>基于流水线的RE模型：<ul><li>Hashimoto等人将RNN应用于语法树分析，并用额外的特征取代了单词依赖矩阵</li><li>曾等人提出使用CNN在单词和会话级别进行关系和特征提取</li><li>针对特定实体，王等人在CNN架构中增加了多层保持机制。</li><li>最近PLM被广泛提出和使用，例如BERT、GPT和ELMo。Christopoulou等人将BERT应用于关系表征，并提出了一种新颖的“匹配空白”预训练任务</li></ul></li></ul><h2 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h2><h3 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h3><p>如图。根据我们在原始文言文的非结构化文本中所定义的实体和关系，我们对《中国二十四史》的部分进行了人工注释。我们把高质量的标记过的数据作为训练集，利用基于BERT的pipeline模型来预测未标记部分的剩余实体和关系</p><p>随后对预测的结果进行手工修改，并与前一个版本相结合，得到下一个版本的数据集。经过多次迭代，我们将最终的数据集导入到Neo4j2中，并构建了一个KG和一个可视化平台</p><blockquote><p>个人理解：上图中的“Train”箭头反过来更好理解。就是用CCNR的数据集来训练BERT，BERT预测未打标签的数据，预测的结果通过人工修改后又加入到CCNR中</p></blockquote><h3 id="CCNR数据集-Classical-Chinese-NER-RE"><a href="#CCNR数据集-Classical-Chinese-NER-RE" class="headerlink" title="CCNR数据集 (Classical-Chinese-NER-RE)"></a>CCNR数据集 (Classical-Chinese-NER-RE)</h3><p>文言文与现代汉语在语法，短语和句法上有很多不同，因此，为了提取文言文的KG，需要将人工标注与模型预测进行结合、迭代</p><h4 id="NER-Dataset"><a href="#NER-Dataset" class="headerlink" title="NER Dataset"></a>NER Dataset</h4><p>NER数据集包含6个实体类别——人(PER)、地点(LOC)、组织(ORG)、工作(JOB)、书籍(BOO)和战争(WAR)。已命名实体的统计信息如表1所示</p><p>我们使用“BIO”标记方法对实体进行注释，将每个元素标记3类：“B-”表示该元素位于片段开头。“I-”表示该元素位于片段中间，“O”表示该元素不属于任何类型。如下图：</p><h4 id="RE-Dataset"><a href="#RE-Dataset" class="headerlink" title="RE Dataset"></a>RE Dataset</h4><p>RE数据集包含4886个例子，包括41种关系类型，可分为三类：亲属关系、个人信息和社会表现。我们删除了由少于20个实例组成的关系类型，并补充配对关系，以此来预处理和标准化数据集</p><p>我们将每个实体标记为关系的主体，一定距离内的其他实体标记为其对应的客体。如果两者之间存在真实关系，则将关系类型标记为“已知”，否则标注为“未知”，以加强模型的泛化能力</p><h3 id="NER模型"><a href="#NER模型" class="headerlink" title="NER模型"></a>NER模型</h3><p>BERT通过无监督学习的方法从大规模的未标记语料库中学习，并关注词语层面和句子层面的特征，增强词向量的语义表示。在NER阶段，我们使用了一个基于BERT的模型，该模型是由现代汉语语料库预先训练的</p><p>该模型为每个单词生成标记 (token)、分段 (segment)和位置嵌入 (position embeddings)，并将它们传送到默认情况下具有768个隐藏层的12个transfomer blocks中。以《北齐书》中的一个句子为例，NER模型的架构如图4</p><h3 id="RE模型"><a href="#RE模型" class="headerlink" title="RE模型"></a>RE模型</h3><p>使用PLM作为编码器，我们将实体类别信息作为输入特征，使模型能够更有效地学习实体之间的关系</p><p>RE模型如图5所示</p><p>此外，我们利用编码器，从实体对和句子中，得到了“池化过后的实体层面(pooled-entity-level)”以及“句子层面(sentence-level)”的特征。</p><p>对特征的线性变换、编码器的隐藏状态，都有利于捕获重要的隐藏特征。随后，将特征和相应的实体类型信息连接起来（也就是上图中上面那个大红框），通过一个全连接的网络，最后利用Softmax函数给出了相关分类结果</p><p>综上所述，我们将实体类型标签作为额外的输入添加到模型中，使RE模型更好地结合NER模型预测出来的实体对所提供的语义信息</p><h2 id="实验-1"><a href="#实验-1" class="headerlink" title="实验"></a>实验</h2><h3 id="设置-1"><a href="#设置-1" class="headerlink" title="设置"></a>设置</h3><h4 id="数据集与baselines"><a href="#数据集与baselines" class="headerlink" title="数据集与baselines"></a>数据集与baselines</h4><p>在NER阶段，我们在Modern Chinese、CCNR-3和CCNR-6三个数据集上，评估了BERT-base、RoBERTa-base、Guwen-BERT和BERT- ITM（一种基于BERT的增量训练方法）四种方法。注意，包含3种实体和6种实体的CCNR分别被命名为CCNR-3和CCNR-6</p><h4 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h4><ul><li>线性预热法的比例设置为10%</li><li>训练批次大小为32</li><li><p>使用默认参数 (β1 = 0.9，β2 = 0.99，ε = 1e−8) 的Adam优化器</p></li><li><p>NER阶段的初始学习率为5e−5</p></li><li><p>RE阶段的初始学习率为1e−5</p></li><li><p>NER阶段，权重衰减为0.01</p></li><li>RE阶段使用Adam优化器，没有权重衰减</li></ul><h4 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h4><p>在先前BERT-base的基础上，我们采用线性预热法，并使用Adam优化器对模型进行优化。我们以精度、召回率和F1为指标，在Tesla-V100 GPU上进行了实验</p><p>对于NER和RE阶段，我们将每个实例的最大长度指定为256，并将标点符号标记为截断位置，从而使模型充分考虑上下文信息</p><h3 id="NER-模型"><a href="#NER-模型" class="headerlink" title="NER 模型"></a>NER 模型</h3><p>“BOO”、“WAR”和“JOB”的实体数量少于其他类，我们选择数据集中频率最高的3种实体“PER”、“ORG”和“LOC”进行实验。除了Guwen-BERT，我们还测试了由现代汉语语料库预训练的BERT-base和RoBERTa-base，因为现代汉语标注数据集中的命名实体与上面选择的3个实体相同</p><p>在CCNR-3数据集上，Guwen-BERT优于RoBERTa-base和BERT- ITM。</p><p>此外，Guwen-BERT在CCNR-6数据集上的表现优于RoBERTa-base。因为Guwen-BERT通过大规模的文言文语料库进行预训练，结合了现代汉语的RoBERTa权重，因此将部分的现代汉语特征迁移到了文言文</p><h3 id="RE-模型"><a href="#RE-模型" class="headerlink" title="RE 模型"></a>RE 模型</h3><p>此处使用原始模型，以及经过改进后（也就是在输入中添加实体类型的tag）的RE模型进行实验。如图所示。</p><ul><li>可以观察到，将实体tag与pooled entity embeddings连接在一起，可以在所有模型上都提供改进，特别是在基于RoBERTa的模型上 (RoBERTa和Guwen-BERT)</li><li>还可以观察到，在语料库有限的情况下，添加标签信息有助于学习实体类别的语义特征</li><li>此外，改进后的Guwen-BERT取得了最好的性能，BERT-ITM也令人印象深刻。主要因为Guwen-BERT和BERT-ITM都是经过文言文语料库的预训练，对文言文语法和词汇有较全面的表征</li></ul><h2 id="中国历史文献KG"><a href="#中国历史文献KG" class="headerlink" title="中国历史文献KG"></a>中国历史文献KG</h2><p>略</p><h2 id="结论-1"><a href="#结论-1" class="headerlink" title="结论"></a>结论</h2><p>本文构建了高质量的NER和RE文言文标注数据集，并通过选择最佳的NER模型，对标注数据集进行自动扩充来增强pipeline模型</p><p>实验表明，结合语义实体信息的RE模型优于原本模型。在中国历史文献的基础上，我们进一步构建了一个KG，并开发了一个可视化平台来展示CHL-KG</p><h1 id="2020-Kumar"><a href="#2020-Kumar" class="headerlink" title="(2020)Kumar"></a>(2020)Kumar</h1><h2 id="Intro-4"><a href="#Intro-4" class="headerlink" title="Intro"></a>Intro</h2><p>本文提出了一种基于语言模型的提取实体间关系的方案，它从语法角度以及上下文角度来对实体之间的关系进行捕捉。之前的学者提出用RNN、LSTM、CNN等各种架构来进行序列建模或分类任务，这些模型也可以用于实体关系的分类。而在过去的几年里，语言模型在这一研究方向上占据了主导地位。本文展示了两种基于BERT的体系结构，主要贡献：</p><ul><li>提出了两种技术：<ul><li>训练一个完整的$(K\times 2+1)$分类器，其中$K$是关系种类的数量。保留了一个人工类(Others)来捕获存在实体但我们不感兴趣的句子</li><li>训练两个分类器，一个二分类器用于实体之间的方向预测，另一个$K$路分类器用于关系类预测，并进一步融合结果</li></ul></li><li>通过消融实验，我们得出了为微调语言模型（BERT）分类器提供实体的信息的最佳格式</li><li>我们在SemEval 2010 task 8数据集上获得了关系分类任务的SOTA结果，F1 Score分别为88.4%和89.41%，优于之前的所有结果</li></ul><h2 id="相关工作-2"><a href="#相关工作-2" class="headerlink" title="相关工作"></a>相关工作</h2><p>为了解决关系提取和知识挖掘的问题，从无监督到有监督学习，学者们进行了大量的研究</p><ul><li>Rink等人使用了人工制作的特征，这些特征捕捉了上下文、语义角色隶属关系以及实体之间可能预先存在的关系</li><li>深度学习方法 (Deep Learning) 也被用于学习实体关系中的长期依赖性。张等人和Socher等人分别将RNN和MVRNN模型应用于关系分类，实现了递归神经网络的变体</li><li>深度卷积神经网络 (Deep convolutional neural network) 也被用于使用卷积核提取词汇和句子级别的特征。Dos Santos等人提出使用CNN进行关系分类任务，该模型通过排名进行分类（CR-CNN）</li><li>一些方法在深度网络模型中使用注意力水平，以捕捉句子的特定片段来抽取关系。王等人提出了基于两个注意力水平的CNN模型，以更好地识别异质环境中的模式。Lee等人采用了基于多头实体意识的Bi-LSTM模型。Yu等人（2014）通过利用单词嵌入中句子级别和子结构的嵌入，使用依赖树和命名实体识别（NER），提出了基于因子的组合嵌入模型（FCM）</li></ul><p>最近很少有人探索像BERT这样的用于关系提取的PLM</p><ul><li>姚等人训练了一个知识图KG-BERT来训练实体的三元组和关系描述，作为BERT的输入</li><li>Soares等人提出了一种通过匹配空格来训练关系表示的方法，而无需知识图或人类注释器的任何监督</li><li>吴等人提出了实体嵌入以及来自BERT的句子编码，作为多层神经网络的输入进行分类</li></ul><p>尽管这些模型取得了良好结果，但简单有效的解决方案不需要人工制作特征、依赖解析或训练多个模型。在第4节中，我们通过实验表明，使用实体感知的原始文本输入对PLM进行微调可以简单地实现这一点，同时还可以显著提高F1 Score</p><h2 id="Methodology-1"><a href="#Methodology-1" class="headerlink" title="Methodology"></a>Methodology</h2><h3 id="System-Design"><a href="#System-Design" class="headerlink" title="System Design"></a>System Design</h3><p>如上方流程图所示。给定一篇文章，调用句子分词器（步骤1），然后实体识别器（步骤2）在句子中划分实体，如位置、个人或组织。在步骤3和4中，运行基于BERT的关系分类器，如果找到了类似 (acquisition, relative-of, located-in and works-at) 的关系，就生成二元实体关系三元组。一旦系统标记了实体关系三元组，它就会填充图数据库中的节点和链接，用于知识图表示（步骤5）</p><h3 id="感知实体的BERT-的模型架构"><a href="#感知实体的BERT-的模型架构" class="headerlink" title="感知实体的BERT 的模型架构"></a>感知实体的BERT 的模型架构</h3><p>我们对BERT模型进行了微调，来为后面的环节提供输入</p><h4 id="输入的表示"><a href="#输入的表示" class="headerlink" title="输入的表示"></a>输入的表示</h4><p>根据BERT的原始论文，特殊符号[CLS]是输入序列的第一个标记，结束符号为[SEP]。为了提供位置信息，我们插入了特殊符号#和$（不带间隙）来包围实体，从而使其成为实体感知的BERT分类器。给定一个带有标记实体$(h,\ t)$的句子$s={w_1,\ …,\ <e1>w_h</e1>,\ …<e2>w_t</e2>,\ …w_n}$，我们将其转换为$s={[CLS],\ w1,\ ..,\ #w_h#,\ …$w_t$,\ …w_n,\ [SEP]}$</p><h4 id="模型架构"><a href="#模型架构" class="headerlink" title="模型架构"></a>模型架构</h4><p>使用与特殊令牌([CLS])对应的最终隐藏状态作为分类任务的聚合序列表示</p><blockquote><p>这句话有点抽象，意思就是，如图中所示，用[CLS]在BERT中的最终状态来对关系进行分类</p></blockquote><p>在微调中引入的参数为$W\in R^{(K\times2+1)\times768}$，其中$K$为关系类型的数量。我们用$C$和$K$计算了一个典型的softmax loss，即$log(softmax(CW^T))$。关系分类的得分函数为：$F(r)=softmax(CW^T)$</p><h3 id="基于BERT的2模型架构"><a href="#基于BERT的2模型架构" class="headerlink" title="基于BERT的2模型架构"></a>基于BERT的2模型架构</h3><p>我们提出了另一种提取有向关系的方法：2-model BERT</p><p>我们在相同的数据集上，用上一节中类似的方式，训练了两个基于BERT的分类器。第一个分类器学习关系的种类，第二个分类器（在给定实体位置的条件下）学习关系的方向</p><p>如图所示，只有当关系分类器预测某个关系类，而不是人工类“Others”时，才考虑关系的方向。我们独立检查了这两个模型的性能。在SemEval 2010数据集上，关系分类模型获得了86.6%的F1 Score，而定向分类器获得了97%的F1 Score。此外，我们在结合关系和方向分类模型方面取得了比以往大多数工作更好的结果，并且与SOTA模型相当</p><h2 id="实验-2"><a href="#实验-2" class="headerlink" title="实验"></a>实验</h2><p>数据集：SemEval-2010 Task 8。包括8000个示例作为训练数据，2717个句子作为测试集。实体之间有九种类型的关系以及实体的划分。还有一种额外的关系类型“其他”（人工类）。此外，每个关系类型都存在方向性信息，因此关系类的总数为19</p><p>实验结果如下：（指标为Macro-F1 Score）</p><h3 id="消融实验与误差分析"><a href="#消融实验与误差分析" class="headerlink" title="消融实验与误差分析"></a>消融实验与误差分析</h3><h4 id="实体信息的影响"><a href="#实体信息的影响" class="headerlink" title="实体信息的影响"></a>实体信息的影响</h4><p>在不对实体进行标记的情况下进行实验。直观地说，实体标记对学习有很大帮助，因为BERT很大程度上基于多头自注意层</p><h4 id="实体标记-Entity-Markers-的影响"><a href="#实体标记-Entity-Markers-的影响" class="headerlink" title="实体标记(Entity Markers)的影响"></a>实体标记(Entity Markers)的影响</h4><p>实体标记种类的选择也有影响。实验中得出，符号标记，如$，#比字母标记表现更好。可能原因是围绕着实体的字母标记扭曲了语言模型学习的语言表示。此外，用符号标记而不用空格的标记比使用空格的标记性能更好</p><h4 id="训练轮数的影响"><a href="#训练轮数的影响" class="headerlink" title="训练轮数的影响"></a>训练轮数的影响</h4><p>在本实验中，微调BERT的最佳轮数为5</p><h4 id="误差分析：出现偏差的原因"><a href="#误差分析：出现偏差的原因" class="headerlink" title="误差分析：出现偏差的原因"></a>误差分析：出现偏差的原因</h4><p>大约80%的误分类，是错误地将关系预测为人工类“Others”、或是将Others类预测为其余18个类中的一个。可能的原因：</p><ul><li>一些被错误预测为“Others”类的例子，包含在训练数据中不存在的单词或短语（粗体）。导致模型可能无法获取上下文，并错误地预测为Others</li></ul><ul><li>其次，该模型可以通过注意力机制学习关注实体之间介词短语的关系种类。例如，“is part of”短语主要指示 (Component-Whole / Member-Collection) 的关系。但是，Other类中的一些例子也有这样的短语，混淆了模型</li></ul><h2 id="结论与未来工作"><a href="#结论与未来工作" class="headerlink" title="结论与未来工作"></a>结论与未来工作</h2><p>本文中，我们提出了两种模型结构来处理实体之间的关系。通过消融研究，我们发现符号标记无间隙地包围实体词是通过微调BERT为关系分类提供文本输入格式的最佳方式</p><p>我们在SemEval-2010任务8基准数据集上进行了实验。所提出的模型，在SemEval-2010任务8中分别获得了88.44%和89.41%的SOTA结果F1 Score</p><p>作为未来方向，我们打算探索多标签分类任务，用于句子中存在两个以上实体导致关系提取不明确的场景。本文的一个局限性是只提取了句子层面的关系，而关系也可以跨越句子。我们打算修改我们的系统来获取长期关系（跨句子的关系）。我们还期待着采用特定领域的自定义预训练模型，以提高系统的准确性</p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><blockquote><p>原文链接/引用：</p><p>(2023)PiVe: Prompting with Iterative Verification Improving Graph-based Generative Capability of LLMs <a href="https://arxiv.org/abs/2305.12392">https://arxiv.org/abs/2305.12392</a></p><p>(2023 AutoKG)LLMs for Knowledge Graph Construction and Reasoning-Recent Capabilities and Future Opportunities <a href="https://arxiv.org/abs/2305.13168">https://arxiv.org/abs/2305.13168</a></p><p>(2022 PL-Marker)- Deming Ye, Yankai Lin, Peng Li, and Maosong Sun. 2022. <a href="https://aclanthology.org/2022.acl-long.337">Packed Levitated Marker for Entity and Relation Extraction</a>. In <em>Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 4904–4917, Dublin, Ireland. Association for Computational Linguistics.</p><p>(2022)Grapher: Grapher: Multi-Stage Knowledge Graph Construction using Pretrained Language Models <a href="https://research.ibm.com/publications/grapher-multi-stage-knowledge-graph-construction-using-pretrained-language-models">https://research.ibm.com/publications/grapher-multi-stage-knowledge-graph-construction-using-pretrained-language-models</a></p><p>(2021 PURE)- Zexuan Zhong and Danqi Chen. 2021. <a href="https://aclanthology.org/2021.naacl-main.5">A Frustratingly Easy Approach for Entity and Relation Extraction</a>. In <em>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>, pages 50–61, Online. Association for Computational Linguistics.</p><p>(2021 Guo)<br> Guo, Q. <em>et al.</em> (2021). Constructing Chinese Historical Literature Knowledge Graph Based on BERT. In: Xing, C., Fu, X., Zhang, Y., Zhang, G., Borjigin, C. (eds) Web Information Systems and Applications. WISA 2021. Lecture Notes in Computer Science(), vol 12999. Springer, Cham. <a href="https://doi.org/10.1007/978-3-030-87571-8_28">https://doi.org/10.1007/978-3-030-87571-8_28</a></p><p>(2020 Kumar)<br> A. Kumar, A. Pandey, R. Gadia and M. Mishra, “Building Knowledge Graph using Pre-trained Language Model for Learning Entity-aware Relationships,” <em>2020 IEEE International Conference on Computing, Power and Communication Technologies (GUCON)</em>, Greater Noida, India, 2020, pp. 310-315, doi: 10.1109/GUCON48875.2020.9231227.</p><p><a href="https://blog.csdn.net/lairongxuan/article/details/124831157">https://blog.csdn.net/lairongxuan/article/details/124831157</a></p></blockquote>]]></content:encoded>
      
      
      <category domain="http://yunsaijc.top/categories/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/">文献阅读</category>
      
      
      <category domain="http://yunsaijc.top/tags/AI/">AI</category>
      
      <category domain="http://yunsaijc.top/tags/LLM/">LLM</category>
      
      <category domain="http://yunsaijc.top/tags/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/">知识图谱</category>
      
      
      <comments>http://yunsaijc.top/2023/10/10/14-%E4%B8%8EPLM%E6%9C%89%E5%85%B3%E7%9A%84%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E6%9E%84%E5%BB%BA%E7%9B%B8%E5%85%B3%E6%96%87%E7%8C%AE-%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Llama部署与运行初尝试</title>
      <link>http://yunsaijc.top/2023/10/09/12-Llama%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%BF%90%E8%A1%8C%E5%88%9D%E5%B0%9D%E8%AF%95/</link>
      <guid>http://yunsaijc.top/2023/10/09/12-Llama%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%BF%90%E8%A1%8C%E5%88%9D%E5%B0%9D%E8%AF%95/</guid>
      <pubDate>Mon, 09 Oct 2023 07:03:23 GMT</pubDate>
      
        
        
      <description>&lt;h2 id=&quot;下载&quot;&gt;&lt;a href=&quot;#下载&quot; class=&quot;headerlink&quot; title=&quot;下载&quot;&gt;&lt;/a&gt;下载&lt;/h2&gt;&lt;p&gt;克隆Llama的官方Github仓库：&lt;/p&gt;
&lt;figure class=&quot;highlight plaintext&quot;&gt;&lt;table&gt;&lt;tr</description>
        
      
      
      
      <content:encoded><![CDATA[<h2 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h2><p>克隆Llama的官方Github仓库：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/facebookresearch/llama.git</span><br></pre></td></tr></table></figure><p>进入<code>llama</code>文件夹，运行命令：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python -m  pip install -e . -i https://pypi.tuna.tsinghua.edu.cn/simple</span><br><span class="line"># -i 后的参数为临时换源</span><br></pre></td></tr></table></figure><br>这会自动执行当前目录下的<code>setup.py</code>文件，以安装当前项目</p><p>到<a href="https://ai.meta.com/llama/">Llama官网</a>进行申请，据说填写邮箱后会收到邮件，但一直没收到。所以到huggingface下载第三方下载好的权重：<a href="https://huggingface.co/NousResearch/Llama-2-7b-hf">https://huggingface.co/NousResearch/Llama-2-7b-hf</a>（这个已经是huggingface格式的了）</p><h2 id="运行"><a href="#运行" class="headerlink" title="运行"></a>运行</h2><p>光是下载就花了四五天，更新一下运行记录。使用中文模型<a href="https://github.com/ymcui/Chinese-LLaMA-Alpaca/">Chinese-LLaMA-Alpaca</a>的<code>scripts/inference/inference_hf.py</code>脚本来运行一下模型（注意安装好依赖包，如果出错可能是依赖包的版本不符合<code>requirements.txt</code>的问题）</p><p>运行命令：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">python /home/jc/workspace/exp/test_llama.py \</span><br><span class="line">--base_model /home/jc/workspace/llama/Llama-2-7b-hf \</span><br><span class="line">--with_prompt \</span><br><span class="line">--interactive</span><br></pre></td></tr></table></figure></p><p>其中<code>--base_model</code>为在下载阶段，下载下来的文件夹的路径。</p><p>运行结果如下：</p><img src="/2023/10/09/12-Llama%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%BF%90%E8%A1%8C%E5%88%9D%E5%B0%9D%E8%AF%95/1.png" class=""><p>初步的运行完成，就是简单备忘一下运行过程～</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><blockquote><p><a href="https://juejin.cn/post/7283803914646814720">https://juejin.cn/post/7283803914646814720</a></p><p><a href="https://zhuanlan.zhihu.com/p/648548363">https://zhuanlan.zhihu.com/p/648548363</a></p></blockquote>]]></content:encoded>
      
      
      <category domain="http://yunsaijc.top/categories/%E9%A1%B9%E7%9B%AE%E8%AE%B0%E5%BD%95/">项目记录</category>
      
      
      <category domain="http://yunsaijc.top/tags/AI/">AI</category>
      
      <category domain="http://yunsaijc.top/tags/LLM/">LLM</category>
      
      
      <comments>http://yunsaijc.top/2023/10/09/12-Llama%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%BF%90%E8%A1%8C%E5%88%9D%E5%B0%9D%E8%AF%95/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>综述&lt;A Survey of Large Language Models&gt;阅读笔记</title>
      <link>http://yunsaijc.top/2023/10/08/11-%E7%BB%BC%E8%BF%B0%3CA%20Survey%20of%20Large%20Language%20Models%3E%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</link>
      <guid>http://yunsaijc.top/2023/10/08/11-%E7%BB%BC%E8%BF%B0%3CA%20Survey%20of%20Large%20Language%20Models%3E%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</guid>
      <pubDate>Sun, 08 Oct 2023 11:47:08 GMT</pubDate>
      
        
        
      <description>&lt;blockquote&gt;
&lt;p&gt;原文链接：&lt;a href=&quot;https://arxiv.org/abs/2303.18223&quot;&gt;https://arxiv.org/abs/2303.18223&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;Intro&quot;&gt;&lt;a hre</description>
        
      
      
      
      <content:encoded><![CDATA[<blockquote><p>原文链接：<a href="https://arxiv.org/abs/2303.18223">https://arxiv.org/abs/2303.18223</a></p></blockquote><h2 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h2><p>为了让机器像人类一样阅读、写作和交流，语言建模(LM)是提高机器语言智能的主要方法之一。其原理是对词语序列的生成概率进行建模，以预测未来的、或缺失的单词的概率。</p><p>LM的发展可以分为4个阶段：</p><ul><li>统计语言模型SLM：基于统计学习方法开发。基本思想是基于马尔可夫假设来建立词频预测模型</li><li>神经语言模型NLM：通过神经网络（如RNN）来描述单词序列的概率</li><li>预训练语言模型PLM：确立了“预训练和微调”的学习范式</li><li>大语言模型LLM：通过对PLM的模型或数据大小进行扩展，能够提高下游任务的模型性能。与较小的PLM相比，LLM展现出了“涌现能力”</li></ul><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>通常，LLM指包含千亿（100B）或更多参数的Transformer语言模型。现有的 LLM 采用类似的 Transformer 架构和与小型语言模型 相同的预训练目标(如语言建模)</p><h3 id="LLM的扩展法则"><a href="#LLM的扩展法则" class="headerlink" title="LLM的扩展法则"></a>LLM的扩展法则</h3><p>KM扩展法则：是由Kaplan等人提出的，神经语言模型的性能与模型规模$(N)$、数据集规模$(D)$和训练计算量$(C)$之间的幂律关系：</p><script type="math/tex; mode=display">\begin{aligned}&L(N)=(\frac{N_c}{N})^{\alpha_N},\ \alpha_N\sim0.076,\ N_c\sim 8.8\times10^{13}\\&L(D)=(\frac{D_c}{D})^{\alpha_D},\ \alpha_D\sim0.095,\ D_c\sim 5.4\times10^{13}\\&L(C)=(\frac{C_c}{C})^{\alpha_C},\ \alpha_C\sim0.050,\ C_c\sim 3.1\times10^{8}\end{aligned}</script><p>其中，$L(\cdot)$为用nats表示的交叉熵损失，$c$为给定的计算预算。</p><h3 id="LLM的涌现能力"><a href="#LLM的涌现能力" class="headerlink" title="LLM的涌现能力"></a>LLM的涌现能力</h3><p>定义：在小型模型中不存在但在大型模型中产生的能力（是区别 LLM 与先前 PLM 的最显著特征之一）</p><p>LLM的三种典型涌现能力：</p><ul><li>上下文学习(ICL)：ICL 能力是由 GPT-3 正式引入的。假设已经为LLM提供了一个自然语言指令和/或几个任务演示，它可以通过完成输入文本的单词序列的方式来为测试实例生成预期的输出，而无需额外的训练或梯度更新（如：175B的 GPT-3 模型在一般情况下表现出强大的 ICL 能力）</li><li>指令遵循：通过使用自然语言描述的混合多任务数据集进行微调（称为指令微调），LLM 在未见过的以指令形式描述的任务上表现出色，具有更好的泛化能力</li><li>逐步推理：对于小型语言模型而言，通常很难解决涉及多个推理步骤的复杂任务，例如数学问题。然而，通过使用思维链(Chain-of-Thought, CoT)提示策略，LLM 可以通过利用包含中间推理步骤的提示机制来解决这类任务，从而得出最终答案。</li></ul><h3 id="LLM的关键技术"><a href="#LLM的关键技术" class="headerlink" title="LLM的关键技术"></a>LLM的关键技术</h3><p>LLM目前的角色就像一个通用、且有能力的学习者。导致其成功的关键技术包括：</p><ul><li><strong>扩展</strong>：如前面所说，Transformer 语言模型存在明显的扩展效应:更大的模型/数据规模和更多的训练计算通常会导致模型能力的提升</li><li><strong>训练</strong>：大规模使分布式训练成为必须；此外，优化技巧对于训练的稳定性和模型性能也非常重要</li><li><strong>能力引导</strong>：LLM 具备了解决通用任务的潜在能力，而当执行一些特定任务时，这些能力可能不会展示出来。设计合适的任务指令或具体的 ICL 策略可以激发这些能力</li><li><strong>对齐微调</strong>：LLM 的预训练数据中包含低质量的数据，它可能会生成有毒、偏见甚至有害的内容。因此，有必要使 LLM 与人类价值观保持一致。InstructGPT 使 LLM 能够按照期望的指令进行操作，它利用了基于人类反馈的强化学习技术，将人类纳入训练循环中；ChatGPT 采用类似于 InstructGPT 的技术，在产生高质量、无害的回答方面表现出很强的对齐能力。</li><li><strong>工具操作</strong>：LLM 基于海量纯文本语料库进行训练，因此在那些非文本的任务上表现不佳（如数字计算）。此外，它们的能力也受限于预训练数据（如无法获取最新信息）。为了解决这些问题，学者们提出利用外部工具来弥补 LLM 的不足（如，利用计算器进行准确计算、利用搜索引擎检索未知信息 ）。ChatGPT 已经实现了使用外部插件的机制，这种机制可以广泛扩展 LLM 的能力范围。</li></ul><h3 id="GPT系列模型的技术演进"><a href="#GPT系列模型的技术演进" class="headerlink" title="GPT系列模型的技术演进"></a>GPT系列模型的技术演进</h3><p>ChatGPT 基于功能强大的 GPT 模型开发，其对话能力得到了专门的优化，在社会上引起了广泛关注。</p><ul><li>2018～GPT-1：GPT 代表生成式预训练(Generative Pre-Training)。它是基于生成型的、仅解码器的 Transformer 架构开发的，并采用了无监督预训练和有监督微调的混合方法。它为 GPT 系列模型建立了核心架构，并确立了对自然语言文本进行建模的基本原则，即预测下一个单词</li><li>2019～GPT-2：采用了与 GPT-1 类似的架构，将参数规模增加到了 15 亿，并使用大规模的网页数据集 WebText 进行训练。它旨在通过无监督语言建模来执行任务，而无需使用标记数据进行显式微调。论文中提出，自然语言文本可以自然地用作为格式化输入、输出和 任务信息的统一方式，<strong>解决任务的过程可以被视为生成解决方案文本的单词预测问题</strong>。也就是说，每个 NLP 任务可以被视为基于世界文本的子集的单词预测问题，因此如果模型训练后具有足够能力以复原世界文本， 无监督语言建模可以解决各种任务</li><li>2020～GPT-3：它扩展了（几乎相同的）生成式预训练架构，将模型参数扩展到了175B。论文中正式介绍了ICL，它是以小样本或零样本的方式使用 LLM，可以指导 LLM 理解以自然语言文本的形式给出的任务。GPT-3 可以被视为从 PLM 到 LLM 进化过程中的一个重要里程碑。它通过实证证明，将神经网络扩展到大的规模可以大幅增加模型的能力</li></ul><p>此后，OpenAI从两个方面对GPT-3进行改进：</p><ol><li>使用代码进行训练，以解决GPT难以推理复杂任务（如代码、数学）的能力；</li><li>与人类进行对齐（人类偏好、道德等）</li></ol><p>接下来，LLM的重要里程碑就出现了：</p><ul><li>2022～ChatPGT：以类似 InstructGPT 的方式进行训练，但专门针对对话能力进行了优化</li><li>2023～GPT-4：将文本输入扩展到多模态信号。总体而言，相比 GPT-3.5，GPT-4 在解决复杂任务方面具有更强的能力</li></ul><h2 id="LLM资源"><a href="#LLM资源" class="headerlink" title="LLM资源"></a>LLM资源</h2><p>略</p><h2 id="预训练"><a href="#预训练" class="headerlink" title="预训练"></a>预训练</h2><h3 id="数据收集"><a href="#数据收集" class="headerlink" title="数据收集"></a>数据收集</h3><ol><li>数据来源可以广义地分为两种类型：</li></ol><ul><li>通用文本数据：如网页、书籍和对话文本等，具有规模大、多样性强且易于获取的特点，可以增强 LLM 的语言建模和泛化能力</li><li>专用文本数据：如多语言数据、科学数据和代码等，可以赋予 LLM 解决专用任务的能力</li></ul><ol><li>数据预处理（如消除噪声、冗余、无关和潜在有害的数据）会极大地影响 LLM 的能力和性能。预处理策略包括：</li></ol><ul><li>质量过滤：删除低质量数据。通常有两种方法<ul><li>基于分类器：训练分类器来筛选</li><li>基于启发式：设计一组规则来筛选，如去除某种语言的数据、利用困惑度来删除不自然的数据、利用统计特征来过滤、利用关键词来过滤</li></ul></li><li>去重</li><li>隐私去除：去除敏感的个人信息</li><li>分词：将原始文本分割成词序列作为LLM的输入（虽然已经有很多分词器，但使用专门为预训练语料库而设计的分词器更有效）</li></ul><ol><li>预训练数据影响LLM的因素：<ul><li>混合来源：数据源自不同的领域或场景，不同领域的数据比例不同，对LLM的能力影响也不同</li><li>数据数量</li><li>数据质量</li></ul></li></ol><h3 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h3><p>Transformer架构是当前LLM的标准骨干。目前的主流架构：</p><ul><li>编码器-解码器架构</li><li>因果解码器架构</li><li>前缀解码器架构</li></ul><p>其余架构上的不同还体现在：层标准化(Layer Norm)，激活函数，位置编码，注意力机制和偏置</p><p>常用的预训练任务：语言建模、去噪自编码</p><h3 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h3><p>略</p><h2 id="LLM的适配微调"><a href="#LLM的适配微调" class="headerlink" title="LLM的适配微调"></a>LLM的适配微调</h2><ul><li>指令微调：是在自然语言格式的实例(instance)集合上微调预训练后的 LLM 的方法。这种方法与有监督微调和多任务提示训练密切相关</li><li>对齐微调：为了避免模型出预期之外的行为（如编造虚假信息、追求不准确的目标，以及产生有害的、误导性的和有偏见的表达）</li></ul><p>为了减少训练的参数量，同时尽可能达到良好的微调效果，有以下几种参数的高效微调方法：</p><ul><li>适配器微调(Adapter tuning)：在 Transformer 模型中引入了小型神经网络模块（称为适配器）。微调过程中，适配器将根据特定的任务目标进行优化，而原始LM的参数保持不变。这样可以有效减少可训练参数的数量</li><li>前缀微调(prefix tuning)：在LM的每个 Transformer 层前添加了一系列前缀（即一组可训练的连续向量）。这些前缀向量具有任务的特异性，可以视为虚拟的 token 嵌入。被训练的参数只有前缀。</li><li>提示微调(prompt tuning)：在输入层中加入可训练的提示向量。即：给每个任务定义自己的prompt，拼接到数据上作为输入，训练的参数是prompt<blockquote><pre><code>可以被视为前缀微调的一种。前缀微调完全由自由参数组成，与真正的token不对应；而提示微调与任务/token 有关</code></pre></blockquote></li><li>低秩适配(LoRA)：通过添加低秩约束来近 似每层的更新矩阵，以减少适配下游任务的可训练参数</li></ul><p>LLM参数微调的实际应用：</p><ul><li>Alpaca-LoRA 是通过 LoRA 训练出的 Alpaca（一个经过微调的 70 亿 LLaMA 模型）的轻量级微调版本</li><li>PEFT 代码库已在 GitHub 上发布。它包括了几种广泛使用的高效微调方法，包括 LoRA/ AdaLoRA、前缀微调、P- Tuning和提示微调。此外，它支持多个语言模型，如 GPT-2 和 LLaMA</li></ul><h2 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h2><p>上下文学习、思维链提示。略</p><h2 id="能力评测"><a href="#能力评测" class="headerlink" title="能力评测"></a>能力评测</h2><p>略</p><h2 id="总结与未来方向"><a href="#总结与未来方向" class="headerlink" title="总结与未来方向"></a>总结与未来方向</h2><ul><li>理论与原理：可解释性</li><li>模型架构：如何构建 LLM 中更高效的 Transformer 变体</li><li>模型训练：开发更系统、经济的预训练方法</li><li>模型应用：在实际应用中微调的成本非常高，所以提示已成为使用 LLM 的主要方法。但是，<ul><li>设计提示需要大量人力。自动生成有效提示以解决各种任务将非常有用</li><li>其次，一些复杂任务（例如形式证明和数值计算）需要特定的知识或逻辑规则，这些规则可能无法用自然语言很好地表达或通过示例演示。因此，开发更具信息量和灵活性的任务格式化方法以进行提示非常重要</li><li>第三，现有的提示策略主要关注单轮性能。需要开发交互式提示机制（例如通过自然语言对话）来解决复杂任务<ul><li>安全与对齐：LLM会产生幻觉，也可能被有意的指令激发以产生有害的、有偏见的或有毒的文本，从而导致滥用风险</li><li>应用与生态：有可能会产生一个以 LLM 为支持的应用生态系统</li></ul></li></ul></li></ul>]]></content:encoded>
      
      
      <category domain="http://yunsaijc.top/categories/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/">文献阅读</category>
      
      
      <category domain="http://yunsaijc.top/tags/AI/">AI</category>
      
      <category domain="http://yunsaijc.top/tags/LLM/">LLM</category>
      
      
      <comments>http://yunsaijc.top/2023/10/08/11-%E7%BB%BC%E8%BF%B0%3CA%20Survey%20of%20Large%20Language%20Models%3E%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>AI方向直博学习日志</title>
      <link>http://yunsaijc.top/2023/10/07/10-%E5%AD%A6%E4%B9%A0%E6%97%A5%E5%BF%97/</link>
      <guid>http://yunsaijc.top/2023/10/07/10-%E5%AD%A6%E4%B9%A0%E6%97%A5%E5%BF%97/</guid>
      <pubDate>Sat, 07 Oct 2023 12:34:16 GMT</pubDate>
      
        
        
      <description>&lt;h2 id=&quot;2023年&quot;&gt;&lt;a href=&quot;#2023年&quot; class=&quot;headerlink&quot; title=&quot;2023年&quot;&gt;&lt;/a&gt;2023年&lt;/h2&gt;&lt;h3 id=&quot;10月&quot;&gt;&lt;a href=&quot;#10月&quot; class=&quot;headerlink&quot; title=&quot;10月&quot;&gt;&lt;/</description>
        
      
      
      
      <content:encoded><![CDATA[<h2 id="2023年"><a href="#2023年" class="headerlink" title="2023年"></a>2023年</h2><h3 id="10月"><a href="#10月" class="headerlink" title="10月"></a>10月</h3><h4 id="26日"><a href="#26日" class="headerlink" title="26日"></a>26日</h4><ul><li>尝试基于LLM的KG实验（以调用为主）</li><li>langchain：<a href="https://www.langchain.com.cn/getting_started/getting_started">https://www.langchain.com.cn/getting_started/getting_started</a></li><li></li></ul><h4 id="25日"><a href="#25日" class="headerlink" title="25日"></a>25日</h4><ul><li>论文阅读：\<A Frustratingly Easy Approach for Entity and Relation Extraction\></li><li>与老师交流，近一步确定研究重点与倾向</li><li>论文粗读：\<Streaming Social Event Detection and Evolution Discovery in Heterogeneous Information Networks\></li><li>论文阅读：\<A Comprehensive Survey on Automatic Knowledge Graph Construction\></li></ul><h4 id="24日"><a href="#24日" class="headerlink" title="24日"></a>24日</h4><ul><li>阅读Grapher代码</li><li>LLM+KG文章继续检索</li></ul><h4 id="23日"><a href="#23日" class="headerlink" title="23日"></a>23日</h4><ul><li>为什么LLM大多是decoder-only架构：encoder的双向注意力会存在低秩问题，这可能会削弱模型表达能力（<a href="https://kexue.fm/archives/9529">https://kexue.fm/archives/9529</a>）</li><li>阅读Grapher代码</li><li>跟李沐学AI</li></ul><h4 id="22日"><a href="#22日" class="headerlink" title="22日"></a>22日</h4><ul><li>训练Grapher模型</li></ul><h4 id="20日"><a href="#20日" class="headerlink" title="20日"></a>20日</h4><ul><li>RLHF和Instruction Tuning的关系：<ul><li>前者是通过人类反馈来训练一个奖励模型，用这个奖励模型去调整输出；</li><li>后者是指导模型理解指令并(以合理的方式)作出响应，而不是“仅仅”补全文本</li><li>两者可以一起使用，可以只用后者，也可以只用前者<br>（<a href="https://news.ycombinator.com/item?id=35925337">https://news.ycombinator.com/item?id=35925337</a>）</li></ul></li><li>前期调研总结整理(PPT)</li></ul><h4 id="19日"><a href="#19日" class="headerlink" title="19日"></a>19日</h4><ul><li>LSTM：基本的RNN难以解决长期依赖（<a href="https://zhuanlan.zhihu.com/p/123857569">https://zhuanlan.zhihu.com/p/123857569</a>）</li><li>Macro-F1：多分类问题中对多个类别求F1（<a href="https://blog.csdn.net/qq_43190189/article/details/105778058">https://blog.csdn.net/qq_43190189/article/details/105778058</a>）</li><li>学习率Warmup：使用较小学习率预热，减少刚开始训练时的震荡（<a href="https://zhuanlan.zhihu.com/p/452448670">https://zhuanlan.zhihu.com/p/452448670</a>）</li><li>前期调研总结整理</li></ul><h4 id="18日"><a href="#18日" class="headerlink" title="18日"></a>18日</h4><ul><li>残差连接：解决深度网络中的梯度消散问题；同时因为不确定某层的效果是否正向，保留部分上一层的信息（<a href="https://blog.csdn.net/qq_39852676/article/details/105886743">https://blog.csdn.net/qq_39852676/article/details/105886743</a>，<a href="https://zhuanlan.zhihu.com/p/422247863">https://zhuanlan.zhihu.com/p/422247863</a>）</li><li>MLP多层感知机：简单的全连接、线性+非线性架构，也属于神经网络（<a href="https://zhuanlan.zhihu.com/p/642537175">https://zhuanlan.zhihu.com/p/642537175</a>）</li><li>CNN结构加深记忆：<a href="https://blog.csdn.net/weixin_57643648/article/details/123990029">https://blog.csdn.net/weixin_57643648/article/details/123990029</a></li></ul><h4 id="17日"><a href="#17日" class="headerlink" title="17日"></a>17日</h4><ul><li>文献精读：\<Attention is all you need\></li><li>多头注意力的理解：<a href="https://www.zhihu.com/question/341222779">https://www.zhihu.com/question/341222779</a></li></ul><h4 id="16日"><a href="#16日" class="headerlink" title="16日"></a>16日</h4><ul><li>文献精读：\<Building Knowledge Graph using Pre-trained Language Model for Learning Entity-aware Relationships\></li><li>文献精读：\<Constructing Chinese Historical Literature Knowledge Graph Based on BERT\></li></ul><h4 id="15日"><a href="#15日" class="headerlink" title="15日"></a>15日</h4><ul><li>运行llama成功初尝试</li><li>阅读Grapher代码</li><li>pytorch-lightning：<a href="https://zhuanlan.zhihu.com/p/556040754">https://zhuanlan.zhihu.com/p/556040754</a></li></ul><h4 id="13日"><a href="#13日" class="headerlink" title="13日"></a>13日</h4><ul><li>文献继续精读：Grapher</li><li>卷积和池化的重新理解：不同的卷积核能够提取不同的特征信息（如边缘）或进行不同的变化（如锐化）；池化用于在保留主要特征的同时减少参数和计算量，防止过拟合（<a href="https://www.zhihu.com/question/49376084">https://www.zhihu.com/question/49376084</a>，<a href="https://zhuanlan.zhihu.com/p/78760534">https://zhuanlan.zhihu.com/p/78760534</a>）<blockquote><p>个人理解：池化是一个特殊形状和步长的卷积，相当于等比例缩小图片。不太应该翻译为“池化”，可以理解为把东西都聚到一个池子里的操作</p></blockquote></li><li>Llama代码简单浏览</li></ul><h4 id="12日"><a href="#12日" class="headerlink" title="12日"></a>12日</h4><ul><li>文献阅读：\<PiVe: Prompting with Iterative Verification Improving Graph-based Generative Capability of LLMs\></li><li>文献粗读：\<Packed Levitated Marker for Entity and Relation Extraction\>（看不太懂）</li><li>文献阅读：\<Grapher: Multi-Stage Knowledge Graph Construction using Pretrained Language Models\></li></ul><h4 id="11日"><a href="#11日" class="headerlink" title="11日"></a>11日</h4><ul><li>文献继续精读：\<LLMs for Knowledge Graph Construction and Reasoning:Recent Capabilities and Future Opportunities\></li><li>继续捣鼓LLM环境…（网站被墙，下载模型好难）</li><li>长尾分布：也就是数据量分布不均衡，少部分类别有大量样本，就好像少部分人有大量资产一样（<a href="https://zhuanlan.zhihu.com/p/422558527">https://zhuanlan.zhihu.com/p/422558527</a>）</li><li>上述文献总结</li><li>准确率(accuracy)，精确率(precision)，召回率(recall)与F1 score：<a href="https://zhuanlan.zhihu.com/p/405658103">https://zhuanlan.zhihu.com/p/405658103</a></li></ul><h4 id="10日"><a href="#10日" class="headerlink" title="10日"></a>10日</h4><ul><li>文献阅读：\<Knowledge Graphs: Opportunities and Challenges\></li><li>捣鼓LLM环境</li><li>文献阅读：\<LLMs for Knowledge Graph Construction and Reasoning:Recent Capabilities and Future Opportunities\></li><li>zero\one\few-shot：简单来说就是给了多少个学习样本（<a href="https://zhuanlan.zhihu.com/p/624793654">https://zhuanlan.zhihu.com/p/624793654</a>）</li></ul><h4 id="9日"><a href="#9日" class="headerlink" title="9日"></a>9日</h4><ul><li>RLHF和InstructGPT：RLHF可以分为3步：<ul><li>根据采集的SFT数据集对GPT-3进行有监督的微调(Supervised FineTune, SFT)</li><li>收集人工标注的对比数据，训练奖励模型(Reword Model, RM)</li><li>使用RM作为强化学习的优化目标，利用PPO算法微调SFT模型<br>InstructGPT是在GPT-3的基础上通过RLHF训练而来的<br>(<a href="https://zhuanlan.zhihu.com/p/637419868">LLM 系列超详细解读 (四)：InstructGPT：训练语言模型以遵从人类指令 - 知乎</a>，<a href="https://zhuanlan.zhihu.com/p/590311003">https://zhuanlan.zhihu.com/p/590311003</a>)</li></ul></li><li>ChatGPT和InstructGPT：是一对姐妹模型，他们在模型结构、训练方式上都完全一致，即都使用了指示学习(Instruction Learning)和人类反馈的强化学习(RLHF)来指导模型的训练，它们不同的仅仅是采集数据的方式上有所差异</li><li>文献阅读笔记</li><li>激活函数的再理解：<ul><li>在神经元中的位置，是输入与权重相乘并相加之后，输出之前</li><li>一般引入非线性因素，来增加神经网络模型的非线性</li><li>否则网络中全部是线性部件，线性的组合还是线性，与单独一个线性分类器无异，这样就做不到用非线性来逼近任意函数<br>（<a href="https://blog.csdn.net/hhhhhhhhhhwwwwwwwwww/article/details/116863273#:~:text=CNN%E5%9F%BA%E7%A1%80%E2%80%94%E2%80%94%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%201%201%E3%80%81%E4%BB%80%E4%B9%88%E6%98%AF%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%20%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%20%28Activation%20functions%29%E5%AF%B9%E4%BA%8E%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20%E6%A8%A1%E5%9E%8B%E5%8E%BB%E5%AD%A6%E4%B9%A0%E3%80%81%E7%90%86%E8%A7%A3%E9%9D%9E%E5%B8%B8%E5%A4%8D%E6%9D%82%E5%92%8C%E9%9D%9E%E7%BA%BF%E6%80%A7%E7%9A%84%E5%87%BD%E6%95%B0%E6%9D%A5%E8%AF%B4%E5%85%B7%E6%9C%89%E5%8D%81%E5%88%86%E9%87%8D%E8%A6%81%E7%9A%84%E4%BD%9C%E7%94%A8%E3%80%82%20%E5%AE%83%E4%BB%AC%E5%B0%86%E9%9D%9E%E7%BA%BF%E6%80%A7%E7%89%B9%E6%80%A7%E5%BC%95%E5%85%A5%E5%88%B0%E6%88%91%E4%BB%AC%E7%9A%84%E7%BD%91%E7%BB%9C%E4%B8%AD%E3%80%82,%E8%BF%99%E6%A0%B7%E5%B0%B1%E5%81%9A%E4%B8%8D%E5%88%B0%E7%94%A8%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%9D%A5%E9%80%BC%E8%BF%91%E4%BB%BB%E6%84%8F%E5%87%BD%E6%95%B0%E3%80%82%20...%204%204%E3%80%81%E5%B8%B8%E7%94%A8%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%20sigmoid%20%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%20%E5%87%BD%E6%95%B0%E7%9A%84%E5%AE%9A%E4%B9%89%E4%B8%BA%EF%BC%9A%20">CNN基础——激活函数</a>）</li></ul></li><li>Llama部署初尝试</li></ul><h4 id="8日"><a href="#8日" class="headerlink" title="8日"></a>8日</h4><ul><li>文献阅读：\<A Survey of Large Language Models\></li><li>Transformer架构（LLM建立的主要基础）：<a href="https://zhuanlan.zhihu.com/p/338817680">https://zhuanlan.zhihu.com/p/338817680</a></li><li>循环神经网络(RNN)：个人理解-就像在CNN中采用了密码学的 CBC/CFB/OFB工作模式一样，将上一轮结果反馈到下一轮，用于更好地处理前后文相关的序列信息（<a href="https://zhuanlan.zhihu.com/p/30844905">https://zhuanlan.zhihu.com/p/30844905</a>）</li><li>机器学习中的正则化(Regulation)：<a href="https://www.zhihu.com/question/20924039">https://www.zhihu.com/question/20924039</a></li><li>LLM微调学习：Hugging face Transformer文档初步阅读</li><li>实验环境初步搭建</li></ul><h4 id="7日"><a href="#7日" class="headerlink" title="7日"></a>7日</h4><ul><li>初步确定短期研究方向</li><li>监督学习：关键在于，数据中有无人工标注的标签（<a href="https://zhuanlan.zhihu.com/p/376931561">https://zhuanlan.zhihu.com/p/376931561</a>）</li><li>泛化(Generalization)：是指模型很好地拟合以前未见过的新数据（从用于创建该模型的同一分布中抽取）的能力（<a href="https://www.cnblogs.com/anliven/p/10264475.html">https://www.cnblogs.com/anliven/p/10264475.html</a>）</li><li>注意力机制：通过权重，将模型的注意力转移到重要的部位（<a href="https://zhuanlan.zhihu.com/p/379722366">https://zhuanlan.zhihu.com/p/379722366</a>）</li><li>欧氏空间的通俗理解：<a href="https://www.zhihu.com/question/27903807">https://www.zhihu.com/question/27903807</a></li><li>图神经网络(GNN)基本原理：<a href="https://blog.csdn.net/weixin_45884316/article/details/115751272">https://blog.csdn.net/weixin_45884316/article/details/115751272</a></li></ul>]]></content:encoded>
      
      
      <category domain="http://yunsaijc.top/categories/AI%E5%9F%BA%E7%A1%80/">AI基础</category>
      
      
      <category domain="http://yunsaijc.top/tags/%E5%AD%A6%E4%B9%A0%E6%97%A5%E5%BF%97/">学习日志</category>
      
      
      <comments>http://yunsaijc.top/2023/10/07/10-%E5%AD%A6%E4%B9%A0%E6%97%A5%E5%BF%97/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>AI中的线性代数（有空更新）</title>
      <link>http://yunsaijc.top/2023/10/05/9-AI%E4%B8%AD%E7%9A%84%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/</link>
      <guid>http://yunsaijc.top/2023/10/05/9-AI%E4%B8%AD%E7%9A%84%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/</guid>
      <pubDate>Thu, 05 Oct 2023 09:08:00 GMT</pubDate>
      
        
        
      <description>&lt;h2 id=&quot;基本符号&quot;&gt;&lt;a href=&quot;#基本符号&quot; class=&quot;headerlink&quot; title=&quot;基本符号&quot;&gt;&lt;/a&gt;基本符号&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;$A \in \mathbb{R}^{m\times n}$表示一个$m$行$n$列的实数矩阵(matrix)&lt;</description>
        
      
      
      
      <content:encoded><![CDATA[<h2 id="基本符号"><a href="#基本符号" class="headerlink" title="基本符号"></a>基本符号</h2><ul><li>$A \in \mathbb{R}^{m\times n}$表示一个$m$行$n$列的实数矩阵(matrix)</li><li>$x\in \mathbb{R}^n$表示一个有$n$个元素的向量(vector)。通常来说，一个$n$维向量指的是一个$n\times 1$的矩阵（即列向量(column vector)）。通过转置(transpose)可以表示对应的$1\times n$矩阵（即行向量(row vector)）</li><li>$x_i$表示向量$x$的第$i$个元素</li><li>$a<em>{ij}\ or\ A</em>{ij}$表示矩阵$A$中$i$行$j$列的元素</li><li>$a<em>j\ or\ A</em>{:,j}$表示矩阵$A$的第$j$列</li><li>$a<em>i^T\ or\ A</em>{i,:}$表示矩阵$A$的第$i$行</li></ul><h2 id="矩阵乘法-Matrix-Multiplication"><a href="#矩阵乘法-Matrix-Multiplication" class="headerlink" title="矩阵乘法(Matrix Multiplication)"></a>矩阵乘法(Matrix Multiplication)</h2><ul><li>矩阵$A \in \mathbb{R}^{m\times n}$和矩阵$B \in \mathbb{R}^{n\times p}$的乘积(product)是$C=AB\in \mathbb{R}^{m\times p}$，其中$C<em>{ij}=\sum^n</em>{k=1}A<em>{ik}B</em>{kj}$</li><li>性质<ul><li>结合律(associative)：$(AB)C=A(BC)$</li><li>分配律(distributive)：$A(B+C)=AB+AC$</li><li>不可交换(not commutative)：$AB\neq BA$</li></ul></li></ul><h2 id="操作与性质"><a href="#操作与性质" class="headerlink" title="操作与性质"></a>操作与性质</h2><h3 id="单位矩阵-Identity-Matrix-与对角矩阵-Diagonal-Matrix"><a href="#单位矩阵-Identity-Matrix-与对角矩阵-Diagonal-Matrix" class="headerlink" title="单位矩阵(Identity Matrix)与对角矩阵(Diagonal Matrix)"></a>单位矩阵(Identity Matrix)与对角矩阵(Diagonal Matrix)</h3><ul><li>单位阵<script type="math/tex">I_{ij}=\left\{\begin{matrix}   1,\ i=j\\0,\ i\neq j \end{matrix}\right.</script></li><li>满足对任意$A\in \mathbb{R}^{m\times n}$都有$AI=A=IA$</li><li>对角阵，通常记为$D=diag(d<em>1,d_2,…,d_n)$，有$$    D</em>{ij}=\left{\begin{matrix}<br>   D_i,\ i=j\0,\ i\neq j \end{matrix}\right.<br>  $$</li></ul><h3 id="转置-Transpose"><a href="#转置-Transpose" class="headerlink" title="转置(Transpose)"></a>转置(Transpose)</h3><ul><li>$(A^T)<em>{ij}=A</em>{ji}$</li><li>性质<ul><li>$(A^T)^T=A$</li><li>$(AB)^T=B^TA^T$</li><li>$(A+B)^T=A^T+B^T$</li></ul></li></ul><h3 id="对称矩阵-Symmetric-Matrix"><a href="#对称矩阵-Symmetric-Matrix" class="headerlink" title="对称矩阵(Symmetric Matrix)"></a>对称矩阵(Symmetric Matrix)</h3><ul><li>当$A\in \mathbb{R}^{n\times n}$且$A^T=A$，则$A$为对称阵；若$A=-A^T$则$A$为反对称阵(anti-symmetric)</li><li>对于任意$A\in \mathbb{R}^{n\times n}$，$A+A^T$是对称阵，$A-A^T$是反对称阵</li><li>因此任意方阵都可以表示为一个对称阵和一个反对称阵的和：$A=\frac{1}{2}(A+A^T)+\frac{1}{2}(A-A^T)$</li><li>对称阵在实践中非常常见，通常将所有大小为$n$（即$n\times n$）的对称阵的集合记为$\mathbb{S}^n$</li></ul><h3 id="迹-Trace"><a href="#迹-Trace" class="headerlink" title="迹(Trace)"></a>迹(Trace)</h3><ul><li>方阵$A\in \mathbb{R}^{n\times n}$的迹（记为$tr(A)$）是对角线上元素的和：<script type="math/tex">trA=\sum^{n}_{i=1}A_{ii}</script></li></ul><p>迹的性质：（以下矩阵均在$\mathbb{R}^{n\times n}$中讨论）</p><ul><li>$trA=trA^T$</li><li>$tr(A+B)=trA+trB$</li><li>$t\in \mathbb{R},\ tr(tA)=t\ trA$</li><li>$AB$是方阵，则$trAB=trBA$</li><li>$ABC$是方阵，则$trABC=trBCA=trCAB$，以此类推</li></ul><h3 id="范式-Norms"><a href="#范式-Norms" class="headerlink" title="范式(Norms)"></a>范式(Norms)</h3><ul><li>向量$x$的范式$||x||$，不正式地来讲，是该向量“长度”的度量。例如，常用的欧几里得范式/ $\ell<em>2$范式为：$$||x||_2=\sqrt{\sum</em>{i=1}^{n}x^2_i}$$<br>  注意，$||x||^2_2=x^Tx$</li></ul><p>正式地来讲，范式是一个满足以下4个条件的任意函数$f\ :\ \mathbb{R}^n\rightarrow\mathbb{R}$：</p><ul><li>非负性(non-negativity)：对于任意$x\in\mathbb{R}^n$，$f(x)\geq0$</li><li>确定性(definiteness, 不确定这个翻译对不对)：$f(x)=0$当且仅当$x=0$</li><li>齐次性(homogeneity)：对于任意$x\in\mathbb{R}^n,\ t\in\mathbb{R}$，$f(tx)=|t|f(x)$</li><li>三角不等式(triangle inequality)：$x,\ y\in\mathbb{R}^n,\ t\in\mathbb{R}$，$f(x+y)\leq f(x)+f(y)$</li></ul><p>常用的范式如：</p><ul><li>$\ell<em>1$范式：$||x||_1=\sum^n</em>{i=1}|x_i|$</li><li>$\ell<em>{\infty}$范式：$||x||</em>\infty=max_i|x_i|$</li></ul><p>事实上，以上的3种范式属于$\ell<em>p\ (p\geq1)$范式，它的定义为：$$||x||_p=(\sum</em>{i=1}^{n}|x_i|^p)^\frac{1}{p}$$</p><p>矩阵也可以定义范式，例如Frobenius范式：$$<br>||A||<em>F=\sqrt{\sum^m</em>{i=1}\sum^n<em>{j=1}A^2</em>{ij}}=\sqrt{tr(A^TA)}</p><p>$$<br>还存在其他的矩阵范式，此处不再叙述</p><h3 id="线性相关-Linear-Independence-与秩-Rank"><a href="#线性相关-Linear-Independence-与秩-Rank" class="headerlink" title="线性相关(Linear Independence)与秩(Rank)"></a>线性相关(Linear Independence)与秩(Rank)</h3><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><blockquote><p><a href="https://www.yanxishe.com/TextTranslation/2965">https://www.yanxishe.com/TextTranslation/2965</a></p><p><a href="https://cs229.stanford.edu/section/cs229-linalg.pdf">https://cs229.stanford.edu/section/cs229-linalg.pdf</a></p></blockquote>]]></content:encoded>
      
      
      <category domain="http://yunsaijc.top/categories/AI%E5%9F%BA%E7%A1%80/">AI基础</category>
      
      
      <category domain="http://yunsaijc.top/tags/AI/">AI</category>
      
      <category domain="http://yunsaijc.top/tags/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/">线性代数</category>
      
      
      <comments>http://yunsaijc.top/2023/10/05/9-AI%E4%B8%AD%E7%9A%84%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>信息安全数学基础总结（有空更新）</title>
      <link>http://yunsaijc.top/2023/10/03/8-%E4%BF%A1%E6%81%AF%E5%AE%89%E5%85%A8%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E6%80%BB%E7%BB%93/</link>
      <guid>http://yunsaijc.top/2023/10/03/8-%E4%BF%A1%E6%81%AF%E5%AE%89%E5%85%A8%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E6%80%BB%E7%BB%93/</guid>
      <pubDate>Tue, 03 Oct 2023 05:48:59 GMT</pubDate>
      
        
        
      <description>&lt;p&gt;大二第一学期，学院开设了《信息安全数学基础》这门课。但由于当时对信息安全的理解还不够深入，再加上已经过去了两年之久，很多知识都理解得不好，且印象模糊。借着大四这个相对空闲的时间，对学过的知识进行复习和总结。&lt;/p&gt;
&lt;h2 id=&quot;初等数论&quot;&gt;&lt;a href=&quot;#初等数论&quot;</description>
        
      
      
      
      <content:encoded><![CDATA[<p>大二第一学期，学院开设了《信息安全数学基础》这门课。但由于当时对信息安全的理解还不够深入，再加上已经过去了两年之久，很多知识都理解得不好，且印象模糊。借着大四这个相对空闲的时间，对学过的知识进行复习和总结。</p><h2 id="初等数论"><a href="#初等数论" class="headerlink" title="初等数论"></a>初等数论</h2><h3 id="整数的因子分解"><a href="#整数的因子分解" class="headerlink" title="整数的因子分解"></a>整数的因子分解</h3><h4 id="整除"><a href="#整除" class="headerlink" title="整除"></a>整除</h4><ul><li>$a,\ b\in \mathbb{Z},\ b\neq 0,\quad 若\ \exists\ q,\ 使得\  a=qb,\ 则\ b\mid a\ (b整除a),\ 否则\ b\nmid a$</li><li>$b$称为$a$的因子，$a$称为$b$的倍数</li><li>性质<ul><li>传递性：$c\mid b,\ b\mid a,\ then\ c\mid a$</li><li>$b\mid a,\ then\ bc\mid ac$</li><li>$c\mid a,\ c\mid b,\ then\ c\mid ma+nb$</li></ul></li></ul><h4 id="Euclid-欧几里得-除法"><a href="#Euclid-欧几里得-除法" class="headerlink" title="Euclid(欧几里得)除法"></a>Euclid(欧几里得)除法</h4><ul><li>$a,\ b\in \mathbb{Z},\ b&gt;0,\ 则存在唯一的整数对q,r\ 使得\ a=qb+r,\ 0\leq r&lt;b$</li><li>$q$称为不完全商，$r$称为余数（最小非负余数）</li><li>若调整$q$（一般是加1）使得$|r|\leq \frac{b}{2}$，则$r$为绝对值最小余数（在Euclid算法中起到加速的作用）</li><li>可用于求整数的$a$进制表示  <img src="/2023/10/03/8-%E4%BF%A1%E6%81%AF%E5%AE%89%E5%85%A8%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E6%80%BB%E7%BB%93/1.png" class=""></li></ul><h4 id="公因子与最大公因子"><a href="#公因子与最大公因子" class="headerlink" title="公因子与最大公因子"></a>公因子与最大公因子</h4><ul><li>设$a,a_2,…,a_n$是$n(n\geq2)$个整数，若整数$d$是它们中每一个数的因数，则$d$称$a,a_2,…,a_n$的一个公因子</li><li>若$a,a_2,…,a_n$不全为零，则$a,a_2,…,a_n$的所有公因子中最大的一个称为最大公因子，记为$(a,a_2,…,a_n)$</li><li>特别地，当$(a,a_2,…,a_n)=1$，称$a,a_2,…,a_n$互素/互质</li><li>定义$(0,a)=a$，因为任何非零整数都是$0$的因数</li></ul><p>辗转相除法/Euclid算法 求最大公因子</p><ul><li>预备定理：$a,b,r是不全为0的整数,\ 若a=qb+r,\ q\in\mathbb{Z},\ 则(a,b)=(b,r)$  <img src="/2023/10/03/8-%E4%BF%A1%E6%81%AF%E5%AE%89%E5%85%A8%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E6%80%BB%E7%BB%93/2.png" class=""></li><li>将上述辗转相除的过程反过来写（即把余数放左边，用$a,b$来表示余数），可以得到最大公因子的线性表示，即贝祖等式</li><li>贝祖等式：对任意两个正整数$a,b$，存在整数$x,y使得(a,b)=xa+yb$</li><li><p>当$(a,b)=1$，可以求出唯一解$x$，即$xa\equiv 1(mod\ b),\ x\equiv a^{-1}(mod\ b)$</p></li><li><p>扩展/广义Euclid算法：求出最大公因子的同时求出系数$x,y$（主要用于求乘法逆元）</p></li><li><p>伪代码</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Input: 非负整数a, b且a&gt;=b (先将待计算的整数取绝对值)</span><br><span class="line">Output: r=(a,b) 以及满足 sa+tb=(a,b)的s,t</span><br><span class="line">Extended Euclid(a,b)&#123;</span><br><span class="line">(r,s,t) &lt;- (a,1,0);</span><br><span class="line">(r&#x27;,s&#x27;,t&#x27;) &lt;- (b,0,1);</span><br><span class="line">While r&#x27;!=0 do&#123;</span><br><span class="line">q &lt;- floor(r/r&#x27;);</span><br><span class="line">(tmp1,tmp2,tmp3) &lt;- (r-qr&#x27;,s-qs&#x27;,t-qt&#x27;);</span><br><span class="line">(r,s,t) &lt;- (r&#x27;,s&#x27;,t&#x27;);</span><br><span class="line">(r&#x27;,s&#x27;,t&#x27;) &lt;- (tmp1,tmp2,tmp3);</span><br><span class="line">&#125;</span><br><span class="line">Return r,s,t;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>最大公因子等价定义：$a,b$是不全为零的整数，则$d=(a,b)$是集合${xa+yb|x,y\in \mathbb{Z}}$中的最小正整数</p></li><li>应用-方程有解的判定：$a,b$是不全为零的整数，则方程$ax+by=c有整数解\Leftrightarrow (a,b)\mid c$<blockquote><p>个人理解：对于整数$a,b$，他们的最大公因子就是使用这两个数能得到的最小度量</p><p>换句话说，即长度为$a,b$的两把尺子可以度量的最小长度</p></blockquote></li></ul><h4 id="素数"><a href="#素数" class="headerlink" title="素数"></a>素数</h4><ul><li>埃拉托色尼素数筛选法-快速计算$1$到$N$之间的所有素数：遍历$1$到$\sqrt{N}$，将其中所有素数的倍数去掉，剩下的便都是素数了</li><li>素性定理：$p$为素数，$a,b$为整数，若$p\mid ab$，则$p\mid a$或$p\mid b$ ^291cce</li><li>算数基本定理：任意整数$n\ (n&gt;1)$都可以分解为有限个素数的乘积$n=p_1p_2…p_s$，该分解除了素数因子的排列之外，是唯一的</li><li>唯一因子分解定理：任意整数$n\ (n&gt;1)$可以唯一地表示成$n=p_1^{\alpha_1}p_2^{\alpha_2}…p_s^{\alpha_s},\ \alpha_i &gt;0$，其中$p_1p_2…p_s$为素数$p_i&lt;p_j\ (i&lt;j)$。上式叫做$n$的标准分解式。</li></ul><p>整数的一些性质</p><ul><li>$a=p_1^{\alpha_1}p_2^{\alpha_2}…p_s^{\alpha_s},\ b=p_1^{\beta_1}p_2^{\beta_2}…p_s^{\beta_s}$，那么$(a,b)=p_1^{\gamma_1}p_2^{\gamma_2}…p_s^{\gamma_s},\ \gamma_i=min(\alpha_i,\beta_i)$，最小公倍数$[a,b]=p_1^{\delta_1}p_2^{\delta_2}…p_s^{\delta_s},\ \delta_i=max(\alpha_i,\beta_i)$</li><li>$a<em>1,a_2,…a_n$为$n$个非零整数，令$[a_1,a_2]=m_1,\ [m_1,a_3]=m_2,…,[m</em>{n-2},a<em>n]=m</em>{n-1}$，则$[a<em>1,a_2,…a_n]=m</em>{n-1}$</li><li>除了2以外，所有素数都是奇素数</li></ul><h4 id="多项式的整除性"><a href="#多项式的整除性" class="headerlink" title="多项式的整除性"></a>多项式的整除性</h4><ul><li>令$\mathbb{Q}={\frac{a}{b}|a,b\in \mathbb{Z},b\neq0}$表示全体有理数的集合。$\mathbb{Q}$上有加减乘除四则运算</li><li>令$\mathbb{Q}[x]={a_0+a_1x+…+a_nx^n|a_i\in\mathbb{Q},0\leq i\leq n}$表示所有系数为有理数的多项式集合。$\mathbb{Q}[x]$有加减乘，但没有除法</li><li>可以发现$\mathbb{Q}[x]$与整数集合$\mathbb{Z}$有很多类似的性质：都有带余除法、最大公因子、唯一因子分解定理等</li></ul><p>（多项式$f(x)$的次数表示为$deg\ f(x)$，以下多项式均在$\mathbb{Q}[x]$内讨论）</p><ul><li>$g(x)\neq 0$，则有$q(x),r(x)$使得$f(x)=q(x)g(x)+r(x),\ r(x)=0或r(x)\neq 0,deg\ r(x) &lt; deg\ g(x)$</li><li>$r(x)=0$时，称$g(x)整除f(x)$，记$g(x)|f(x)$，$g(x)$称为$f(x)$的因子</li><li>当$g(x)$为$f(x)$的因子，且$deg\ g(x) &lt; deg\ f(x)$时，$g(x)$称为$f(x)$的真因子</li><li>当$f(x)$没有真因子时，称为不可约多项式</li></ul><blockquote><p>个人理解：把函数的符号$f,g$当成整数的符号来看，多项式的整除与整数的整除几乎相同，只是多了$x$而已</p><p>因此其余的不再赘述：</p><p>多项式整除性质——参考整数整除性质</p><p>多项式最大公因子——参考整数最大公因子</p><p>多项式最大公因子的表示——参考整数最大公因子的表示（贝祖等式）</p><p>多项式最大公因子的求法——参考整数的辗转相除法</p><p>不可约多项式的有关定理——参考整数的素性定理</p><p>多项式的唯一分解定理——参考整数的唯一分解定理</p></blockquote><h3 id="同余式"><a href="#同余式" class="headerlink" title="同余式"></a>同余式</h3><h3 id="二次剩余"><a href="#二次剩余" class="headerlink" title="二次剩余"></a>二次剩余</h3><h2 id="抽象代数"><a href="#抽象代数" class="headerlink" title="抽象代数"></a>抽象代数</h2><h3 id="群"><a href="#群" class="headerlink" title="群"></a>群</h3><h3 id="环"><a href="#环" class="headerlink" title="环"></a>环</h3><h3 id="域"><a href="#域" class="headerlink" title="域"></a>域</h3><h3 id="有限域"><a href="#有限域" class="headerlink" title="有限域"></a>有限域</h3><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2>]]></content:encoded>
      
      
      <category domain="http://yunsaijc.top/categories/%E7%BD%91%E7%BB%9C%E7%A9%BA%E9%97%B4%E5%AE%89%E5%85%A8%E5%9F%BA%E7%A1%80/">网络空间安全基础</category>
      
      
      <category domain="http://yunsaijc.top/tags/%E6%95%B0%E5%AD%A6/">数学</category>
      
      <category domain="http://yunsaijc.top/tags/%E6%95%B0%E8%AE%BA/">数论</category>
      
      <category domain="http://yunsaijc.top/tags/%E6%8A%BD%E8%B1%A1%E4%BB%A3%E6%95%B0/">抽象代数</category>
      
      
      <comments>http://yunsaijc.top/2023/10/03/8-%E4%BF%A1%E6%81%AF%E5%AE%89%E5%85%A8%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E6%80%BB%E7%BB%93/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>关于HTTP反向代理的理解</title>
      <link>http://yunsaijc.top/2023/09/27/7-%E5%85%B3%E4%BA%8EHTTP%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86%E7%9A%84%E7%90%86%E8%A7%A3/</link>
      <guid>http://yunsaijc.top/2023/09/27/7-%E5%85%B3%E4%BA%8EHTTP%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86%E7%9A%84%E7%90%86%E8%A7%A3/</guid>
      <pubDate>Wed, 27 Sep 2023 05:48:59 GMT</pubDate>
      
        
        
      <description>&lt;h2 id=&quot;正向代理-Forward-Proxy&quot;&gt;&lt;a href=&quot;#正向代理-Forward-Proxy&quot; class=&quot;headerlink&quot; title=&quot;正向代理(Forward Proxy)&quot;&gt;&lt;/a&gt;正向代理(Forward Proxy)&lt;/h2&gt;&lt;p&gt;正向代理</description>
        
      
      
      
      <content:encoded><![CDATA[<h2 id="正向代理-Forward-Proxy"><a href="#正向代理-Forward-Proxy" class="headerlink" title="正向代理(Forward Proxy)"></a>正向代理(Forward Proxy)</h2><p>正向代理通常直接称为代理(<code>Proxy</code>)，也就是日常情景下使用的各种<code>BrupSuite、梯子</code>等的代理，需要我们手动进行配置。</p><h3 id="代理的作用"><a href="#代理的作用" class="headerlink" title="代理的作用"></a>代理的作用</h3><ul><li>便于安全审计：所有流量经过一台代理服务器，那么安全审计就容易得多</li><li>加速访问/节省带宽：代理服务器可以返回缓存好的内容，无需向外网发出请求，从而加快访问速度、节省带宽</li><li>保护个人信息：使用代理隐藏自己的真实IP地址</li><li>突破访问限制：有时自己的地址被禁止访问，可以连接白名单上的代理服务器来进行访问</li></ul><h2 id="反向代理-Reverse-Proxy"><a href="#反向代理-Reverse-Proxy" class="headerlink" title="反向代理((Reverse Proxy)"></a>反向代理((Reverse Proxy)</h2><p>个人理解：类似于计算机网络中的NAT(网络地址转发)技术。用户并不知道自己被“反向代理”了，反向代理服务器(如<code>Nginx</code>)接收到请求后，转发给Web服务器(如<code>Apache</code>)进行处理。</p><p>为了方便阅读，此处开始用<code>Nginx</code>来指代<code>反向代理服务器</code>。</p><h3 id="反向代理作用"><a href="#反向代理作用" class="headerlink" title="反向代理作用"></a>反向代理作用</h3><ul><li>负载均衡：访问压力大时，<code>Nginx</code>可以将请求分配给多个不同的Web服务器进行处理，而用户访问的地址仍然是相同的地址(即<code>Nginx</code>服务器的地址)</li><li>提高安全性：Web服务器对外不可见，此时<code>Nginx</code>相当于一个防火墙</li><li>节省IP资源：如上述对反向代理的个人理解所说，类似于NAT，能够节省IP资源</li><li>加速访问：<code>Nginx</code>同样可以缓存网页内容</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>可以理解为：</p><ul><li>正向代理是面向用户的代理</li><li>反向代理是面向服务器的代理，对用户透明（也就是说把服务器也当成用户，服务器在给用户返回请求时自己挂了一个代理）</li></ul><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><blockquote><p><a href="https://zhuanlan.zhihu.com/p/464965616">https://zhuanlan.zhihu.com/p/464965616</a></p><p><a href="https://www.zhihu.com/question/388303662">https://www.zhihu.com/question/388303662</a></p><p><a href="https://zhuanlan.zhihu.com/p/486180243">https://zhuanlan.zhihu.com/p/486180243</a></p></blockquote>]]></content:encoded>
      
      
      <category domain="http://yunsaijc.top/categories/%E5%AD%A6%E4%B9%A0%E6%9D%82%E8%AE%B0/">学习杂记</category>
      
      
      <category domain="http://yunsaijc.top/tags/Web/">Web</category>
      
      
      <comments>http://yunsaijc.top/2023/09/27/7-%E5%85%B3%E4%BA%8EHTTP%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86%E7%9A%84%E7%90%86%E8%A7%A3/#disqus_thread</comments>
      
    </item>
    
  </channel>
</rss>
