<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"
  xmlns:atom="http://www.w3.org/2005/Atom"
  xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Yunsaijc&#39;s Blog</title>
    <link>http://yunsaijc.top/</link>
    
    <atom:link href="http://yunsaijc.top/rss2.xml" rel="self" type="application/rss+xml"/>
    
    <description>Enjoy</description>
    <pubDate>Wed, 11 Oct 2023 08:03:24 GMT</pubDate>
    <generator>http://hexo.io/</generator>
    
    <item>
      <title>&lt;LLMs for Knowledge Graph Construction and Reasoning: Recent Capabilities and Future Opportunities&gt;阅读笔记</title>
      <link>http://yunsaijc.top/2023/10/10/14-%3CLLMs%20for%20Knowledge%20Graph%20Construction%20and%20Reasoning-Recent%20Capabilities%20and%20Future%20Opportunities%3E%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</link>
      <guid>http://yunsaijc.top/2023/10/10/14-%3CLLMs%20for%20Knowledge%20Graph%20Construction%20and%20Reasoning-Recent%20Capabilities%20and%20Future%20Opportunities%3E%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</guid>
      <pubDate>Tue, 10 Oct 2023 09:28:12 GMT</pubDate>
      
        
        
      <description>&lt;blockquote&gt;
&lt;p&gt;原文链接：&lt;a
href=&quot;https://arxiv.org/abs/2305.13168&quot;&gt;https://arxiv.org/abs/2305.13168&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;intro&quot;&gt;Intro&lt;</description>
        
      
      
      
      <content:encoded><![CDATA[<blockquote><p>原文链接：<ahref="https://arxiv.org/abs/2305.13168">https://arxiv.org/abs/2305.13168</a></p></blockquote><h1 id="intro">Intro</h1><p>知识图（KG）是一个由实体、概念和关系组成的语义网络，它可以催化各种场景的应用，如推荐系统、搜索引擎和问答系统。通常，KG构建由几个任务组成，包括：</p><ul><li>命名实体识别(Named Entity Recognition, NER)</li><li>关系提取(Relation Extraction, RE)</li><li>事件提取（Event Extraction, EE）</li><li>实体链接（Entity Link, EL）</li></ul><p>另一方面，KG推理，通常被称为链接预测(LP)，在理解这些KG中发挥着重要作用。此外，通过对与问题相关的关系子图进行推理，可以在问答(QA)任务中使用KG</p><p>早期的KG构建与推理通常以监督学习的方式进行。本文以ChatGPT和GPT-4为例，调研了LLM在KG构建与推理方面的潜力。本文贡献可以归纳为以下几点：</p><ul><li>使用8个数据集，通过评估LLM（包括GPT-3.5、ChatGPT、GPT-4）在KG构建和推理方面的zero-shot和one-shot表现，以对他们的能力有一个初步的了解</li><li>设计了一个全新的虚拟知识抽取(Virtual KnowledgeExtraction)任务，构建了一个VINE数据集。通过评估LLM在这个数据集上的表现，本文证明了LLM具有强大的泛化能力</li><li>引入了新的概念AutoKG，使用通信代理(communicativeagents)来自动地进行KG构建与推理。利用LLM的知识库，通过迭代的对话，让LLM的多个代理来协助KG的构建和推理过程</li></ul><h1 id="相关工作">相关工作</h1><h2 id="大语言模型llm">大语言模型LLM</h2><p>LLM是在大量的文本数据上预先训练的，已经成为当代NLP研究的重要组成部分。NLP的进步导致了高性能LLM的开发，如GPT-3、ChatGPT和GPT-4，它们在各种NLP任务中表现出非凡的性能，包括机器翻译、文本求和和问答。同时，先前的几项研究表明，LLMs可以在相关的下游任务中取得显著的结果，而在提示中只需很少甚至没有演示。这为LLMs的复杂性和通用性提供了进一步的证据</p><h2 id="chatgpt和gpt-4">ChatGPT和GPT-4</h2><p>ChatGPT是OpenAI开发的一种高级LLM，主要用于进行类似人类的对话。在微调过程中，ChatGPT利用RLHF，从而增强其与人类偏好和价值观的一致性。</p><p>GPT-4是在GPT-3和ChatGPT等前辈的成功基础上发展起来的。它在规模空前的计算和数据上进行了训练，在各个领域都表现出了非凡的泛化、引用和解决问题的能力。此外，作为一个大规模的多模态模型，GPT-4能够处理图像和文本输入</p><p>越来越多的研究人员正在探索LLM所具有的特定的涌现能力和优势。Bang等人（2023）对ChatGPT在多任务、多语言和多时间方面进行了深入分析。研究结果表明，Chat-GPT在各种任务中的零样本(zero-shot)学习方面表现出色，甚至在某些情况下优于微调模型。然而，当将其推广到低资源语言时，它面临着挑战。此外，在多模态方面，与其他先进的视觉语言模型相比，ChatGPT的能力仍然较为有限</p><p>ChatGPT在其他各个领域也受到了相当大的关注，包括信息提取、推理、文本摘要、问答和机器翻译。这体现了了它在更广阔的NLP领域的通用性和适用性</p><h1id="近期llm在kg构建和推理方面的能力">近期LLM在KG构建和推理方面的能力</h1><p>本文选择了代表性的ChatGPT和GPT-4，在八个不同数据集中，对他们在KG构建和推理中的表现进行了评估</p><h2 id="评估原则">评估原则</h2><p>首先，分析这些模型在零样本(zero-shot)和单样本(one-shot)NLP任务中的能力。我们的主要目的是检查他们在有限数据下的泛化能力，以及他们在没有演示(demonstration)的情况下有效使用预训练的知识的能力</p><p>其次，根据评估结果，我们全面分析了导致模型在不同任务中表现不同的因素。我们目的在于探究他们在某些任务中表现优异的原因和潜在的缺点</p><h2 id="kg构建与推理">KG构建与推理</h2><h3 id="实验设置数据集">实验设置（数据集）</h3><ul><li>实体、关系、事件提取(Entity, Relation and Event Extraction)<ul><li>DuIE2.0：业界最大的基于模式的中文关系提取数据集，包括超过21万个中文句子和48个预定义的关系类别</li><li>SciERC：一个由七个关系注释的科学摘要集</li><li>Re-TACRED：是用于关系提取的TACRED数据集的一个显著增强版本，包含分布在40个关系中的91000多个句子</li><li>MAVEN：是一个通用的领域事件提取基准，包含4480个文档和168个事件类型</li></ul></li><li>链接预测(Link Prediction)<ul><li>FB15K-237：被广泛用作评估KG嵌入模型在链接预测方面性能的基准，包括237个关系和14541个实体</li><li>ATOMIC2020：一个全面的综合常识库，包含133万个关于实体和事件的推理知识元组</li></ul></li><li>问答(Question Answering)<ul><li>FreebaseQA:一个基于Freebase知识图构建的开放域QA数据集，专门为知识图QA任务设计。包括来自各种来源的问答对，例如TriviaQA数据集等</li><li>MetaQA：是从WikiMovies数据集扩展而来的，提供了大量的单跳和多跳(hop)问答集，总数超过400000</li></ul></li></ul><h3 id="总体结果">总体结果</h3><h4 id="实体和关系提取">实体和关系提取</h4><p>本文在SciERC、Re-TACRED和DuIE2.0数据集上进行了实验，每个实验都涉及test/valid集中的20个样本，并使用标准微F1Score进行评估。本文使用PaddleNLP，PL-Marker，EXOBRAIN作为基准模型来进行评估（也就是State-of-the-art,SOTA）</p><p>结果如表1所示，在零样本和单样本任务中，GPT-4在这些数据集上都展现了相对不错的性能，也比ChatGPT有了一些进步，但仍然比不过全监督的小模型</p><img src="/2023/10/10/14-%3CLLMs%20for%20Knowledge%20Graph%20Construction%20and%20Reasoning-Recent%20Capabilities%20and%20Future%20Opportunities%3E%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/1.png" class=""><h5 id="零样本">零样本</h5><p>GPT-4在零样本时的性能，展现出在3个数据集中都有进步，其中最显著的进步体现在DuIE2.0中。其中，GPT-4的Score为31.03，而ChatGPT的Score为10.3.。该结果进一步证明了GPT-4在提取复杂和新知识方面具有强大的泛化能力</p><p>如图2中的（1）和（2）所示，与ChatGPT相比，GPT-4在头尾(Head-to-tail)实体提取方面表现出更明显的进步。</p><p>在Re-TACRED的例句中，目标三元组是(Helen Keller Inter-national, org:alternate_names，HKI)。ChatGPT未能提取出这样的关系，可能是因为头部和尾部的位置非常接近，并且此时谓词较为模糊。而GPT-4成功提取了头部和尾部实体之间的“org:alternate_names”关系，完成了三重提取。这也在一定程度上证明了GPT-4增强了语言理解（阅读）能力</p><h5 id="单样本">单样本</h5><p>同时，文本指令的优化有助于提高GPT-4的性能。结果表明，在单样本的方式中，引入一个训练样本可以进一步提高GPT-4和ChatGPT的三重提取能力</p><p>以DuIE2.0数据集为例。考虑以下句子：“George Wilcombe was selected forthe Honduras national team in 2008, and he par- ticipated in the 2009North and Central America and Caribbean Gold Cup with theteam（乔治·威尔科姆于2008年入选洪都拉斯国家队，他曾与该队一起参加2009年北美、中美洲和加勒比金杯赛）”</p><p>相应的三元组应该是(George Wilcombe, Nationality,Honduras)。尽管文本中没有明确说明，但GPT-4成功地提取了这些信息。这不仅是因为我们提供了有价值的单个训练示例，也源于GPT-4的综合知识库——GPT-4根据乔治·威尔科姆加入了国家队这一事实，从而推断出他的国籍</p><p>尽管如此，但GPT-4的表现并不完美，而且在面对复杂的句子时表现不佳。需要注意的是，数据集噪声、类型歧义、复杂的句子上下文，都会导致这种结果。</p><p>此外，为了实现数据集上不同模型的公平、横向比较，实验没有在提示中指定提取实体的类型（这也可能会影响实验结果）。这一结果有助于解释为什么GPT-4在SciERC和Re-TACRED上的性能不如在DuIE2.0上的性能</p><h4 id="事件提取">事件提取</h4><p>我们对MAVEN数据集中的20个随机样本进行了事件检测(EventDetection)实验。此外，王等人（2022a）被用作微调过的SOTA</p><p>值得注意的是，GPT-4在没有演示的情况下取得了值得称赞的成果。此处使用F-Score作为评估指标</p><h5 id="零样本-1">零样本</h5><p>表1中的结果显示，GPT-4在零样本实验中表现得比ChatGPT要好。对于例句"Nowan established member of the line-up, he agreed to sing it moreoften"，ChatGPT的结果是"Becoming_a_member"，而GPT-4确定了三种事件类型："Becoming_a_member","Agree_or_refuse_to_act"和"Performing"</p><p>可以发现，ChatGPT的答案经常只有一种事件类型，而GPT-4更善于获取上下文信息，产生更多样的答案，提取更全面的事件类型。因此，GPT-4在MAVEN数据集上取得了优异的结果（该数据集包含具有一个或多个关系的集合）</p><h5 id="单样本-1">单样本</h5><p>此时我们观察到，ChatGPT的性能显著提高，而GPT-4的性能略有下降。单个的演示能够纠正ChatGPT在零样本下做出的错误判断，从而提高其性能</p><p>我们便着重分析GPT-4的表现。如图3所示：</p><p>数据集中的事件类型为Process_end 和Com_together。但是GPT-4会生成三个结果：Comparison、Earnings_and_losss和Ranking。在这个过程中，GPT-4确实注意到了句子中隐藏的排序和比较信息，但忽略了与Process_end和相对应的触发词final、以及Come_together相对应的触发器host。同时，在单样本下，当GPT-4无法正确识别类型时，它往往会产生更多的错误。这在一定程度上导致了GPT-4的性能下降</p><p>我们认为这可能是由于数据集中提供的类型不明确。此外，单句话中存在多个事件类型，这进一步增加了任务的复杂性，导致了不太好的结果</p><h4 id="链接预测">链接预测</h4><p>链接预测在两个数据集FB15k-237和ATOMIC2020上进行实验。前者是一个包含25个实例的随机样本集，而后者包含代表所有可能关系的23个实例。SOTA中，对于FB15k-237数据集表现最好的是基于BERT的C-LMKE，对于ATOMIC2020数据集表现最好的是COMET</p><h5 id="零样本-2">零样本</h5><p>在表2中，GPT-4在零样本时优于text-davinci-003和ChatGPT模型。值得注意的是，GPT-4在数据集FB15k-237上的Hits@1Score表明，它达到了最先进的水平，超过了微调模型的性能。关于ATOMIC2020，尽管GPT-4超过了其他两个模型，但GPT-4的性能和微调过的SOTA在blue1Score上仍存在相当大的差距</p><p>在零样本的响应中，在预测的链接中存在歧义的情况下，ChatGPT拒绝直接回答，并主动要求提供额外的上下文来解决这种歧义。这在GPT-4中并不常见，它倾向于直接提供答案。这一结果表明了它们推理和决策过程中的差异</p><h5 id="单样本-2">单样本</h5><p>结果表明，单样本下的GPT-4在两个数据集上的表现都有所提升，有助于更准确地预测三元组的尾部实体</p><p>在图4的例子中，[MASK]对应的答案是"Primetime EmmyAward"。零样本时，GPT-4没能正确理解；但单样本时，他就能成功给出尾部的实体了</p><h4 id="问答">问答</h4><p>我们基于两个问答数据集进行了评估：FreebaseQA和MetaQA。从每个数据集中随机抽取20个实例。对于MetaQA，它由具有不同跳数的问题组成，我们根据它们在数据集中的比例进行采样。我们对两个数据集使用的评估矩阵是AnswerExactMatch</p><h5 id="零样本-3">零样本</h5><p>如表2所示，text-davinci-003、ChatGPT和GPT-4在FreebaseQA数据集上具有相同的性能，并且都比以前的全监督SOTA提高了16%。FreebaseQA数据集上的监督SOTA（Yuet等人，2022）提出了一种新的框架DECAF，它可以共同生成逻辑的形式和直接的答案。然而，GPT-4与text-davinci-003或ChatGPT相比没有任何优势。</p><p>至于MetaQA，在LLM和监督SOTA之间仍然存在很大的差距。监督SOTA模型（Madani和Joseph，2023）为T5-small配备了经典的逻辑编程语言，并以Prolog查询的形式表示问题，然后可以使用Prolog查询以编程方式回答查询。SOTA模型能够正确回答MetaQA测试数据集中的所有问题。</p><p>造成这种差距的一个可能原因可能是MetaQA中存在多个答案的问题，而FreebaseQA数据集的问题通常只有一个答案。此外，由于输入Token长度的限制，MetaQA数据集提供的知识图无法提供到LLM中，LLM只能依靠其内部知识进行多跳推理，因此LLM往往无法覆盖所有正确答案</p><p>不过，GPT-4在LLM中仍是最优的，分别比text-davinci-003和ChatGPT高了29.9分和11.1分。例如MetaQA在图4中的示例问题。回答这个问题需要一个多跳推理过程，包括将电影与其他电影的编剧联系起来，最后与上映年份联系起来。GPT-4能够正确回答这个问题，而ChatGPT和text-davinci-003未能提供正确的答案，体现了GPT-4在多跳问答任务中的卓越性能</p><h5 id="单样本-3">单样本</h5><p>我们还从训练集中随机抽取一个示例，在单样本下进行实验。表2中的结果表明，只有text-davinci-003在提示下得到改良，而ChatGPT和GPT-4的性能都有所下降。这可以归因于臭名昭著的调整税(alignmenttax)，模型牺牲了一些上下文学习能力，来与人类的反馈保持一致</p><h3 id="kg构建vs推理">KG构建vs推理</h3><p>在KG构造和KG推理的实验中，LLM的推理能力通常优于构造能力。对于KG的构造，LLM在零样本和单样本上都没有超过SOTA的性能。这与之前在信息提取任务上进行的实验（Maet al.，2023）一致，表明LLM在少样本条件下的信息提取并不理想</p><p>相反地，在KG推理任务中，所有LLM在单样本的条件下，以及GPT-4在零样本的条件下，都达到了SOTA的水准</p><p>我们对这种现象提出了几个可能的解释：</p><ul><li>首先，KG构建包含了对实体、关系、事件等的识别和提取，比推理任务更复杂；相反，以链接预测为代表的KG推理主要依赖于现有的实体和关系进行推理，任务相对简单。</li><li>其次，LLM在推理中的优异表现，可能是因为他们在预训练阶段就处理过了相关的知识</li></ul><h3 id="通用领域vs专业领域">通用领域vs专业领域</h3><p>为了探索大模型在通用、专业领域下的性能差异，我们设计了不同知识领域的任务。我们把目光放在SciERC和Re-TACRED数据集上：</p><ul><li>SciERC专注于科学领域，包括大量与科学研究相关的文章摘要和实体/关系</li><li>相反，Re-TACRED数据集的目标是通用领域，包括各种实体和关系类型</li></ul><p>结果表明，与Re-TACRED数据集相比，ChatGPT和GPT-4在SciERC数据集上表现出相对较差的性能。尽管Re-TACRED中的关系类型范围更广，但类型的增加并不会导致性能的下降。这表明，LLM在辨识专业数据时仍然存在局限性，与全监督的SOTA相比，性能差距变得更加明显。</p><p>以GPT-4为例，引入单样本可将其在Re-TACRED上的性能从15.5显著提高到22.5。然而，单样本对GPT-4在SciERC上性能的提升较为一般。这说明，引入单样本对于模型在专业数据集上的表现的影响是有限的。随着样本数量的增加，可能会有更大的性能提升</p><p>我们认为，大模型在专业数据集上的性能不良，可能的原因是：</p><ul><li>它们主要在大规模的通用语料库上进行训练，而这些语料库可能没有包含足够的专业领域知识</li><li>此外，在专业数据集中，某些实体、关系或概念可能呈现长尾分布</li></ul><h2id="讨论为什么llm在某些任务上的表现不尽人意">讨论：为什么LLM在某些任务上的表现不尽人意？</h2><p>上述结果表明，LLM能够提取各种类型和领域的知识，但尚未超过微调模型的性能，这与之前的研究结果一致（Weiet al.，2023b；Gao et al.，2021）。</p><p>影响评估结果的可能因素如下：</p><ul><li>数据集质量：以KG构建为例，数据中的噪声会导致某些类型的模糊（例如在关系提取中，数据集可能没有明确地提供头部和尾部的实体类型）。此外，数据集可能包含高度复杂的文本和不准确的标签</li><li>指令质量：指令的语义丰富度显著地影响了模型的性能。使用不同指令进行实验，从而确定最有效的指令，可能会提高性能（prompt工程研究的就是如何通过指令来充分利用模型的能力）。此外，在上下文学习（Dongetal.，2023）中整合相关的样本可以进一步提高性能。值得注意的是，Code4Construct（Wangetal.，2022b）证明，使用基于代码的提示可以提高模型在提取结构化信息方面的性能</li><li>评估方法：现有的评估方法可能不完全适合评估LLM的能力。例如，数据集的标签可能不包含所有的正确答案，但LLM可能会给出给定答案范围外的响应。此外，在涉及相同指称（如同义词）的情况下，答案也可能无法正确识别</li></ul><h2id="讨论llm是记住了知识还是真正具有泛化能力">讨论：LLM是记住了知识，还是真正具有泛化能力？</h2><p>从先前的实验中可以看出，大型模型善于从微小信息中快速提取结构化知识。这让我们想到一个关于LLM性能优势的理论来源的问题：这是由于在预训练阶段使用了大量的文本数据，使模型能够获得相关的知识，还是因为它们强大的推理和泛化能力？</p><p>为了研究这个问题，我们设计了一个虚拟知识提取任务，用于评估LLM泛化和提取陌生知识的能力。而当前的数据集无法满足我们的需求，所以我们引入了一种新的虚拟知识提取数据集——VINE</p><p>更加具体地说，我们构建了现实世界中不存在的实体和关系，并将它们组织成知识三元组。随后，我们通过指令来让模型提取这些虚拟的知识，提取的效果用于衡量LLM处理陌生知识的能力。我们基于Re-TACRED数据集的测试集构建了VINE。构建过程的主要思想是用看不见的实体和关系替换原始数据集中的实体和关系，从而创建独特的虚拟知识场景</p><h3 id="数据收集">数据收集</h3><p>考虑到像GPT-4这样LLM的庞大训练集，我们很难找到它们不熟悉的知识。以截至2021年9月的GPT-4数据为基础，我们从《纽约时报》组织的两次竞赛中选择了参与者的部分回答作为数据来源，包括2022年1月和2023年2月举行的两项创造新单词的比赛</p><p>然而，由于上述竞赛的回答数量有限，为了增强数据的多样性，我们还通过随机生成字母序列来创建新词。我们生成了长度在7到9个字符之间的随机序列，并随机添加常用的名词后缀</p><h3 id="数据分析">数据分析</h3><p>我们构建的数据集包括1400个句子、39个全新的关系和786个独一无二的实体，并确保每个关系类型至少有10个样本。此外，我们发现在Re-TACRED测试集中，某些关系类型的样本少于10个，于是我们从训练集中选择了相应类型的句子来进行弥补</p><h3 id="初步结果">初步结果</h3><p>实验中，我们随机挑选了不同关系类型的10个句子来进行评估。在学习了相同关系类型的两个示例后，我们评估了ChatGPT和GPT-4在这十个测试样本上的性能</p><p>结果表明，在学习了一定数量的虚拟知识后，ChatGPT在虚拟知识提取中的表现显著低于GPT-4，而GPT-4能够根据指令准确地提取从未见过的实体和关系的知识。GPT-4成功提取了80%的虚拟三元组，而ChatGPT的准确率仅为27%</p><p>在图5所示的例子中，我们提供了由虚拟关系类型和虚拟头尾实体组成的三元组以及相应的示例。结果表明，GPT-4有效地完成了虚拟三元组的提取</p><p>因此我们初步得出结论，GPT-4表现出相对较强的泛化能力，可以通过指令快速获取新知识，而不仅仅依赖相关知识的记忆。相关的工作（Weiet al.，2023a）也证实了大型模型对指令具有异常强大的泛化能力</p><h1 id="未来机遇自动的kg构建与推理">未来机遇：自动的KG构建与推理</h1><p>近来LLM的成功仍然主要依赖于大量的人类输入来指导会话文本的生成。随着用户不断完善任务描述和要求，并通过ChatGPT建立对话上下文，该模型可以提供越来越精确和高质量的响应。然而，从模型开发的角度来看，这个过程仍然是劳动密集型和耗时的。因此，研究人员已经开始研究预测<strong>大型模型自主生成引导文本</strong>的潜力</p><p>例如，AutoGPT可以独立生成提示并执行事件分析、营销计划创建、编程和数学运算等任务。同时，李等人（2023）深入研究了通信代理之间自主合作的潜力，并提出了一种新的合作代理框架，称为角色扮演(Role-playing)。在此基础上，我们进一步探究：利用通信代理来完成KG构建和推理任务是否可行？</p><p>本实验中，我们使用了CAMEL中的角色扮演方法（Li etal.，2023）。AI助手被指定为顾问，AI用户被指定为KG领域专家。在收到提示和指定的角色分配后，”任务指定代理“提供详细的描述来将概念具体化。此后，AI助手和AI用户在多方设置中合作完成指定的任务，直到AI用户确认完成为止</p><h2 id="评论">评论</h2><p>通过将人工智能和人类专业知识相结合，AutoKG可以快速构建特定领域的KG。该系统允许领域专家与LLM进行交互，从而通过专家知识和经验的交流，为构建特定领域的知识图创造一个协作环境</p><p>同时，由于这种人机协作，它可以提高大型语言模型在处理特定领域任务时的准确性。AutoKG不仅可以加快特定领域知识图的定制，还可以提高大型模型的透明度和代理间的互动</p><p>更准确地说，AutoKG有助于更深入地理解LLM的内部知识结构和操作机制，从而增强模型的透明度。此外，AutoKG可以部署为一个可操作的人机交互平台，实现人与模型之间的有效沟通和互动，从而提高模型在处理复杂任务时的效率和准确性</p><h2 id="局限性">局限性</h2><p>尽管我们的方法带来了显著的进步，但它并非没有局限性，然而，它本身就是进一步探索和改进的机会：</p><ul><li>API的使用受到最大token限制：此限制会影响KGs的构建，因为如果超过此限制，任务可能无法正确执行</li><li>AutoKG目前在促进高效人机交互方面存在不足：在任务完全由机器自主执行的情况下，人类无法及时记录通信中发生的错误；相反，让人类参与机器通信的每一步都会大大增加时间和劳动力成本。因此，确定人为干预的最佳时刻至关重要</li><li>LLM的训练数据对时间敏感：未来的工作可能需要结合互联网上的检索，以弥补当前LLM在获取最新或特定领域知识方面的不足</li></ul><h1 id="结论与后续工作">结论与后续工作</h1><p>本文中，我们试图初步研究LLM在KG构建、推理等任务中的性能，以GPT系列为例。虽然这些模型擅长于此类任务，但我们提出了一个问题：LLM在提取任务方面的优势是来自其庞大的知识库还是潜在的上下文学习能力？为了探索这一点，我们提出了一个虚拟知识提取任务，并创建了相应的数据集进行实验。结果表明，LLM的确拥有强大的上下文学习能力</p><p>此外，我们提出了一种创新方法，通过部署多个代理来完成KG的构建和推理。这不仅减少了劳动，还补偿了LLM在各个领域专业知识的不足，从而提高了LLM的性能。尽管这种方法仍然有一些局限性，但它为LLM的未来应用提供了一个新的视角</p><p>虽然我们的研究已经取得了一些结果，但它也有一定的局限性</p><ul><li>如前所述，无法访问GPT-4API使我们不得不依赖交互式界面进行实验，这无疑增加了工作量和时间成本</li><li>此外，由于GPT-4的多模态功能目前还不可供公众使用，我们暂时无法深入研究其性能以及对多模式处理的贡献</li></ul>]]></content:encoded>
      
      
      <category domain="http://yunsaijc.top/categories/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/">文献阅读</category>
      
      
      <category domain="http://yunsaijc.top/tags/AI/">AI</category>
      
      <category domain="http://yunsaijc.top/tags/LLM/">LLM</category>
      
      <category domain="http://yunsaijc.top/tags/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/">知识图谱</category>
      
      
      <comments>http://yunsaijc.top/2023/10/10/14-%3CLLMs%20for%20Knowledge%20Graph%20Construction%20and%20Reasoning-Recent%20Capabilities%20and%20Future%20Opportunities%3E%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>&lt;Knowledge Graphs:  Opportunities and Challenges&gt;阅读笔记</title>
      <link>http://yunsaijc.top/2023/10/10/13-%3CKnowledge%20Graphs-%20Opportunities%20and%20Challenges%3E%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</link>
      <guid>http://yunsaijc.top/2023/10/10/13-%3CKnowledge%20Graphs-%20Opportunities%20and%20Challenges%3E%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</guid>
      <pubDate>Tue, 10 Oct 2023 01:21:12 GMT</pubDate>
      
        
        
      <description>&lt;blockquote&gt;
&lt;p&gt;Peng, C., Xia, F., Naseriparsa, M. &lt;em&gt;et al.&lt;/em&gt; Knowledge Graphs:
Opportunities and Challenges. &lt;em&gt;Artif Intell Rev&lt;/em&gt;</description>
        
      
      
      
      <content:encoded><![CDATA[<blockquote><p>Peng, C., Xia, F., Naseriparsa, M. <em>et al.</em> Knowledge Graphs:Opportunities and Challenges. <em>Artif Intell Rev</em><strong>56</strong>, 13071–13102 (2023).https://doi.org/10.1007/s10462-023-10465-9</p><p>本次阅读主要关注KG的构建，其余省略</p></blockquote><h2 id="intro">Intro</h2><p>知识图(Knowledge Graph,KG)被定义为：收集并传达现实世界知识的数据图。知识图中的节点表示实体，边表示关系。这种结构化的表示形式，使得计算机能够更加高效地处理它。</p><p>KG已经广泛应用于各种AI系统中，如：推荐系统、问答、检索系统。在拥有这些机遇的同时，也面临着很多挑战。</p><h2 id="kg概述">KG概述</h2><ul><li>知识库：以三元组(triplet)形式来表达事实与联系的数据集合</li><li>KG：当上述三元组是“边为关系、节点为实体”的图时，则为KG</li><li>也就是说，KG是由节点和边组成的有向图，一个节点表示一个（食物或抽象）的实体，两节点之间的边则表示两个实体之间的语义联系</li><li>组成KG的基本单位是（主语，谓语，宾语）或（头，关系，尾）</li></ul><h3 id="kg嵌入embedding">KG嵌入(Embedding)</h3><p>这是KG研究的核心问题之一。KG嵌入指的是将KG中的实体和关系映射到一个低维的向量空间，以有效地涵盖知识图中的语义与结构。有3种主要的KG嵌入方法：</p><ul><li>基于张量因子分解的方法</li><li>基于翻译的方法</li><li>基于神经网络的方法</li></ul><h3 id="kg获取acquisition">KG获取(Acquisition)</h3><p>侧重于KG的建模和构建。可以分为两种形式：</p><ul><li>通过映射语言从结构化的知识源导入</li><li>通过关系、实体、属性提取方法，从非结构化的知识源提取</li></ul><h3 id="kg补全completion">KG补全(Completion)</h3><p>大多数的KG仍然缺乏大量的实体和关系，可以通过预测额外的关系和实体来补全KG。</p><h3 id="kg融合fusion">KG融合(Fusion)</h3><p>指的是从不同的源中获取知识，并整合成一个KG。KG融合的方法对于KG生成和补全也有参考意义。</p><h3 id="kg推理reasoning">KG推理(Reasoning)</h3><p>指的是通过推理来充实KG的内容，即通过KG中已经存在的数据来进行推断。特别是推断两个无关实体之间的新关系，从而组成新的三元组。此外，也可以推理出虚假事实，从而找出KG中有误的知识。主要方法包括：</p><ul><li>基于逻辑规则的方法</li><li>基于分布表示的方法</li><li>基于神经网络的方法</li></ul><h3 id="ai系统">AI系统</h3><p>如今，KG被AI系统广泛使用，如推荐系统、问答系统和信息检索工具。通常，KG中信息的丰富性会提高这些解决方案的性能</p><h2 id="kg的挑战">KG的挑战</h2><h3 id="kg获取">KG获取</h3><p>知识是从结构化和非结构化数据中提取的，三种主要方法包括关系提取、实体提取和属性提取（Fuetal.2019）。属性提取可以被视为实体提取的一种特殊情况。张等人（2019b）利用KG嵌入和图卷积网络提取长尾关系。施等人（2021）提出了实体集展开来构造大规模KG。但包含如下问题：</p><ul><li>现有的知识获取方法仍然面临着准确性低的挑战，这可能导致KG不完整或有噪声，并阻碍下游任务。因此，第一个关键问题是知识获取工具的可靠性及其评估。</li><li>此外，特定领域的KG模式是面向知识的，而构建的KG架构是面向数据的（这样就能覆盖所有的数据特征）。因此，通过从原始数据中提取实体和属性来生成特定领域的KG非常低效，这成为一个亟待解决的问题</li><li>此外，现有的知识获取方法大多侧重于用一种特定的语言。然而，为了使KG中的信息更加丰富和全面，我们需要跨语言的实体提取</li></ul><blockquote><p>注：笔者认为这篇文章价值不高，所以至此了解了KG的基本知识和研究方向即可，便不再继续往下翻译和阅读。</p></blockquote>]]></content:encoded>
      
      
      <category domain="http://yunsaijc.top/categories/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/">文献阅读</category>
      
      
      <category domain="http://yunsaijc.top/tags/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/">知识图谱</category>
      
      
      <comments>http://yunsaijc.top/2023/10/10/13-%3CKnowledge%20Graphs-%20Opportunities%20and%20Challenges%3E%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Llama部署与运行初尝试</title>
      <link>http://yunsaijc.top/2023/10/09/12-Llama%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%BF%90%E8%A1%8C%E5%88%9D%E5%B0%9D%E8%AF%95/</link>
      <guid>http://yunsaijc.top/2023/10/09/12-Llama%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%BF%90%E8%A1%8C%E5%88%9D%E5%B0%9D%E8%AF%95/</guid>
      <pubDate>Mon, 09 Oct 2023 07:03:23 GMT</pubDate>
      
        
        
      <description>&lt;h2 id=&quot;下载&quot;&gt;下载&lt;/h2&gt;
&lt;p&gt;克隆Llama的官方Github仓库：&lt;/p&gt;
&lt;figure class=&quot;highlight plaintext&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/</description>
        
      
      
      
      <content:encoded><![CDATA[<h2 id="下载">下载</h2><p>克隆Llama的官方Github仓库：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/facebookresearch/llama.git</span><br></pre></td></tr></table></figure><p>进入<code>llama</code>文件夹，运行命令： <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python -m  pip install -e . -i https://pypi.tuna.tsinghua.edu.cn/simple</span><br><span class="line"># -i 后的参数为临时换源</span><br></pre></td></tr></table></figure>这会自动执行当前目录下的<code>setup.py</code>文件，以安装当前项目</p><p>到<ahref="https://ai.meta.com/llama/">Llama官网</a>进行申请，填写邮箱后会收到邮件</p><h2 id="参考">参考</h2><blockquote><p><ahref="https://juejin.cn/post/7283803914646814720">https://juejin.cn/post/7283803914646814720</a></p><p><ahref="https://zhuanlan.zhihu.com/p/648548363">https://zhuanlan.zhihu.com/p/648548363</a></p></blockquote>]]></content:encoded>
      
      
      <category domain="http://yunsaijc.top/categories/%E9%A1%B9%E7%9B%AE%E8%AE%B0%E5%BD%95/">项目记录</category>
      
      
      <category domain="http://yunsaijc.top/tags/AI/">AI</category>
      
      <category domain="http://yunsaijc.top/tags/LLM/">LLM</category>
      
      
      <comments>http://yunsaijc.top/2023/10/09/12-Llama%E9%83%A8%E7%BD%B2%E4%B8%8E%E8%BF%90%E8%A1%8C%E5%88%9D%E5%B0%9D%E8%AF%95/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>&lt;A Survey of Large Language Models&gt;阅读笔记</title>
      <link>http://yunsaijc.top/2023/10/08/11-%3CA%20Survey%20of%20Large%20Language%20Models%3E%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</link>
      <guid>http://yunsaijc.top/2023/10/08/11-%3CA%20Survey%20of%20Large%20Language%20Models%3E%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</guid>
      <pubDate>Sun, 08 Oct 2023 11:47:08 GMT</pubDate>
      
        
        
      <description>&lt;blockquote&gt;
&lt;p&gt;原文链接：&lt;a
href=&quot;https://arxiv.org/abs/2303.18223&quot;&gt;https://arxiv.org/abs/2303.18223&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;intro&quot;&gt;Intro&lt;</description>
        
      
      
      
      <content:encoded><![CDATA[<blockquote><p>原文链接：<ahref="https://arxiv.org/abs/2303.18223">https://arxiv.org/abs/2303.18223</a></p></blockquote><h2 id="intro">Intro</h2><p>为了让机器像人类一样阅读、写作和交流，语言建模(LM)是提高机器语言智能的主要方法之一。其原理是对词语序列的生成概率进行建模，以预测未来的、或缺失的单词的概率。</p><p>LM的发展可以分为4个阶段：</p><ul><li>统计语言模型SLM：基于统计学习方法开发。基本思想是基于马尔可夫假设来建立词频预测模型</li><li>神经语言模型NLM：通过神经网络（如RNN）来描述单词序列的概率</li><li>预训练语言模型PLM：确立了“预训练和微调”的学习范式</li><li>大语言模型LLM：通过对PLM的模型或数据大小进行扩展，能够提高下游任务的模型性能。与较小的PLM相比，LLM展现出了“涌现能力”</li></ul><h2 id="背景">背景</h2><p>通常，LLM指包含千亿（100B）或更多参数的Transformer语言模型。现有的LLM 采用类似的 Transformer 架构和与小型语言模型相同的预训练目标(如语言建模)</p><h3 id="llm的扩展法则">LLM的扩展法则</h3><p>KM扩展法则：是由Kaplan等人提出的，神经语言模型的性能与模型规模<spanclass="math inline">\((N)\)</span>、数据集规模<spanclass="math inline">\((D)\)</span>和训练计算量<spanclass="math inline">\((C)\)</span>之间的幂律关系: <spanclass="math display">\[\begin{aligned}&amp;L(N)=(\frac{N_c}{N})^{\alpha_N},\ \alpha_N\sim0.076,\ N_c\sim8.8\times10^{13}\\&amp;L(D)=(\frac{D_c}{D})^{\alpha_D},\ \alpha_D\sim0.095,\ D_c\sim5.4\times10^{13}\\&amp;L(C)=(\frac{C_c}{C})^{\alpha_C},\ \alpha_C\sim0.050,\ C_c\sim3.1\times10^{8}\end{aligned}\]</span> 其中，<spanclass="math inline">\(L(\cdot)\)</span>为用nats表示的交叉熵损失，<spanclass="math inline">\(c\)</span>为给定的计算预算。</p><h3 id="llm的涌现能力">LLM的涌现能力</h3><p>定义：在小型模型中不存在但在大型模型中产生的能力（是区别 LLM 与先前PLM 的最显著特征之一）</p><p>LLM的三种典型涌现能力：</p><ul><li>上下文学习(ICL)：ICL 能力是由 GPT-3正式引入的。假设已经为LLM提供了一个自然语言指令和/或几个任务演示，它可以通过完成输入文本的单词序列的方式来为测试实例生成预期的输出，而无需额外的训练或梯度更新（如：175B的GPT-3 模型在一般情况下表现出强大的 ICL 能力）</li><li>指令遵循：通过使用自然语言描述的混合多任务数据集进行微调（称为指令微调），LLM在未见过的以指令形式描述的任务上表现出色，具有更好的泛化能力</li><li>逐步推理：对于小型语言模型而言，通常很难解决涉及多个推理步骤的复杂任务，例如数学问题。然而，通过使用思维链(Chain-of-Thought,CoT)提示策略，LLM可以通过利用包含中间推理步骤的提示机制来解决这类任务，从而得出最终答案。</li></ul><h3 id="llm的关键技术">LLM的关键技术</h3><p>LLM目前的角色就像一个通用、且有能力的学习者。导致其成功的关键技术包括：</p><ul><li><strong>扩展</strong>：如前面所说，Transformer语言模型存在明显的扩展效应:更大的模型/数据规模和更多的训练计算通常会导致模型能力的提升</li><li><strong>训练</strong>：大规模使分布式训练成为必须；此外，优化技巧对于训练的稳定性和模型性能也非常重要</li><li><strong>能力引导</strong>：LLM具备了解决通用任务的潜在能力，而当执行一些特定任务时，这些能力可能不会展示出来。设计合适的任务指令或具体的ICL 策略可以激发这些能力</li><li><strong>对齐微调</strong>：LLM的预训练数据中包含低质量的数据，它可能会生成有毒、偏见甚至有害的内容。因此，有必要使LLM 与人类价值观保持一致。InstructGPT 使 LLM能够按照期望的指令进行操作，它利用了基于人类反馈的强化学习技术，将人类纳入训练循环中；ChatGPT采用类似于 InstructGPT的技术，在产生高质量、无害的回答方面表现出很强的对齐能力。</li><li><strong>工具操作</strong>：LLM基于海量纯文本语料库进行训练，因此在那些非文本的任务上表现不佳（如数字计算）。此外，它们的能力也受限于预训练数据（如无法获取最新信息）。为了解决这些问题，学者们提出利用外部工具来弥补LLM 的不足（如，利用计算器进行准确计算、利用搜索引擎检索未知信息）。ChatGPT 已经实现了使用外部插件的机制，这种机制可以广泛扩展 LLM的能力范围。</li></ul><h3 id="gpt系列模型的技术演进">GPT系列模型的技术演进</h3><p>ChatGPT 基于功能强大的 GPT模型开发，其对话能力得到了专门的优化，在社会上引起了广泛关注。</p><ul><li>2018～GPT-1：GPT 代表生成式预训练(GenerativePre-Training)。它是基于生成型的、仅解码器的 Transformer架构开发的，并采用了无监督预训练和有监督微调的混合方法。它为 GPT系列模型建立了核心架构，并确立了对自然语言文本进行建模的基本原则，即预测下一个单词</li><li>2019～GPT-2：采用了与 GPT-1 类似的架构，将参数规模增加到了 15亿，并使用大规模的网页数据集 WebText进行训练。它旨在通过无监督语言建模来执行任务，而无需使用标记数据进行显式微调。论文中提出，自然语言文本可以自然地用作为格式化输入、输出和任务信息的统一方式，<strong>解决任务的过程可以被视为生成解决方案文本的单词预测问题</strong>。也就是说，每个NLP任务可以被视为基于世界文本的子集的单词预测问题，因此如果模型训练后具有足够能力以复原世界文本，无监督语言建模可以解决各种任务</li><li>2020～GPT-3：它扩展了（几乎相同的）生成式预训练架构，将模型参数扩展到了175B。论文中正式介绍了ICL，它是以小样本或零样本的方式使用LLM，可以指导 LLM 理解以自然语言文本的形式给出的任务。GPT-3 可以被视为从PLM 到 LLM进化过程中的一个重要里程碑。它通过实证证明，将神经网络扩展到大的规模可以大幅增加模型的能力</li></ul><p>此后，OpenAI从两个方面对GPT-3进行改进：</p><ol type="1"><li>使用代码进行训练，以解决GPT难以推理复杂任务（如代码、数学）的能力；</li><li>与人类进行对齐（人类偏好、道德等）</li></ol><p>接下来，LLM的重要里程碑就出现了：</p><ul><li>2022～ChatPGT：以类似 InstructGPT的方式进行训练，但专门针对对话能力进行了优化</li><li>2023～GPT-4：将文本输入扩展到多模态信号。总体而言，相比GPT-3.5，GPT-4 在解决复杂任务方面具有更强的能力</li></ul><h2 id="llm资源">LLM资源</h2><p>略</p><h2 id="预训练">预训练</h2><h3 id="数据收集">数据收集</h3><ol type="1"><li>数据来源可以广义地分为两种类型：</li></ol><ul><li>通用文本数据：如网页、书籍和对话文本等，具有规模大、多样性强且易于获取的特点，可以增强LLM 的语言建模和泛化能力</li><li>专用文本数据：如多语言数据、科学数据和代码等，可以赋予 LLM解决专用任务的能力</li></ul><ol start="2" type="1"><li>数据预处理（如消除噪声、冗余、无关和潜在有害的数据）会极大地影响 LLM的能力和性能。预处理策略包括：</li></ol><ul><li>质量过滤：删除低质量数据。通常有两种方法<ul><li>基于分类器：训练分类器来筛选</li><li>基于启发式：设计一组规则来筛选，如去除某种语言的数据、利用困惑度来删除不自然的数据、利用统计特征来过滤、利用关键词来过滤</li></ul></li><li>去重</li><li>隐私去除：去除敏感的个人信息</li><li>分词：将原始文本分割成词序列作为LLM的输入（虽然已经有很多分词器，但使用专门为预训练语料库而设计的分词器更有效）</li></ul><ol start="3" type="1"><li>预训练数据影响LLM的因素：<ul><li>混合来源：数据源自不同的领域或场景，不同领域的数据比例不同，对LLM的能力影响也不同</li><li>数据数量</li><li>数据质量</li></ul></li></ol><h3 id="架构">架构</h3><p>Transformer架构是当前LLM的标准骨干。目前的主流架构：</p><ul><li>编码器-解码器架构</li><li>因果解码器架构</li><li>前缀解码器架构</li></ul><p>其余架构上的不同还体现在：层标准化(LayerNorm)，激活函数，位置编码，注意力机制和偏置</p><p>常用的预训练任务：语言建模、去噪自编码</p><h3 id="模型训练">模型训练</h3><p>略</p><h2 id="llm的适配微调">LLM的适配微调</h2><ul><li>指令微调：是在自然语言格式的实例(instance)集合上微调预训练后的 LLM的方法。这种方法与有监督微调和多任务提示训练密切相关</li><li>对齐微调：为了避免模型出预期之外的行为（如编造虚假信息、追求不准确的目标，以及产生有害的、误导性的和有偏见的表达）</li></ul><p>为了减少训练的参数量，同时尽可能达到良好的微调效果，有以下几种参数的高效微调方法：</p><ul><li>适配器微调(Adapter tuning)：在 Transformer模型中引入了小型神经网络模块（称为适配器）。微调过程中，适配器将根据特定的任务目标进行优化，而原始LM的参数保持不变。这样可以有效减少可训练参数的数量</li><li>提示微调(prompttuning)：在输入层中加入可训练的提示向量。即：给每个任务定义自己的prompt，拼接到数据上作为输入，训练的参数是prompt</li><li>前缀微调(prefix tuning)：在LM的每个 Transformer层前添加了一系列前缀（即一组可训练的连续向量）。这些前缀向量具有任务的特异性，可以视为虚拟的token 嵌入。被训练的参数只有前缀。（也可以被视为提示微调的一种。但与提示微调不同的是，前缀微调完全由自由参数组成，与真正的token不对应）</li><li>低秩适配(LoRA)：通过添加低秩约束来近似每层的更新矩阵，以减少适配下游任务的可训练参数</li></ul><p>LLM参数微调的实际应用：</p><ul><li>Alpaca-LoRA 是通过 LoRA 训练出的 Alpaca（一个经过微调的 70 亿 LLaMA模型）的轻量级微调版本</li><li>PEFT 代码库已在 GitHub上发布。它包括了几种广泛使用的高效微调方法，包括 LoRA/AdaLoRA、前缀微调、P- Tuning和提示微调。此外，它支持多个语言模型，如GPT-2 和 LLaMA</li></ul><h2 id="使用">使用</h2><p>上下文学习、思维链提示。略</p><h2 id="能力评测">能力评测</h2><p>略</p><h2 id="总结与未来方向">总结与未来方向</h2><ul><li>理论与原理：可解释性</li><li>模型架构：如何构建 LLM 中更高效的 Transformer 变体</li><li>模型训练：开发更系统、经济的预训练方法</li><li>模型应用：在实际应用中微调的成本非常高，所以提示已成为使用 LLM的主要方法。但是，<ul><li>设计提示需要大量人力。自动生成有效提示以解决各种任务将非常有用</li><li>其次，一些复杂任务（例如形式证明和数值计算）需要特定的知识或逻辑规则，这些规则可能无法用自然语言很好地表达或通过示例演示。因此，开发更具信息量和灵活性的任务格式化方法以进行提示非常重要</li><li>第三，现有的提示策略主要关注单轮性能。需要开发交互式提示机制（例如通过自然语言对话）来解决复杂任务</li></ul></li><li>安全与对齐：LLM会产生幻觉，也可能被有意的指令激发以产生有害的、有偏见的或有毒的文本，从而导致滥用风险</li><li>应用与生态：有可能会产生一个以 LLM 为支持的应用生态系统</li></ul>]]></content:encoded>
      
      
      <category domain="http://yunsaijc.top/categories/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/">文献阅读</category>
      
      
      <category domain="http://yunsaijc.top/tags/AI/">AI</category>
      
      <category domain="http://yunsaijc.top/tags/LLM/">LLM</category>
      
      
      <comments>http://yunsaijc.top/2023/10/08/11-%3CA%20Survey%20of%20Large%20Language%20Models%3E%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>AI方向直博学习日志</title>
      <link>http://yunsaijc.top/2023/10/07/10-%E5%AD%A6%E4%B9%A0%E6%97%A5%E5%BF%97/</link>
      <guid>http://yunsaijc.top/2023/10/07/10-%E5%AD%A6%E4%B9%A0%E6%97%A5%E5%BF%97/</guid>
      <pubDate>Sat, 07 Oct 2023 12:34:16 GMT</pubDate>
      
        
        
      <description>&lt;h2 id=&quot;年&quot;&gt;2023年&lt;/h2&gt;
&lt;h3 id=&quot;月&quot;&gt;10月&lt;/h3&gt;
&lt;h4 id=&quot;日&quot;&gt;11日&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;文献继续精读：&amp;lt;LLMs for Knowledge Graph Construction and
Reasoning:Recent</description>
        
      
      
      
      <content:encoded><![CDATA[<h2 id="年">2023年</h2><h3 id="月">10月</h3><h4 id="日">11日</h4><ul><li>文献继续精读：&lt;LLMs for Knowledge Graph Construction andReasoning:Recent Capabilities and Future Opportunities&gt;</li><li>继续捣鼓LLM环境...（网站被墙，下载模型好难）</li><li>长尾分布：也就是数据量分布不均衡，少部分类别有大量样本，就好像少部分人有大量资产一样（<ahref="https://zhuanlan.zhihu.com/p/422558527">https://zhuanlan.zhihu.com/p/422558527</a>）</li></ul><h4 id="日-1">10日</h4><ul><li>文献阅读：&lt;Knowledge Graphs: Opportunities andChallenges&gt;</li><li>捣鼓LLM环境</li><li>文献阅读：&lt;LLMs for Knowledge Graph Construction andReasoning:Recent Capabilities and Future Opportunities&gt;</li><li>zero\one\few-shot：简单来说就是给了多少个学习样本（<ahref="https://zhuanlan.zhihu.com/p/624793654">https://zhuanlan.zhihu.com/p/624793654</a>）</li></ul><h4 id="日-2">9日</h4><ul><li>RLHF和InstructGPT：RLHF可以分为3步：<ul><li>根据采集的SFT数据集对GPT-3进行有监督的微调(Supervised FineTune,SFT)</li><li>收集人工标注的对比数据，训练奖励模型(Reword Model, RM)</li><li>使用RM作为强化学习的优化目标，利用PPO算法微调SFT模型InstructGPT是在GPT-3的基础上通过RLHF训练而来的 (<ahref="https://zhuanlan.zhihu.com/p/637419868">LLM 系列超详细解读(四)：InstructGPT：训练语言模型以遵从人类指令 - 知乎</a>，<ahref="https://zhuanlan.zhihu.com/p/590311003">https://zhuanlan.zhihu.com/p/590311003</a>)</li></ul></li><li>ChatGPT和InstructGPT：是一对姐妹模型，他们在模型结构、训练方式上都完全一致，即都使用了指示学习(InstructionLearning)和人类反馈的强化学习(RLHF)来指导模型的训练，它们不同的仅仅是采集数据的方式上有所差异</li><li>文献阅读笔记</li><li>激活函数的再理解：<ul><li>在神经元中的位置，是输入与权重相乘并相加之后，输出之前</li><li>一般引入非线性因素，来增加神经网络模型的非线性</li><li>否则网络中全部是线性部件，线性的组合还是线性，与单独一个线性分类器无异，这样就做不到用非线性来逼近任意函数（<ahref="https://blog.csdn.net/hhhhhhhhhhwwwwwwwwww/article/details/116863273#:~:text=CNN%E5%9F%BA%E7%A1%80%E2%80%94%E2%80%94%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%201%201%E3%80%81%E4%BB%80%E4%B9%88%E6%98%AF%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%20%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%20%28Activation%20functions%29%E5%AF%B9%E4%BA%8E%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20%E6%A8%A1%E5%9E%8B%E5%8E%BB%E5%AD%A6%E4%B9%A0%E3%80%81%E7%90%86%E8%A7%A3%E9%9D%9E%E5%B8%B8%E5%A4%8D%E6%9D%82%E5%92%8C%E9%9D%9E%E7%BA%BF%E6%80%A7%E7%9A%84%E5%87%BD%E6%95%B0%E6%9D%A5%E8%AF%B4%E5%85%B7%E6%9C%89%E5%8D%81%E5%88%86%E9%87%8D%E8%A6%81%E7%9A%84%E4%BD%9C%E7%94%A8%E3%80%82%20%E5%AE%83%E4%BB%AC%E5%B0%86%E9%9D%9E%E7%BA%BF%E6%80%A7%E7%89%B9%E6%80%A7%E5%BC%95%E5%85%A5%E5%88%B0%E6%88%91%E4%BB%AC%E7%9A%84%E7%BD%91%E7%BB%9C%E4%B8%AD%E3%80%82,%E8%BF%99%E6%A0%B7%E5%B0%B1%E5%81%9A%E4%B8%8D%E5%88%B0%E7%94%A8%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%9D%A5%E9%80%BC%E8%BF%91%E4%BB%BB%E6%84%8F%E5%87%BD%E6%95%B0%E3%80%82%20...%204%204%E3%80%81%E5%B8%B8%E7%94%A8%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%20sigmoid%20%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%20%E5%87%BD%E6%95%B0%E7%9A%84%E5%AE%9A%E4%B9%89%E4%B8%BA%EF%BC%9A%20">CNN基础——激活函数</a>）</li></ul></li><li>Llama部署初尝试</li></ul><h4 id="日-3">8日</h4><ul><li>文献阅读：&lt;A Survey of Large Language Models&gt;</li><li>LLM建立的主要基础-Transformer架构：<ahref="https://zhuanlan.zhihu.com/p/338817680">https://zhuanlan.zhihu.com/p/338817680</a></li><li>循环神经网络(RNN)：个人理解-就像在CNN中采用了密码学的CBC/CFB/OFB工作模式一样，将上一轮结果反馈到下一轮，用于更好地处理前后文相关的序列信息（<ahref="https://zhuanlan.zhihu.com/p/30844905">https://zhuanlan.zhihu.com/p/30844905</a>）</li><li>机器学习中的正则化(Regulation)：<ahref="https://www.zhihu.com/question/20924039">https://www.zhihu.com/question/20924039</a></li><li>LLM微调学习：Hugging face Transformer文档初步阅读</li><li>实验环境初步搭建</li></ul><h4 id="日-4">7日</h4><ul><li>初步确定短期研究方向</li><li>监督学习：关键在于，数据中有无人工标注的标签（<ahref="https://zhuanlan.zhihu.com/p/376931561">https://zhuanlan.zhihu.com/p/376931561</a>）</li><li>泛化(Generalization)：是指模型很好地拟合以前未见过的新数据（从用于创建该模型的同一分布中抽取）的能力（<ahref="https://www.cnblogs.com/anliven/p/10264475.html">https://www.cnblogs.com/anliven/p/10264475.html</a>）</li><li>注意力机制：通过权重，将模型的注意力转移到重要的部位（<ahref="https://zhuanlan.zhihu.com/p/379722366">https://zhuanlan.zhihu.com/p/379722366</a>）</li><li>欧氏空间的通俗理解：<ahref="https://www.zhihu.com/question/27903807">https://www.zhihu.com/question/27903807</a></li><li>图神经网络(GNN)基本原理：<ahref="https://blog.csdn.net/weixin_45884316/article/details/115751272">https://blog.csdn.net/weixin_45884316/article/details/115751272</a></li></ul>]]></content:encoded>
      
      
      <category domain="http://yunsaijc.top/categories/AI%E5%9F%BA%E7%A1%80/">AI基础</category>
      
      
      <category domain="http://yunsaijc.top/tags/%E5%AD%A6%E4%B9%A0%E6%97%A5%E5%BF%97/">学习日志</category>
      
      
      <comments>http://yunsaijc.top/2023/10/07/10-%E5%AD%A6%E4%B9%A0%E6%97%A5%E5%BF%97/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>将博客搬至CSDN</title>
      <link>http://yunsaijc.top/2023/10/07/11-%E5%B0%86%E5%8D%9A%E5%AE%A2%E6%90%AC%E8%87%B3CSDN/</link>
      <guid>http://yunsaijc.top/2023/10/07/11-%E5%B0%86%E5%8D%9A%E5%AE%A2%E6%90%AC%E8%87%B3CSDN/</guid>
      <pubDate>Sat, 07 Oct 2023 12:34:16 GMT</pubDate>
      
        
        
      <description>&lt;h2 id=&quot;年&quot;&gt;2023年&lt;/h2&gt;
</description>
        
      
      
      
      <content:encoded><![CDATA[<h2 id="年">2023年</h2>]]></content:encoded>
      
      
      
      
      <comments>http://yunsaijc.top/2023/10/07/11-%E5%B0%86%E5%8D%9A%E5%AE%A2%E6%90%AC%E8%87%B3CSDN/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>AI中的线性代数（有空更新）</title>
      <link>http://yunsaijc.top/2023/10/05/9-AI%E4%B8%AD%E7%9A%84%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/</link>
      <guid>http://yunsaijc.top/2023/10/05/9-AI%E4%B8%AD%E7%9A%84%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/</guid>
      <pubDate>Thu, 05 Oct 2023 09:08:00 GMT</pubDate>
      
        
        
      <description>&lt;h2 id=&quot;基本符号&quot;&gt;基本符号&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&quot;math inline&quot;&gt;\(A \in \mathbb{R}^{m\times
n}\)&lt;/span&gt;表示一个&lt;span class=&quot;math inline&quot;&gt;\(m\)&lt;/span&gt;</description>
        
      
      
      
      <content:encoded><![CDATA[<h2 id="基本符号">基本符号</h2><ul><li><span class="math inline">\(A \in \mathbb{R}^{m\timesn}\)</span>表示一个<span class="math inline">\(m\)</span>行<spanclass="math inline">\(n\)</span>列的实数矩阵(matrix)</li><li><span class="math inline">\(x\in\mathbb{R}^n\)</span>表示一个有<spanclass="math inline">\(n\)</span>个元素的向量(vector)。通常来说，一个<spanclass="math inline">\(n\)</span>维向量指的是一个<spanclass="math inline">\(n\times 1\)</span>的矩阵（即列向量(columnvector)）。通过转置(transpose)可以表示对应的<spanclass="math inline">\(1\times n\)</span>矩阵（即行向量(rowvector)）</li><li><span class="math inline">\(x_i\)</span>表示向量<spanclass="math inline">\(x\)</span>的第<spanclass="math inline">\(i\)</span>个元素</li><li><span class="math inline">\(a_{ij}\ or\ A_{ij}\)</span>表示矩阵<spanclass="math inline">\(A\)</span>中<spanclass="math inline">\(i\)</span>行<spanclass="math inline">\(j\)</span>列的元素</li><li><span class="math inline">\(a_j\ or\ A_{:,j}\)</span>表示矩阵<spanclass="math inline">\(A\)</span>的第<spanclass="math inline">\(j\)</span>列</li><li><span class="math inline">\(a_i^T\ or\ A_{i,:}\)</span>表示矩阵<spanclass="math inline">\(A\)</span>的第<spanclass="math inline">\(i\)</span>行</li></ul><h2 id="矩阵乘法matrix-multiplication">矩阵乘法(MatrixMultiplication)</h2><ul><li>矩阵<span class="math inline">\(A \in \mathbb{R}^{m\timesn}\)</span>和矩阵<span class="math inline">\(B \in \mathbb{R}^{n\timesp}\)</span>的乘积(product)是<span class="math inline">\(C=AB\in\mathbb{R}^{m\times p}\)</span>，其中<spanclass="math inline">\(C_{ij}=\sum^n_{k=1}A_{ik}B_{kj}\)</span></li><li>性质<ul><li>结合律(associative)：<spanclass="math inline">\((AB)C=A(BC)\)</span></li><li>分配律(distributive)：<spanclass="math inline">\(A(B+C)=AB+AC\)</span></li><li>不可交换(not commutative)：<span class="math inline">\(AB\neqBA\)</span></li></ul></li></ul><h2 id="操作与性质">操作与性质</h2><h3id="单位矩阵identity-matrix与对角矩阵diagonal-matrix">单位矩阵(IdentityMatrix)与对角矩阵(Diagonal Matrix)</h3><ul><li>单位阵<span class="math display">\[  I_{ij}=\left\{\begin{matrix}   1,\ i=j\\0,\ i\neq j \end{matrix}\right.  \]</span></li><li>满足对任意<span class="math inline">\(A\in \mathbb{R}^{m\timesn}\)</span>都有<span class="math inline">\(AI=A=IA\)</span></li><li>对角阵，通常记为<spanclass="math inline">\(D=diag(d_1,d_2,...,d_n)\)</span>，有<spanclass="math display">\[ D_{ij}=\left\{\begin{matrix}   D_i,\ i=j\\0,\ i\neq j \end{matrix}\right.  \]</span></li></ul><h3 id="转置transpose">转置(Transpose)</h3><ul><li><span class="math inline">\((A^T)_{ij}=A_{ji}\)</span></li><li>性质<ul><li><span class="math inline">\((A^T)^T=A\)</span></li><li><span class="math inline">\((AB)^T=B^TA^T\)</span></li><li><span class="math inline">\((A+B)^T=A^T+B^T\)</span></li></ul></li></ul><h3 id="对称矩阵symmetric-matrix">对称矩阵(Symmetric Matrix)</h3><ul><li>当<span class="math inline">\(A\in \mathbb{R}^{n\timesn}\)</span>且<span class="math inline">\(A^T=A\)</span>，则<spanclass="math inline">\(A\)</span>为对称阵；若<spanclass="math inline">\(A=-A^T\)</span>则<spanclass="math inline">\(A\)</span>为反对称阵(anti-symmetric)</li><li>对于任意<span class="math inline">\(A\in \mathbb{R}^{n\timesn}\)</span>，<span class="math inline">\(A+A^T\)</span>是对称阵，<spanclass="math inline">\(A-A^T\)</span>是反对称阵</li><li>因此任意方阵都可以表示为一个对称阵和一个反对称阵的和：<spanclass="math inline">\(A=\frac{1}{2}(A+A^T)+\frac{1}{2}(A-A^T)\)</span></li><li>对称阵在实践中非常常见，通常将所有大小为<spanclass="math inline">\(n\)</span>（即<span class="math inline">\(n\timesn\)</span>）的对称阵的集合记为<spanclass="math inline">\(\mathbb{S}^n\)</span></li></ul><h3 id="迹trace">迹(Trace)</h3><ul><li>方阵<span class="math inline">\(A\in \mathbb{R}^{n\timesn}\)</span>的迹（记为<spanclass="math inline">\(tr(A)\)</span>）是对角线上元素的和：<spanclass="math display">\[trA=\sum^{n}_{i=1}A_{ii}\]</span></li></ul><p>迹的性质：（以下矩阵均在<spanclass="math inline">\(\mathbb{R}^{n\times n}\)</span>中讨论）</p><ul><li><span class="math inline">\(trA=trA^T\)</span></li><li><span class="math inline">\(tr(A+B)=trA+trB\)</span></li><li><span class="math inline">\(t\in \mathbb{R},\ tr(tA)=t\trA\)</span></li><li><span class="math inline">\(AB\)</span>是方阵，则<spanclass="math inline">\(trAB=trBA\)</span></li><li><span class="math inline">\(ABC\)</span>是方阵，则<spanclass="math inline">\(trABC=trBCA=trCAB\)</span>，以此类推</li></ul><h3 id="范式norms">范式(Norms)</h3><ul><li>向量<span class="math inline">\(x\)</span>的范式<spanclass="math inline">\(||x||\)</span>，不正式地来讲，是该向量“长度”的度量。例如，常用的欧几里得范式/<span class="math inline">\(\ell_2\)</span>范式为：<spanclass="math display">\[||x||_2=\sqrt{\sum_{i=1}^{n}x^2_i}\]</span>注意，<span class="math inline">\(||x||^2_2=x^Tx\)</span></li></ul><p>正式地来讲，范式是一个满足以下4个条件的任意函数<spanclass="math inline">\(f\ :\\mathbb{R}^n\rightarrow\mathbb{R}\)</span>：</p><ul><li>非负性(non-negativity)：对于任意<spanclass="math inline">\(x\in\mathbb{R}^n\)</span>，<spanclass="math inline">\(f(x)\geq0\)</span></li><li>确定性(definiteness, 不确定这个翻译对不对)：<spanclass="math inline">\(f(x)=0\)</span>当且仅当<spanclass="math inline">\(x=0\)</span></li><li>齐次性(homogeneity)：对于任意<spanclass="math inline">\(x\in\mathbb{R}^n,\ t\in\mathbb{R}\)</span>，<spanclass="math inline">\(f(tx)=|t|f(x)\)</span></li><li>三角不等式(triangle inequality)：<span class="math inline">\(x,\y\in\mathbb{R}^n,\ t\in\mathbb{R}\)</span>，<spanclass="math inline">\(f(x+y)\leq f(x)+f(y)\)</span></li></ul><p>常用的范式如：</p><ul><li><span class="math inline">\(\ell_1\)</span>范式：<spanclass="math inline">\(||x||_1=\sum^n_{i=1}|x_i|\)</span></li><li><span class="math inline">\(\ell_{\infty}\)</span>范式：<spanclass="math inline">\(||x||_\infty=max_i|x_i|\)</span></li></ul><p>事实上，以上的3种范式属于<span class="math inline">\(\ell_p\(p\geq1)\)</span>范式，它的定义为：<spanclass="math display">\[||x||_p=(\sum_{i=1}^{n}|x_i|^p)^\frac{1}{p}\]</span></p><p>矩阵也可以定义范式，例如Frobenius范式：<span class="math display">\[||A||_F=\sqrt{\sum^m_{i=1}\sum^n_{j=1}A^2_{ij}}=\sqrt{tr(A^TA)}\]</span> 还存在其他的矩阵范式，此处不再叙述</p><h3 id="线性相关linear-independence与秩rank">线性相关(LinearIndependence)与秩(Rank)</h3><h2 id="参考">参考</h2><blockquote><p><ahref="https://www.yanxishe.com/TextTranslation/2965">https://www.yanxishe.com/TextTranslation/2965</a></p><p><ahref="https://cs229.stanford.edu/section/cs229-linalg.pdf">https://cs229.stanford.edu/section/cs229-linalg.pdf</a></p></blockquote>]]></content:encoded>
      
      
      <category domain="http://yunsaijc.top/categories/AI%E5%9F%BA%E7%A1%80/">AI基础</category>
      
      
      <category domain="http://yunsaijc.top/tags/AI/">AI</category>
      
      <category domain="http://yunsaijc.top/tags/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/">线性代数</category>
      
      
      <comments>http://yunsaijc.top/2023/10/05/9-AI%E4%B8%AD%E7%9A%84%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>信息安全数学基础总结（有空更新）</title>
      <link>http://yunsaijc.top/2023/10/03/8-%E4%BF%A1%E6%81%AF%E5%AE%89%E5%85%A8%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E6%80%BB%E7%BB%93/</link>
      <guid>http://yunsaijc.top/2023/10/03/8-%E4%BF%A1%E6%81%AF%E5%AE%89%E5%85%A8%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E6%80%BB%E7%BB%93/</guid>
      <pubDate>Tue, 03 Oct 2023 05:48:59 GMT</pubDate>
      
        
        
      <description>&lt;p&gt;大二第一学期，学院开设了《信息安全数学基础》这门课。但由于当时对信息安全的理解还不够深入，再加上已经过去了两年之久，很多知识都理解得不好，且印象模糊。借着大四这个相对空闲的时间，对学过的知识进行复习和总结。&lt;/p&gt;
&lt;h2 id=&quot;初等数论&quot;&gt;初等数论&lt;/h2&gt;
&lt;h3 i</description>
        
      
      
      
      <content:encoded><![CDATA[<p>大二第一学期，学院开设了《信息安全数学基础》这门课。但由于当时对信息安全的理解还不够深入，再加上已经过去了两年之久，很多知识都理解得不好，且印象模糊。借着大四这个相对空闲的时间，对学过的知识进行复习和总结。</p><h2 id="初等数论">初等数论</h2><h3 id="整数的因子分解">整数的因子分解</h3><h4 id="整除">整除</h4><ul><li><span class="math inline">\(a,\ b\in \mathbb{Z},\ b\neq 0,\quad 若\\exists\ q,\ 使得\  a=qb,\ 则\ b\mid a\ (b整除a),\ 否则\ b\nmida\)</span></li><li><span class="math inline">\(b\)</span>称为<spanclass="math inline">\(a\)</span>的因子，<spanclass="math inline">\(a\)</span>称为<spanclass="math inline">\(b\)</span>的倍数</li><li>性质<ul><li>传递性：<span class="math inline">\(c\mid b,\ b\mid a,\ then\ c\mida\)</span></li><li><span class="math inline">\(b\mid a,\ then\ bc\mid ac\)</span></li><li><span class="math inline">\(c\mid a,\ c\mid b,\ then\ c\midma+nb\)</span></li></ul></li></ul><h4 id="euclid欧几里得除法">Euclid(欧几里得)除法</h4><ul><li><span class="math inline">\(a,\ b\in \mathbb{Z},\ b&gt;0,\则存在唯一的整数对q,r\ 使得\ a=qb+r,\ 0\leq r&lt;b\)</span></li><li><span class="math inline">\(q\)</span>称为不完全商，<spanclass="math inline">\(r\)</span>称为余数（最小非负余数）</li><li>若调整<span class="math inline">\(q\)</span>（一般是加1）使得<spanclass="math inline">\(|r|\leq \frac{b}{2}\)</span>，则<spanclass="math inline">\(r\)</span>为绝对值最小余数（在Euclid算法中起到加速的作用）</li><li>可用于求整数的<span class="math inline">\(a\)</span>进制表示<img src="/2023/10/03/8-%E4%BF%A1%E6%81%AF%E5%AE%89%E5%85%A8%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E6%80%BB%E7%BB%93/1.png" class=""></li></ul><h4 id="公因子与最大公因子">公因子与最大公因子</h4><ul><li>设<span class="math inline">\(a,a_2,…,a_n\)</span>是<spanclass="math inline">\(n(n\geq2)\)</span>个整数，若整数<spanclass="math inline">\(d\)</span>是它们中每一个数的因数，则<spanclass="math inline">\(d\)</span>称<spanclass="math inline">\(a,a_2,…,a_n\)</span>的一个公因子</li><li>若<span class="math inline">\(a,a_2,…,a_n\)</span>不全为零，则<spanclass="math inline">\(a,a_2,…,a_n\)</span>的所有公因子中最大的一个称为最大公因子，记为<spanclass="math inline">\((a,a_2,…,a_n)\)</span></li><li>特别地，当<spanclass="math inline">\((a,a_2,…,a_n)=1\)</span>，称<spanclass="math inline">\(a,a_2,…,a_n\)</span>互素/互质</li><li>定义<spanclass="math inline">\((0,a)=a\)</span>，因为任何非零整数都是<spanclass="math inline">\(0\)</span>的因数</li></ul><p>辗转相除法/Euclid算法 求最大公因子</p><ul><li><p>预备定理：<span class="math inline">\(a,b,r是不全为0的整数,\若a=qb+r,\ q\in\mathbb{Z},\ 则(a,b)=(b,r)\)</span><img src="/2023/10/03/8-%E4%BF%A1%E6%81%AF%E5%AE%89%E5%85%A8%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E6%80%BB%E7%BB%93/2.png" class=""></p></li><li><p>将上述辗转相除的过程反过来写（即把余数放左边，用<spanclass="math inline">\(a,b\)</span>来表示余数），可以得到最大公因子的线性表示，即贝祖等式</p></li><li><p>贝祖等式：对任意两个正整数<spanclass="math inline">\(a,b\)</span>，存在整数<spanclass="math inline">\(x,y使得(a,b)=xa+yb\)</span></p></li><li><p>当<spanclass="math inline">\((a,b)=1\)</span>，可以求出唯一解<spanclass="math inline">\(x\)</span>，即<span class="math inline">\(xa\equiv1(mod\ b),\ x\equiv a^{-1}(mod\ b)\)</span></p></li><li><p>扩展/广义Euclid算法：求出最大公因子的同时求出系数<spanclass="math inline">\(x,y\)</span>（主要用于求乘法逆元）</p></li><li><p>伪代码 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Input: 非负整数a, b且a&gt;=b (先将待计算的整数取绝对值)</span><br><span class="line">Output: r=(a,b) 以及满足 sa+tb=(a,b)的s,t</span><br><span class="line">Extended Euclid(a,b)&#123;</span><br><span class="line">(r,s,t) &lt;- (a,1,0);</span><br><span class="line">(r&#x27;,s&#x27;,t&#x27;) &lt;- (b,0,1);</span><br><span class="line">While r&#x27;!=0 do&#123;</span><br><span class="line">q &lt;- floor(r/r&#x27;);</span><br><span class="line">(tmp1,tmp2,tmp3) &lt;- (r-qr&#x27;,s-qs&#x27;,t-qt&#x27;);</span><br><span class="line">(r,s,t) &lt;- (r&#x27;,s&#x27;,t&#x27;);</span><br><span class="line">(r&#x27;,s&#x27;,t&#x27;) &lt;- (tmp1,tmp2,tmp3);</span><br><span class="line">&#125;</span><br><span class="line">Return r,s,t;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p></li><li><p>最大公因子等价定义：<spanclass="math inline">\(a,b\)</span>是不全为零的整数，则<spanclass="math inline">\(d=(a,b)\)</span>是集合<spanclass="math inline">\(\{xa+yb|x,y\in\mathbb{Z}\}\)</span>中的最小正整数</p></li><li><p>应用-方程有解的判定：<spanclass="math inline">\(a,b\)</span>是不全为零的整数，则方程<spanclass="math inline">\(ax+by=c有整数解\Leftrightarrow (a,b)\midc\)</span> &gt;个人理解：对于整数<spanclass="math inline">\(a,b\)</span>，他们的最大公因子就是使用这两个数能得到的最小度量&gt; &gt;换句话说，即长度为<spanclass="math inline">\(a,b\)</span>的两把尺子可以度量的最小长度</p></li></ul><h4 id="素数">素数</h4><ul><li>埃拉托色尼素数筛选法-快速计算<spanclass="math inline">\(1\)</span>到<spanclass="math inline">\(N\)</span>之间的所有素数：遍历<spanclass="math inline">\(1\)</span>到<spanclass="math inline">\(\sqrt{N}\)</span>，将其中所有素数的倍数去掉，剩下的便都是素数了</li><li>素性定理：<span class="math inline">\(p\)</span>为素数，<spanclass="math inline">\(a,b\)</span>为整数，若<spanclass="math inline">\(p\mid ab\)</span>，则<spanclass="math inline">\(p\mid a\)</span>或<spanclass="math inline">\(p\mid b\)</span> ^291cce</li><li>算数基本定理：任意整数<span class="math inline">\(n\(n&gt;1)\)</span>都可以分解为有限个素数的乘积<spanclass="math inline">\(n=p_1p_2...p_s\)</span>，该分解除了素数因子的排列之外，是唯一的</li><li>唯一因子分解定理：任意整数<span class="math inline">\(n\(n&gt;1)\)</span>可以唯一地表示成<spanclass="math inline">\(n=p_1^{\alpha_1}p_2^{\alpha_2}...p_s^{\alpha_s},\\alpha_i &gt;0\)</span>，其中<spanclass="math inline">\(p_1p_2...p_s\)</span>为素数<spanclass="math inline">\(p_i&lt;p_j\ (i&lt;j)\)</span>。上式叫做<spanclass="math inline">\(n\)</span>的标准分解式。</li></ul><p>整数的一些性质</p><ul><li><spanclass="math inline">\(a=p_1^{\alpha_1}p_2^{\alpha_2}...p_s^{\alpha_s},\b=p_1^{\beta_1}p_2^{\beta_2}...p_s^{\beta_s}\)</span>，那么<spanclass="math inline">\((a,b)=p_1^{\gamma_1}p_2^{\gamma_2}...p_s^{\gamma_s},\\gamma_i=min(\alpha_i,\beta_i)\)</span>，最小公倍数<spanclass="math inline">\([a,b]=p_1^{\delta_1}p_2^{\delta_2}...p_s^{\delta_s},\\delta_i=max(\alpha_i,\beta_i)\)</span></li><li><span class="math inline">\(a_1,a_2,...a_n\)</span>为<spanclass="math inline">\(n\)</span>个非零整数，令<spanclass="math inline">\([a_1,a_2]=m_1,\[m_1,a_3]=m_2,...,[m_{n-2},a_n]=m_{n-1}\)</span>，则<spanclass="math inline">\([a_1,a_2,...a_n]=m_{n-1}\)</span></li><li>除了2以外，所有素数都是奇素数</li></ul><h4 id="多项式的整除性">多项式的整除性</h4><ul><li>令<span class="math inline">\(\mathbb{Q}=\{\frac{a}{b}|a,b\in\mathbb{Z},b\neq0\}\)</span>表示全体有理数的集合。<spanclass="math inline">\(\mathbb{Q}\)</span>上有加减乘除四则运算</li><li>令<spanclass="math inline">\(\mathbb{Q}[x]=\{a_0+a_1x+...+a_nx^n|a_i\in\mathbb{Q},0\leqi\leq n\}\)</span>表示所有系数为有理数的多项式集合。<spanclass="math inline">\(\mathbb{Q}[x]\)</span>有加减乘，但没有除法</li><li>可以发现<spanclass="math inline">\(\mathbb{Q}[x]\)</span>与整数集合<spanclass="math inline">\(\mathbb{Z}\)</span>有很多类似的性质：都有带余除法、最大公因子、唯一因子分解定理等</li></ul><p>（多项式<span class="math inline">\(f(x)\)</span>的次数表示为<spanclass="math inline">\(deg\ f(x)\)</span>，以下多项式均在<spanclass="math inline">\(\mathbb{Q}[x]\)</span>内讨论）</p><ul><li><span class="math inline">\(g(x)\neq 0\)</span>，则有<spanclass="math inline">\(q(x),r(x)\)</span>使得<spanclass="math inline">\(f(x)=q(x)g(x)+r(x),\ r(x)=0或r(x)\neq 0,deg\ r(x)&lt; deg\ g(x)\)</span></li><li><span class="math inline">\(r(x)=0\)</span>时，称<spanclass="math inline">\(g(x)整除f(x)\)</span>，记<spanclass="math inline">\(g(x)|f(x)\)</span>，<spanclass="math inline">\(g(x)\)</span>称为<spanclass="math inline">\(f(x)\)</span>的因子</li><li>当<span class="math inline">\(g(x)\)</span>为<spanclass="math inline">\(f(x)\)</span>的因子，且<spanclass="math inline">\(deg\ g(x) &lt; deg\ f(x)\)</span>时，<spanclass="math inline">\(g(x)\)</span>称为<spanclass="math inline">\(f(x)\)</span>的真因子</li><li>当<spanclass="math inline">\(f(x)\)</span>没有真因子时，称为不可约多项式</li></ul><blockquote><p>个人理解：把函数的符号<spanclass="math inline">\(f,g\)</span>当成整数的符号来看，多项式的整除与整数的整除几乎相同，只是多了<spanclass="math inline">\(x\)</span>而已</p><p>因此其余的不再赘述：</p><p>多项式整除性质——参考整数整除性质</p><p>多项式最大公因子——参考整数最大公因子</p><p>多项式最大公因子的表示——参考整数最大公因子的表示（贝祖等式）</p><p>多项式最大公因子的求法——参考整数的辗转相除法</p><p>不可约多项式的有关定理——参考整数的素性定理</p><p>多项式的唯一分解定理——参考整数的唯一分解定理</p></blockquote><h3 id="同余式">同余式</h3><h3 id="二次剩余">二次剩余</h3><h2 id="抽象代数">抽象代数</h2><h3 id="群">群</h3><h3 id="环">环</h3><h3 id="域">域</h3><h3 id="有限域">有限域</h3><h2 id="参考">参考</h2>]]></content:encoded>
      
      
      <category domain="http://yunsaijc.top/categories/%E7%BD%91%E7%BB%9C%E7%A9%BA%E9%97%B4%E5%AE%89%E5%85%A8%E5%9F%BA%E7%A1%80/">网络空间安全基础</category>
      
      
      <category domain="http://yunsaijc.top/tags/%E6%95%B0%E5%AD%A6/">数学</category>
      
      <category domain="http://yunsaijc.top/tags/%E6%95%B0%E8%AE%BA/">数论</category>
      
      <category domain="http://yunsaijc.top/tags/%E6%8A%BD%E8%B1%A1%E4%BB%A3%E6%95%B0/">抽象代数</category>
      
      
      <comments>http://yunsaijc.top/2023/10/03/8-%E4%BF%A1%E6%81%AF%E5%AE%89%E5%85%A8%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E6%80%BB%E7%BB%93/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>关于HTTP反向代理的理解</title>
      <link>http://yunsaijc.top/2023/09/27/7-%E5%85%B3%E4%BA%8EHTTP%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86%E7%9A%84%E7%90%86%E8%A7%A3/</link>
      <guid>http://yunsaijc.top/2023/09/27/7-%E5%85%B3%E4%BA%8EHTTP%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86%E7%9A%84%E7%90%86%E8%A7%A3/</guid>
      <pubDate>Wed, 27 Sep 2023 05:48:59 GMT</pubDate>
      
        
        
      <description>&lt;h2 id=&quot;正向代理forward-proxy&quot;&gt;正向代理(Forward Proxy)&lt;/h2&gt;
&lt;p&gt;正向代理通常直接称为代理(&lt;code&gt;Proxy&lt;/code&gt;)，也就是日常情景下使用的各种&lt;code&gt;BrupSuite、梯子&lt;/code&gt;等的代理，需要我们手动进行配</description>
        
      
      
      
      <content:encoded><![CDATA[<h2 id="正向代理forward-proxy">正向代理(Forward Proxy)</h2><p>正向代理通常直接称为代理(<code>Proxy</code>)，也就是日常情景下使用的各种<code>BrupSuite、梯子</code>等的代理，需要我们手动进行配置。</p><h3 id="代理的作用">代理的作用</h3><ul><li>便于安全审计：所有流量经过一台代理服务器，那么安全审计就容易得多</li><li>加速访问/节省带宽：代理服务器可以返回缓存好的内容，无需向外网发出请求，从而加快访问速度、节省带宽</li><li>保护个人信息：使用代理隐藏自己的真实IP地址</li><li>突破访问限制：有时自己的地址被禁止访问，可以连接白名单上的代理服务器来进行访问</li></ul><h2 id="反向代理reverse-proxy">反向代理((Reverse Proxy)</h2><p>个人理解：类似于计算机网络中的NAT(网络地址转发)技术。用户并不知道自己被“反向代理”了，反向代理服务器(如<code>Nginx</code>)接收到请求后，转发给Web服务器(如<code>Apache</code>)进行处理。</p><p>为了方便阅读，此处开始用<code>Nginx</code>来指代<code>反向代理服务器</code>。</p><h3 id="反向代理作用">反向代理作用</h3><ul><li>负载均衡：访问压力大时，<code>Nginx</code>可以将请求分配给多个不同的Web服务器进行处理，而用户访问的地址仍然是相同的地址(即<code>Nginx</code>服务器的地址)</li><li>提高安全性：Web服务器对外不可见，此时<code>Nginx</code>相当于一个防火墙</li><li>节省IP资源：如上述对反向代理的个人理解所说，类似于NAT，能够节省IP资源</li><li>加速访问：<code>Nginx</code>同样可以缓存网页内容</li></ul><h2 id="总结">总结</h2><p>可以理解为：</p><ul><li>正向代理是面向用户的代理</li><li>反向代理是面向服务器的代理，对用户透明（也就是说把服务器也当成用户，服务器在给用户返回请求时自己挂了一个代理）</li></ul><h2 id="参考">参考</h2><blockquote><p><ahref="https://zhuanlan.zhihu.com/p/464965616">https://zhuanlan.zhihu.com/p/464965616</a></p><p><ahref="https://www.zhihu.com/question/388303662">https://www.zhihu.com/question/388303662</a></p><p><ahref="https://zhuanlan.zhihu.com/p/486180243">https://zhuanlan.zhihu.com/p/486180243</a></p></blockquote>]]></content:encoded>
      
      
      <category domain="http://yunsaijc.top/categories/%E5%AD%A6%E4%B9%A0%E6%9D%82%E8%AE%B0/">学习杂记</category>
      
      
      <category domain="http://yunsaijc.top/tags/Web/">Web</category>
      
      
      <comments>http://yunsaijc.top/2023/09/27/7-%E5%85%B3%E4%BA%8EHTTP%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86%E7%9A%84%E7%90%86%E8%A7%A3/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>HTB-sau Walkthrough</title>
      <link>http://yunsaijc.top/2023/09/17/6-HTB-sau-Walkthrough/</link>
      <guid>http://yunsaijc.top/2023/09/17/6-HTB-sau-Walkthrough/</guid>
      <pubDate>Sun, 17 Sep 2023 02:10:19 GMT</pubDate>
      
        
        
      <description>&lt;blockquote&gt;
&lt;p&gt;运行环境：macOS 13.2.1; Parallel Desktop: Kali Linux 2022.2 ARM64;
Windows 11&lt;/p&gt;
&lt;p&gt;靶机链接：&lt;a
href=&quot;https://app.hackthebox.com/mac</description>
        
      
      
      
      <content:encoded><![CDATA[<blockquote><p>运行环境：macOS 13.2.1; Parallel Desktop: Kali Linux 2022.2 ARM64;Windows 11</p><p>靶机链接：<ahref="https://app.hackthebox.com/machines/Sau">https://app.hackthebox.com/machines/Sau</a></p></blockquote><h2 id="过程">过程</h2><h3 id="信息收集">信息收集</h3><p>基础的四项扫描： <img src="/2023/09/17/6-HTB-sau-Walkthrough/1.png" class=""></p><p>在端口扫描时，采用默认的<code>-sS</code>方式，能够扫到<code>-sT</code>方式扫不到的80和8338端口：</p><img src="/2023/09/17/6-HTB-sau-Walkthrough/2.png" class=""><img src="/2023/09/17/6-HTB-sau-Walkthrough/3.png" class=""><img src="/2023/09/17/6-HTB-sau-Walkthrough/4.png" class=""><img src="/2023/09/17/6-HTB-sau-Walkthrough/5.png" class=""><p>扫描完成后， 进行子目录爆破：</p><img src="/2023/09/17/6-HTB-sau-Walkthrough/6.png" class=""><h3 id="web部分">Web部分</h3><p>进入<code>hostname/web/</code>，发现是一个<code>request basket</code>的Web应用，可以收集各种请求：</p><img src="/2023/09/17/6-HTB-sau-Walkthrough/7.png" class=""><p>创建一个名为<code>sau</code>的basket：</p><img src="/2023/09/17/6-HTB-sau-Walkthrough/8.png" class=""><p>搜索发现该应用存在一个SSRF漏洞<code>CVE-2023-27163</code>：</p><img src="/2023/09/17/6-HTB-sau-Walkthrough/9.png" class=""><p>尝试使用网上的脚本进行利用，但没有成功：</p><img src="/2023/09/17/6-HTB-sau-Walkthrough/10.png" class=""><p>不再尝试网上的poc，继续往下进行。</p><p>搜索发现这是一个类似于代理的应用。在设置中可以设置目标的URL。</p><p>将访问<code>http://hostname/sau</code>的主机记为主机A（攻击机），主机A想要访问的主机记为主机B。</p><p><code>Forward URL</code>是需要转发到的地址，即主机B地址；</p><p><code>Proxy Response</code>勾选后，主机B的响应会发送回主机A；</p><p><code>Expand Forward</code>勾选后，主机A访问时扩展的部分会被添到<code>Forward URL</code>的后面。如：主机A访问<code>http://hostname/sau/login</code>，如果勾选该项，那么实际访问的就是<code>&#123;Forward URL&#125;/login</code>；否则就是<code>&#123;Forward URL&#125;</code>，后面的<code>/login</code>是无效的。</p><p>此处务必要注意勾选后面两个！否则后续的步骤不成立！</p><p>在端口扫描过程中发现了一个80端口，但不对外开放。此处将目标URL设为服务器本地地址的80端口进行尝试：</p><img src="/2023/09/17/6-HTB-sau-Walkthrough/11.png" class=""><h3 id="主机立足">主机立足</h3><p>设置完成后访问<code>http://hostname/sau</code>，是一个名为<code>Maltrail</code>的恶意流量监测应用。搜索其漏洞：</p><img src="/2023/09/17/6-HTB-sau-Walkthrough/12.png" class=""><p>下载poc镜像到本地，利用成功。获得shell：</p><img src="/2023/09/17/6-HTB-sau-Walkthrough/13.png" class=""><p>获得userflag：</p><img src="/2023/09/17/6-HTB-sau-Walkthrough/14.png" class=""><h3 id="提权">提权</h3><p>通过<code>sudo -l</code>查看权限后，在GTFOBins查找提权方法并进行提权：</p><img src="/2023/09/17/6-HTB-sau-Walkthrough/15.png" class=""><p>获得systemflag：</p><img src="/2023/09/17/6-HTB-sau-Walkthrough/16.png" class=""><h2 id="总结">总结</h2><ol type="1"><li>Request-basket存在SSRF，可以访问服务器本地的80端口</li><li>Maltrail自身存在漏洞</li></ol>]]></content:encoded>
      
      
      <category domain="http://yunsaijc.top/categories/%E9%9D%B6%E6%9C%BAWalkthrough/">靶机Walkthrough</category>
      
      
      <category domain="http://yunsaijc.top/tags/HTB/">HTB</category>
      
      <category domain="http://yunsaijc.top/tags/RedTeam/">RedTeam</category>
      
      
      <comments>http://yunsaijc.top/2023/09/17/6-HTB-sau-Walkthrough/#disqus_thread</comments>
      
    </item>
    
  </channel>
</rss>
